---
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| echo: true
#| message: false
#| warning: false


library(tidyverse)
library(here)
library(knitr)
library(mvtnorm)
library(ggfortify)
library(HSAUR2)
library(pheatmap)

#아래 3 문장은 한글을 포함한 ggplot 그림이 포함된 HTML, PDF로 만드는 경우 사용
library(showtext)
#font_add_google("Nanum Pen Script", "gl")
font_add_google(name = "Noto Sans KR", family = "noto")
showtext_auto()
```





# 탐색적 인자 분석

인자분석(factor analysis)은 관측된 여러 변수들 간의 상관관계를 설명하기 위해, 관측된 여러 개의 변수들이 소수의 잠재적 요인(latent factors)에 의해 영향을 받는다는 가정 하에 사용되는 통계적 모형이다. 


다변량 관측자료에 포함된 정보가 소수의 잠재 요인(latent factors)으로부터 생성된다고 가정하는 것은 매우 흥미로운 모형이다. 실제로 소수의 잠재적 요인이 다양한 정보들과 관련되어 있다면 현실에 나타나는 여러가지 다양한 현상들을 깊이 이해하고 잘 설명해 줄 수 있다. 인자분석은 복잡한 데이터 구조를 단순화하고 이해하는 데 도움을 준다.

또한 심리학, 사화학, 경제학에서는 자연과학과 다르게 연구의 대상이 되는 개념을 직접 측정할 수 없는 경우가 흔하게 일어난다. 예를 들어 자연과학에서 온도(temperature)라는 개념은 과학적인 정의에 따라서 다양한 방법으로 쉽고 정확하게 측정할 수 있다. 하지만 지능(intelligence), 스트레스(stress), 고통(pain), 만족도와 같은 연구 대상들은 온도와 다르게 정의도 어렵고 측정은 더 어려운 것이 현실이다.

예를 들어 학교에서 배우는 여러 과목의 시험점수들에 개인이 가지고 있는 지능(intelligence)라는 잠재변수가 공통적으로 영향을 미친다고 가정할 수 있다. 여기서 지능은 측정할 수 없으며 시험의 결과는 개인이 가진 지능에 영향을 받으며 점수로 측정값을 구할 수 있다.

이렇게 정의와 측정이 어려운 개념을 직접 관찰할 수 없는 요인(**latent factor; 인자**)으로 보고 이러한 요인에 영향을 받아 그 값을 실제로 관측할 수 있는 다양한 파생적인 변수(**manifest variables; 명시변수, 관측변수**)들을 관측할 수 있다고 가정할 수 있다. 이렇게 소수의 인자로 다수의 관측 가능한 변수들의 관계를 선형 관계로 가정하고 해석하는 분석을 인자 분석(factor analysis)라고 한다.   인자분석은 심리학, 마케팅, 경제학, 정치학 등 매우 다양한 분야에서도 중요한 분석 방법으로 사용된다.


인자 분석은 두 가지 유형으로 나뉜다. 첫 번쨰는 탐색적 요인분석(explanatory factor analysis)으로, 어떤 관측변수가 어떤 요인과 관련되는지에 대한 가정을 하지 않은 채 관측변수와 요인 간의 관계를 조사하는 데 사용된다. 확증적 요인 분석(confirmatory factor analysis)으로, 사전에 가정된 특정 요인 모델이 관측 변수들 간의 분산이나 상관관계에 적합한지 검증하는 데 사용된다. 이 장에서는 탐색적 요인분석만을 다루고 확증적 요인 분석은 다루지 않을 것이다. 

인자분석의 목적과 방법을 간단하게 요약하면 다음과 같이 말할 수 있다.

- 가정된 인자(factor) 또는 잠재변수(latent variable)와 측정변수들의 관계를 찾는것이 탐색적 인자분석의 목적이다.

- 인자는 측정할 수 없는 변수로서 각 측정변수에 영향을 미친다고 가정한다.

- 여러 개의 관측변수에 영향을 미치는 공통의 잠재변수는 공통인자(common factor)라고 말한다.
 
 
## 인자 모형 
 
### 단순 인자 모형 
 
 이 장에서는 단순 인자 모형(one factor model)을 사용하여 인자분석의 기본 개념을 설명한다. 단순 인자 모형은 하나의 잠재 요인(factor)이 여러 관측 변수들에 영향을 미친다고 가정하는 모델이다. 
 
탐색적 인자 분석을 이용한 단순 인자 모형은 Spearman 이 1904년 학생들의 시험 성적과 지능에 대한 모형을 고려하면서 처음 제안되었다. 이제 Spearman 이 제안한 단순 인자 분석에 대한 통계적 가정과 모형을 살펴보자.

 Spearman 은 세 과목의 시험점수에 대한 자료를 얻어서 다음과 같은 상관행렬 $\pmb R$을 얻었다.

- $X_1$ : Classics
- $X_2$ : French
- $X_3$ : English


$$ 
\pmb R=
\begin{bmatrix}
1.00 & & \\
0.83 & 1.00 & \\
0.78 & 0.67 & 1.0
\end{bmatrix}
$$

지능(intelligence)을 나타내는 잠재 변수인 $f$가 3개의 시험 점수와 다음과 같은 관계를 가진다고 가정하는 것이 일인자 인자 모형(one factor model)이다. 모형은 다음과 같은 수식으로 표현된다.

$$
\begin{aligned}
X_1 & = \lambda_1 f + u_1  \\
X_2 & = \lambda_2 f + u_2 \\
X_3 & = \lambda_3 f + u_3 
\end{aligned}
$$ {#eq-multivar-factor-one-factor}


@eq-multivar-factor-one-factor 에서 제시된 일인자 모형에 대한 특성은 다음과 같다. 


- $f$ 는 공통 인자(common factor, latent variable)로 관측할 수 없는 확률 변수(random variable)이다.

- $\lambda_1, \lambda_2, \lambda_3$는 인자 적재값 (factor loading) 이라고 부르며  고정된 값을 가진 계수이다. 

- $u_i$ 는 임의 변동(random disturbance)으로 점수 $X_i$ 에 반영되는 양으로 개인의 특정한 능력(specific factor) 과 단순 오차(random error)의 합으로 구성된다. 



@eq-multivar-factor-one-factor 에서 정의된 항들의 분포 가정과 관측 변수의 분산 구조는 다음과 같다. 

- $X_i$'s 는 평균이 0 이고 분산이 1인 확률변수이다 (표준화)

- $f$ 는 평균이 0 이고 분산이 1인 확률변수이다.

- $u_i$'s 평균이 0 이고 분산이 $\psi_i$ 인 확률변수이다

- $u_i$ 들은 서로 독립이다.

- $f$ 와 $u_i$ 들도 서로 독립이다.

이제 관측 변수 $X_i$'s 의 분산과 공분산을 계산하여 @eq-multivar-factor-one-factor 에서 제시된 모형이 상관행렬 $\pmb R$를 어떻게 설명하는지 살펴보자. 

먼저 $X_i$'s 의 분산을 계산하면 다음과 같다.
$$
\begin{aligned}
V(X_i) & \equiv  1 \\
    &=  V(\lambda_i f +u_i)  \\ 
   &= \lambda_i^2 V(f) + V(u_i) \\
   &= \lambda_i^2 +\psi_i
\end{aligned}
$$

다음으로 $X_i$'s 의 공분산을 계산하면 다음과 같다.
$$
\begin{aligned}
Cov(X_i,X_j) &= Cov(\lambda_i f +u_i, \lambda_j f + u_j) \\
  &= \lambda_i \lambda_j Cov(f,f) + \lambda_i Cov(f,u_j) + \lambda_j Cov(f, u_i) +Cov(u_i, u_j) \\
  &= \lambda_i \lambda_j V(f) +0 +0+0  \\
  &= \lambda_i \lambda_j \\
  &\equiv corr(X_i,X_j) \quad \text{since } V(X_i)=1 
\end{aligned}
$$ {#eq-multivar-factor-one-factor-cov}

### k-인자 모형

이제 @eq-multivar-factor-one-factor 에 나타난 일인자 모형을 확장하여  k 개의 인자를 가지는 k-인자 모형을 고려할 수 있다.
관측이 가능한 확률 변수의 개수는 $q$ 개라고 하자.

여기서 인자 분석의 목적에 따라서 인자의 개수 $k$ 는 가능한 확률 변수의 개수는 $q$ 보다 작게 설정하는 것이 일반적이다 ($k \le q$).   

$$
\begin{aligned}
X_1   &= \lambda_{11} f_1 + \lambda_{12} f_2 + \dots \lambda_{1k} f_k +u_1  \\
X_2   &= \lambda_{21} f_1 + \lambda_{22} f_2 + \dots \lambda_{2k} f_k +u_2  \\
  & \dots  \dots    \\
X_q   &= \lambda_{q1} f_1 + \lambda_{q2} f_2 + \dots \lambda_{qk} f_k +u_q  \\
\end{aligned}
$$ {#eq-multivar-factor-k-factor}

@eq-multivar-factor-k-factor 에서 정의된 항들의 분포 가정과 관측 변수의 분산 구조는 다음과 같다. 

- 확률 변수 $X_i$'s 는 평균이 0 이고 분산이 1인 확률변수이다 (표준화)

- k-인자 벡터 $\pmb f$ 는 평균이 $\pmb 0$ 이고 분산이 $\pmb I_k$ 인 확률벡터이다.

$$ E(\pmb f) =0, \quad Var(\pmb f) = \pmb I_k $$

- $\pmb u$'s 평균이 $\pmb 0$ 이고 각 분산이 $\psi_i$ 이며 서로 독립인 확률변수이다

$$ E(\pmb u) =0, \quad Var(\pmb u) = \pmb \Psi 
= \begin{bmatrix}
\psi_1 & 0 & \dots & 0  \\
0 & \psi_2 & \dots & 0  \\
  &  &  \dots &  \\
0 & 0 & \dots & \psi_q  \\
\end{bmatrix}
$$

- $\pmb f$ 와 $\pmb u$ 들도 서로 독립이다.

@eq-multivar-factor-k-factor 의 k-인자 모형을 행렬식으로 표현하면 다음과 같다. 

$$ 
\pmb X = \pmb \Lambda  \pmb f + \pmb u 
$$ {#eq-multivar-factor-k-factor-matrix}


여기서 $\pmb \Lambda$는 다음과 같은 $q \times k$ 행렬이다
$$ 
\Lambda =
\begin{bmatrix}
\lambda_{11} & \lambda_{12} & \dots & \lambda_{1k}  \\
\lambda_{21} & \lambda_{22} & \dots & \lambda_{2k}  \\
  &  &  \dots &  \\
\lambda_{q1} & \lambda_{q2} & \dots & \lambda_{qk}  \\
\end{bmatrix}
$$

@eq-multivar-factor-k-factor 에서 제시된 k-인자 모형과 통계적 가정에 따라서 관측 변수들의 분산 구조는 다음과 같다.


$$ 
\begin{aligned}
Var(X_i) & = Var(\lambda_{i1} f_1 + \lambda_{i2} f_2 + \dots \lambda_{ik} f_k +u_i ) \\
 & = \sum_{l=1}^q \lambda_{il}^2 + \psi_i  \\
 & = h_i^2 + \psi_i  \\
 & \equiv 1
 \end{aligned}
$$ {#eq-multivar-factor-k-factor-var1}

@eq-multivar-factor-k-factor-var1 에서 나타난 분산의 두 부분을 다음과 같이 부른다.

1. $h_i^2 = \sum_{l=1}^q \lambda_{il}^2$를 변수의 공통성(communality)라고 부른다.
  + 공통성(communality)의 의미는 관측변수들 모두가 공통으로 영향을 받는 k개의 인자들에 의해 설명되는 분산의 크기를 나타낸다. 공통성이 크다는 것은 해당 변수가 인자들에 의해 잘 설명된다는 것을 의미한다.

2. $\psi_i$는 임의변동(random disturbance) 또는 유일분산(unique variance)라 부른다.
  + 임의변동은 변수 $X_i$의 분산 중에서 인자들에 의해 설명되지 않는 부분을 나타낸다. 임의변동이 크다는 것은 해당 변수가 공통 인자들이 아닌 특정한 요인에 의해 영향을 더 많이 받는다는 것을 의미한다.



각 변수간의 상관 관계는 다음과 같다. 변수간의 상관관계는 특수분산과 관계없다.

$$ 
\begin{aligned}
cor(X_i, X_j) & = cor( \sum_{l=1}^q \lambda_{il} f_l +u_i , \sum_{l=1}^q \lambda_{jl} f_l +u_j ) \\
 & = \sum_{l=1}^q \lambda_{il} \lambda_{jl} Var(f_l) + 0 +0 +0 \\
 & =  \sum_{l=1}^q \lambda_{il} \lambda_{jl}
\end{aligned}
$$

위의 결과와 관측벡터에 대한 표준화 가정을 이용하면 관측벡터 $\pmb X$ 의 공분산은 다음과 같이 상관계수 행렬 $\pmb R$ 로 나타난다.



$$ 
\begin{aligned}
 \pmb \Sigma  & = Cov( \pmb X) \\
 & = Cov(\pmb \Lambda \pmb f + \pmb u) \\
 & = \pmb \Lambda Cov(\pmb f) \pmb \Lambda^t + Cov(\pmb u) \\
 & = \pmb \Lambda \pmb  \Lambda^t + \pmb \Psi \\
  & \equiv \pmb R
\end{aligned} 
$$ {#eq-multivar-factor-k-factor-cov}
여기서 $\pmb \Psi$는 특수분산 $\psi_i$들을 대각원소로 가지는 대각행렬이다.

$$
\pmb \Psi =
\begin{bmatrix}
\psi_1 & 0 & \dots & 0  \\
0 & \psi_2 & \dots & 0  \\
  &  &  \dots &  \\
0 & 0 & \dots & \psi_q  \\
\end{bmatrix}
$$



### 척도 불변성

지금까지 관측변수 $X_i$가 평균은 0 이고(언제나 관측값에서  평균을 빼면 가능하다) 분산이 1 이라고 표준화 가정을 사용하였다. 이제 관측변수 $X_i$가의 분산이 1이 아니라고 가정하고 $X_i$에 대한 척도 변환(scale transformation)을 생각해보자. 즉, 새로운 확률변수 $Y_i$ 를 $X_i$에 대한 척도 변환으로 다음과 같이 정의한다.

$$ 
Y_i =c_i X_i, \quad i=1,2,3\dots, q
$$

행렬 $\pmb C$를 $c_i$ 들로 이루어진 대각 행렬이라고 한다면 

$$
\pmb C =
\begin{bmatrix}
c_1 & 0 & \dots & 0  \\
0 & c_2 & \dots & 0  \\
  &  &  \dots &  \\
0 & 0 & \dots & c_q  \\
\end{bmatrix}
$$


새로운 관측값 확률벡터는 $\pmb Y = \pmb C \pmb X$이고 변환된 $\pmb Y$의 공분산은 @eq-multivar-factor-k-factor-cov 에 의하여 다음과 같이 쓸 수 있다. 

$$ 
\begin{aligned}
Cov(\pmb Y) & = Cov(\pmb C \pmb X) \\
  & = \pmb C Cov(\pmb X) \pmb C^t \\
  & = \pmb C \pmb \Lambda \pmb \Lambda^t \pmb C^t + \pmb C \pmb \Psi \pmb C^t \\
\end{aligned}
$$ 

위의 식에서 $\pmb {\Lambda}_y = \pmb C \pmb \Lambda$로 정의하고 
${\pmb u}_y=  \pmb C \pmb u$ 로 놓으면 변환된 확률 벡터 $\pmb Y$에 대한 인자모형은 다음과 같다.

$$ 
\pmb Y = {\pmb \Lambda}_y \pmb f + {\pmb u}_y
$$ {#eq-multivar-factor-k-factor-scale-cov}


@eq-multivar-factor-k-factor-scale-cov 을 척도 변환과 함께 풀어 쓰면 다음과 같다. 

$$
\begin{aligned}
Y_1   &= c_1 \lambda_{11} f_1 + c_1 \lambda_{12} f_2 + \dots + c_1 \lambda_{1k} f_k +c_1 u_1  \\
Y_2   &= c_2 \lambda_{21} f_1 + c_2 \lambda_{22} f_2 + \dots + c_2\lambda_{2k} f_k +c_2u_2  \\
  &    \dots \\
Y_q   &= c_q \lambda_{q1} f_1 + c_q \lambda_{q2} f_2 + \dots + c_q \lambda_{qk} f_k +c_qu_q  \\
\end{aligned}
$$

위의 식에서 볼 수 있듯이 척도변환을 하면 각 인자의 적재값과 임의변동의 값도 같은 척도로 변하는 것을 알 수 있다.
만약에  $c_i$를  관측변수의 표준편차 $s_i$ 의 역수로 놓으면 $c_i = 1/s_i$ $\pmb Y$의 공분산 행렬은 
$\pmb X$의 상관계수 행렬이 된다. 

따라서 인자분석을 하는 경우 $\pmb X$의 공분산 행렬을 이용하는 것과 상관계수 행렬을 이용하는 것이 일치하는 결과를 준다. 일치하는 결과라는 것은 척도변환에 의하여 언제나 대응되는 계수를 구할 수 있다는 의미이다 (**척도의 불변성, scale invariance**)

참고로 주성분 분석은  공분산 행렬을 이용하는 경우와  상관계수 행렬을 이용하는 경우 척도가 달라지기 떄문에 동일한 결과를 얻을 수 없다. 

### 인자의 비유일성

임의의 $k \times k$의 직교행렬 $\pmb P$을 생각하고 인자모형을 변환하여 보자 

$$ 
\begin{aligned}
\pmb X & = \pmb \Lambda \pmb f + \pmb u \\
  & = \pmb \Lambda \pmb P  \pmb P^t \pmb f + \pmb u \\
  & = (\pmb \Lambda \pmb P) (\pmb P^t \pmb f) + \pmb u \\
  & = \pmb \Lambda_1 \pmb f_1 + \pmb u 
\end{aligned}
$$
여기서 $\pmb \Lambda_1 = \pmb \Lambda \pmb P$이고 $\pmb f_1 = \pmb P^t \pmb f$이다. 위의 새로운 인자 모형에서의 인자 $\pmb f_1$의 분포와 관측변수 $\pmb X$의 공분산은 
윈래 인자 $\pmb f$의 분포와 동일한다. 

$$ 
E(\pmb f_1) =E( \pmb P^t \pmb f)=\pmb P^t E(\pmb f) =0, \quad
Cov(\pmb f_1)=\pmb P^t Var(\pmb f) \pmb P=\pmb P^t \pmb P =\pmb I 
$$

따라서 새로운 인자 모형에서 관측벡터 $\pmb X$의 공분산은 다음과 같다.

$$ 
Cov( \pmb X) = Cov ({\pmb \Lambda}_1 {\pmb f}_1 + \pmb u) = Cov ({\pmb \Lambda} {\pmb f} + \pmb u) = \pmb \Lambda \pmb \Lambda^t + \pmb \Psi 
$$
 
따라서 인자적재값은 같은 자료라도 유일하게 존재하지 않는다(non-uniquesness). 더나아가 인자의 적재행렬에 제한 조건을 주면 유일하게 존재할 수 있다(인자의 회전; factor rotation)


## 모형의 추정

이제 인자 모형을 추정하는 방법에 대해서 알아보자.

### 단순 인자모형

먼저 @eq-multivar-factor-one-factor 에 나타난 단순 인자 모형과 공분산에 대한 결과 @eq-multivar-factor-one-factor-cov 를 고려하면 다음과 같은 방정식을 유도할 수 있다.


$$
\begin{aligned}
\pmb R & = \pmb \Lambda \pmb \Lambda^t + \pmb \Psi \\
& = 
\begin{bmatrix}
\lambda_1 \\
\lambda_2 \\
\lambda_3 \\
\end{bmatrix}
\begin{bmatrix}
\lambda_1 & \lambda_2 & \lambda_3 \\
\end{bmatrix}
+
\begin{bmatrix}
\psi_1 & 0 & 0 \\
0 & \psi_2 & 0 \\
0 & 0 & \psi_3 \\
\end{bmatrix} \\
& =
\begin{bmatrix}
\lambda_1^2 + \psi_1 & \lambda_1 \lambda_2 & \lambda_1 \lambda_3 \\
\lambda_2 \lambda_1 & \lambda_2^2 + \psi_2 & \lambda_2 \lambda_3 \\
\lambda_3 \lambda_1 & \lambda_3 \lambda_2 & \lambda_3^2 + \psi_3 \\
\end{bmatrix}
\end{aligned}
$$
실제 관측한 3개의 시험점수의 상관행렬 $\pmb R$를 위의 식에 대입하면 다음과 같은 방정식을 얻는다.

$$
\begin{aligned}
\lambda_1 \lambda_2 & = 0.83 \\
\lambda_1 \lambda_3 & = 0.78 \\
\lambda_2 \lambda_3 & = 0.67 \\
\lambda_1^2 + \psi_1 & = 1 \\
\lambda_2^2 + \psi_2 & = 1 \\   
\lambda_3^2 + \psi_3 & = 1 \\
\end{aligned}
$$

위의 비선형 방정식 시스템을 풀면 다음과 같은 해를 얻는다. 이러한 해로 @eq-multivar-factor-one-factor 에 나타난 단순 인자 모형의 인자와 분산성분을 추정할 수 있다.

$$
\begin{aligned}
\hat \lambda_1 & = 0.99, \quad \hat \psi_1 = 0.02 \\
\hat \lambda_2 & = 0.84, \quad \hat \psi_2 = 0.3 \\
\hat \lambda_3 & = 0.79, \quad \hat \psi_3 = 0.38 \\
\end{aligned}
$$

### 최대 가능도 추정법

탐색적 인자분석에서 인자 적재값과 특수분산을 추정하는 방법으로 최대 가능도 추정법(Maximum Likelihood Estimation; MLE)을 사용할 수 있다. 최대 가능도 추정법은 관측된 데이터가 주어진 모형에 의해 생성될 확률을 최대화하는 모수 값을 찾는 방법이다.

다변량 정규분포 모형에서 분산 행렬 $\pmb \Sigma$가 인자 모형에 의해 @eq-multivar-factor-k-factor-cov 과 같이 나타난다고 가정하고 최대 가능도 추정법을 적용할 수 있다.

이 장에서는 최대 가능도 추정법에 대한 자세한 설명은 생략하고 주성분 분석을 이용한 간단한 모형 추정법을 다음 절에 소개하려고 한다.


### 주성분 인자분석 

탐색적 인자분석에서 요인의 초기 추정값을 얻는 방법으로 주성분분석을 사용할 수 있다.
주성분분석를 통해 얻은 상위 몇 개의 주성분이 데이터의 분산을 대부분 설명한다면,
이 주성분들을 잠재요인의 초기 근사치로 사용할 수 있다.

이러한 접근 방법을 주성분 인자분석(Principal Component Factor Analysis) 이라고 부른다.

$q$-차원의 임의 벡터 $\pmb X^t=(X_1,X_2,\dots, X_q)$ 가 평균이 0이고 공분산이 $\pmb \Sigma$이라 가정하자.


먼저 표본 공분산 행렬 $\hat {\pmb \Sigma}$을 이용하여 주성분 분석을 실시한다.

$$
\begin{aligned}
Z_1   &= a_{11} X_1 + a_{12} X_2 + \dots a_{1q} X_q   \\
Z_2  &= a_{21} X_1 + a_{22} X_2 + \dots a_{2q} X_q   \\
  &    \dots \\
Z_q   &= a_{q1} X_1 + a_{q2} X_2 + \dots a_{qq} X_q   \\
\end{aligned}
$$
위의 주성분 분석의 결과를 행렬식으로 나타내면 다음과 같다.

$$ \pmb Z = \pmb A \pmb X $$


행렬 $\pmb A$의 역행렬은 @sec-multivar-pca-coef 에 의하여 표본 공분산 행렬에 대한 고유벡터 행렬이므로 직교행렬이다 ($\pmb A \pmb A^t = \pmb I$). 따라서 다음과 같은 역변환을 고려한다. 

$$ \pmb X = \pmb A^t \pmb Z $$
위의 식을 다시 쓰면 다음과 같다.

$$
\begin{aligned}
X_1   &= a_{11} Z_1 + a_{21} Z_2 + \dots a_{q1} Z_q   \\
X_2  &= a_{12} Z_1 + a_{22} Z_2 + \dots a_{q2} Z_q   \\
  &    \dots \\
X_q   &= a_{1q} Z_1 + a_{2q} Z_2 + \dots a_{qq} Z_q   \\
\end{aligned}
$$
이제 $q$ 개의 주성분들 중에서 $k<q$개의 주성분을 선택한다. 선택된 주성분을 인자로
생각할 수 있다.
$$
 \begin{aligned}
X_1   &= a_{11} Z_1 + a_{21} Z_2 + \dots a_{k1} Z_k +u_1   \\
X_2  &= a_{12} Z_1 + a_{22} Z_2 + \dots a_{k2} Z_k + u_2  \\
  &    \dots \\
X_q   &= a_{1q} Z_1 + a_{2q} Z_2 + \dots a_{kq} Z_k  + u_3 \\
\end{aligned}
$$ {#eq-multivar-factor-pca-factor}

여기서  유의할 점은 $u_i$ 는 나머지 주성분 $Z_{k+1}, Z_{k+2},\dots ,Z_q$의 선형조합으로 사실 서로 독립은 아니다.


마지막으로 인자의 분포 조건에 만추기 위하여 $Z_i$들을 표준화한다. $i$ 번째 주상분의 분산은 표본 공분산 행렬의 $i$ 번째 고유값이므로 ($Var(Z_i)=\lambda_i$) 각 $Z_i$를 
고유값의 제곱근으로 나누어 주고 각 계수에 곱해준다. 

$$ 
f_i = Z_i /\sqrt{\lambda_i}, \quad \lambda_{ij} = \sqrt{\lambda_i} a_{ji} 
$$

이렇게 표준화하면 다음과 같이 주성분을 이용한 인자모형을 구할 수 있다

$$
\begin{aligned}
X_1   &= \lambda_{11} f_1 + \lambda_{12} f_2 + \dots \lambda_{1k} f_k +u_1  \\
X_2   &= \lambda_{21} f_1 + \lambda_{22} f_2 + \dots \lambda_{2k} f_k +u_2  \\
  &    \dots \\
X_q   &= \lambda_{q1} f_1 + \lambda_{q2} f_2 + \dots \lambda_{qk} f_k +u_q  \\
\end{aligned}
$$







