---
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| echo: true
#| message: false
#| warning: false


library(tidyverse)
library(here)
library(knitr)
library(mvtnorm)

#아래 3 문장은 한글을 포함한 ggplot 그림이 포함된 HTML, PDF로 만드는 경우 사용
library(showtext)
#font_add_google("Nanum Pen Script", "gl")
font_add_google(name = "Noto Sans KR", family = "noto")
showtext_auto()
```




# 군집 분석

군집 분석(clustering)은 다변량 관측값들 사이의 유사성(또는 거리)을 기반으로 관측값들을 그룹화하는 방법이다. 군집분석은 다음과 같은 특징을 가진다.

- 군집의 개수나 각 대상의 소속 집단에 대해 사전 가정을 두지 않는다.
- 각 군집의 형태나 구조에 대한 가정도 필요하지 않다.
- 비지도학습(unsupervised learning) 방법으로, 교사의 역활이 없는 상태에서 스스로 학습한다.
- 여러 변수들의 값을 이용하여 대상들을 유사한 특성을 가진 집단으로 분류하는 알고리즘을 구성한다.
- 유사한 특성을 가진 개체들은 동일한 군집에 포함된다.
- 개체 간의 유사성 또는 차이를 판단하기 위해 거리(distance) 또는 유사도(similarity)의 개념을 사용한다.


군집분석은 계층적 군집분석(hierarchical clustering)과 비계층적 군집분석(non-hierarchical clustering)으로 나뉘며 두 방법에 대한 차이는 다음과 같다. 

- 계층적 군집분석 

  + 군집 간의 포함 관계를 단계적으로 형성해 나가는 방법이다.
  + 처음에는 각 개체를 하나의 군집으로 두고, 가까운 군집끼리 차례로 병합(병합적 방법)하거나, 반대로 전체를 하나의 군집으로 두고 점차 분할(분할적 방법)하는 방식으로 진행된다.
  + 사전에 군집의 개수를 정할 필요가 없지만, 해석 단계에서 적절한 군집 수를 선택해야 한다.

- 비계층적 군집분석 

  + 군집의 개수를 미리 정해두고, 개체들을 그 개수만큼의 군집에 나누는 방법이다.
  + 대표적인 예로 K-평균(K-means) 군집분석이 있다.
  + 초기 군집 중심을 정한 뒤, 개체들을 가장 가까운 중심에 배정하고, 군집 중심을 다시 계산하는 과정을 반복(iteration)하면서 군집을 안정화시킨다.
  + 대규모 자료에 적합하지만, 초기 설정(군집 수, 중심점)에 따라 결과가 달라질 수 있다.


먼저 군집 분석에 사용할 수 있는 다양한 거리의 정의와 성질에 대해 알아보고, 이후에 계층적 군집 분석과 비계층적 군집 분석 방법을 살펴본다.

## 거리의 정의와 성질

### 연속형 변수 

우리는 세상을 수많은 요소를 통해 관찰한다. 사람을 구분할 때 외모, 언어, 문화, 지리적 배경 같은 요인이 서로 다른 차이를 만들어낸다. 한국인과 일본인은 서로 가까워 보이지만, 아프리카인과는 그 차이가 훨씬 크다고 느낀다. 이런 차이의 크기를 수학적으로 정의한 것이 바로 **거리(distance)**다. 이번 절에서는 다변량 변수를 다루는 경우 개별 관측값들 사이의 거리를 어떻게 정의할 수 있는지 다양한 거리의 정의와 성질을 알아본다.

먼저 p-차원 확률 벡터 $\pmb X$ 를 생각하고 $n$ 개의 관측벡터가 있다고 가정하자.

$$ \pmb X_1, \pmb X_2, \ldots, \pmb X_n $$
여기서 $\pmb X_i^t = (X_{i1}, X_{i2}, \dots, X_{ip})$ 이다.


**유클리디안 거리 (Euclidean distance)**는 우리가 일상에서 사용하는 거리 개념과 같다. $p$-차원 공간에서 두 관측 벡터 $\pmb X_1$ 과 $\pmb X_2$  사이의 유클리디안 거리는 다음과 같이 정의된다.

$$
d(\pmb X_1, \pmb X_2) = \sqrt{ (\pmb X_1 - \pmb X_2)^t (\pmb X_1 - \pmb X_2) } = \sqrt{\sum_{k=1}^p (X_{1k} - X_{2k})^2} 
$$


유클리디안 거리는 실수 공간상의 거리이므로 다변량 변수들이 연속인 경우에 적용되는 거리이다. 


유클리드 거리는 확률 변수들 간의 상관관계나 분산의 차이(불확실성의 차이)를 고려하지 않는다. 예를 들어, 두 변수들이 서로 다른 분산를 가진다면, 두 변수의 관측값을 이용한 거리가 같더라도 확률적 불확실성이 다를 수 있다.

예를 들어 두 확률 변수의 분산이 각각 1 과 10 이라고 한다면 두 개의 관측값에 의한 계산된 유클리디안 거리, 즉 대수적인 차이가 같더라고 차이에에 대한 확률적인 불확실성은 다르다. 

유클리디안 거리를 확률적으로 보완하기 위해 통계학에서는 **통계적 거리(statistical distance)** 또는  **마할라노비스 거리(Mahalanobis distance)** 개념을 도입한다. 통계적 거리는 변수들 간의 상관관계와 분산을 고려하여 거리를 계산한다. 만약 확률벡터 $\pmb X$ 의 공분산 행렬을 $\pmb V$ 라고 하면 두 관측 벡터  $\pmb X_1$ 과 $\pmb X_2$  사이의 통계적 거리는 다음과 같이 정의된다.

$$
d_M(\pmb X_1, \pmb X_2) = \sqrt{ (\pmb X_1 - \pmb X_2)^t \pmb V^{-1} (\pmb X_1 - \pmb X_2) }
$$

::: {.callout-note}

#### 표준화

유클리디안 거리는 측정값의 단위(scale)에 따라서 달라진다. 예를 들어, 어떤 사람의 키(cm)와 치아의 길이(mm)를 측정했다고 하자. 키의 변동폭은 20~30mm, 치아 길이의 변동폭은 1~2mm 정도다. 이때 단순히 두 변수의 차이를 이용하면, 거리는 대부분 키의 차이에 의해 좌우된다. 그래서 모든 변수가 거리 계산에 비슷한 영향을 주도록 하기 위해서 **표준화(standardization)**가 필요하다. 표준화는 각 변수에서 평군값을 뺴고 그 표준편차로 나누는 작업이며 표준화된 변수의 평균은 0 이고 분산은 1 이다.

$$
Z_{ik} = \frac{X_{ik} - \bar{X}_k}{s_k} 
$$

여기서  $\bar X_k$ 는  $k$-번쨰 변수의 표본 평균이고  $s_k$ 는 표본 표준편차이다. 표준화된 변수를 이용하여 유클리디안 거리를 계산하면, 각 변수가 거리 계산에 동일한 영향을 미치게 된다. 

::: 

거리의 기본 성질은 다음과 같다.

1. 비음수성(Non-negativity): 모든 관측값 쌍에 대하여 거리는 0 이상이다.

$$
d(\pmb X_i, \pmb X_j) \geq 0
$$

2. 대칭성(Symmetry): 두 관측값 사이의 거리는 순서에 상관없이 같다. 

$$
d(\pmb X_i, \pmb X_j) = d(\pmb X_j, \pmb X_i)
$$

3. 삼각 부등식(Triangle inequality): 세 관측값 $\pmb X_i$, $\pmb X_j$, $\pmb X_k$에 대하여 다음을 만족한다.

$$
d(\pmb X_i, \pmb X_k) \leq d(\pmb X_i, \pmb X_j) + d(\pmb X_j, \pmb X_k)
$$

### 이항 변수

이항 변수(binary variable)는 두 가지 값(예: 0과 1 또느 성걸과 실패)만을 가질 수 있는 변수이다. 예를 들어, 성별(남성/여성), 흡연 여부(흡연자/비흡연자) 등이 이항 변수에 해당한다. 이항 변수로 구성된 다변량 데이터에서 거리 계산을 할 때는 앞에서 살펴보 유클리디안 거리와 다른 거리의 개념을 적용할 필요가 있다.

에를 들어 마케팅 조사에서 고객 성향을 분석하기 위하여 10개의 항목에 대한 선호도 조사를 한다고 하자. 각 항목을  좋아하면 1, 좋아하지 않으면 0 이라고 하자. 아래 표에 2 명에 대한 조사결과가 있다면 10 개의 선호도 항목에 대한 자료로 두 사람이 얼마나 가까운지 어떻게 거리를 계산할 수 있을까? 참고로 아래와 같은 자료의 형식은 출현–부재 자료 (present-absence data)라고 부른다. 

| 항목 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |
|------|---|---|---|---|---|---|---|---|---|----|
| 사람 1 ($\pmb X_1$)  | 0 | 0 | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 0 |
| 사람 2 ($\pmb X_2$) | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 1 |

위의 선호도 조사에서 얻은 자료를 다음과 같은 교차표로 정리할 수 있다.

|           | 사람 1 -좋아함 | 사람 1 싫어함 | 합계 |
|-----------|---------------|---------------|------|
| 사람 2 좋아함 | a  (3)           | b (3)             | a+b (6)  |
| 사람 2 싫어함 | c  (3)           | d (1)            | c+d  (4) |
| 합계      | a+c   (6)        | b+d  (4)         | n (10)    |

여기서,

- $a$: 두 사람 모두 좋아하는 항목의 수
- $b$: 사람 2는 좋아하지만 사람 1은 싫어하는 항목의 수
- $c$: 사람 1은 좋아하지만 사람 2는 싫어하는 항목의 수
- $d$: 두 사람 모두 싫어하는 항목의 수
- $n$: 전체 항목의 수 ($n = a + b + c + d$)

이항 변수를 포함한 다변량 데이터에서 거리 계산을 위해 다음과 같은 거리 척도들이 사용된다.

1. 단순 유사도 지표(simple matching index) : 전체 항목의 개수 중에서 이항변수의 관측값이 동일한 비율로 계산한다. 

$$
d(\pmb X_1, \pmb X_2) = \frac{a + d }{n}
$$

2. 자카드 거리(Jaccard distance): 두 관측값이 모두 좋아하는 항목의 수에 대한 비율로 계산된다. 자카드 거리는 출현-부재 자료에서 서로에게 공통으로 존재하지 않는 항목이 반드시 두 개체의 유사성을 반영할 지 못 할 수 있는 경우를 고려한 거리이다. 예를 들어 두 사람이 모두 축구를 좋아할 경우와 모두 싫어할 경우 유사도가 다를 수 있다. 즉, 축구를 모두 좋아한다면 두 사람이 가깝다고 판단할 수 있지만 둘 다 싫어하는 경우 반드시 두 사람이 가깝지 않을 수 있다. 


$$
d(\pmb X_1, \pmb X_2) = \frac{a}{a + b + c}
$$



3. 다이스 거리(Dice distance): 자카드 거리와 유사하지만, 공통으로 좋아하는 항목의 수에 2를 곱하여 계산된다. 서로 동시에 존재하는 항목의 중요성을 강조한 거리이다.

$$
d(\pmb X_1, \pmb X_2) = \frac{2a}{2a + b + c}
$$

4. 오치아이 거리(Ochiai distance): 두 관측값이 모두 좋아하는 항목의 수를 두 관측값이 좋아하는 항목의 수를 $(a+b)(a+c)$의 제곱근으로 나누어 계산된 거리이다. 오치아이 거리는 생태학에서 종간 공동으로 출현하는 유사도, 정보검색의 관점에서 보는 유사한 정도을 거리형식으로 측정할 때 자주 사용하는 거리이다.

$$
d(\pmb X_1, \pmb X_2) = \frac{a}{\sqrt{(a+b)(a+c)}}
$$




이항변수로 이루어진 다변량 자료에 대한  거리는 단순히 수학적 계산으로 표현되는 추상적 개념이 아니다. 이는 복잡한 현실을 수량적으로 비교하고 이해하기 위한 하나의 언어적 체계로서 기능한다. 서로 다른 사람, 집단, 생태계, 혹은 시장과 같은 다양한 대상들은 이러한 거리 개념을 통해 하나의 공통된 축 위에서 비교되고 분석될 수 있다.

::: {.callout-caution}

이항변수로 이루어진 출현-부재 자료에서 정의된 거리는 공간상에서 정의된 거리의 성질을 만족하지 않을 수 있디.

:::



## 계층적 군집 분석

계층적 군집 분석은 방법은 데이터 내에 존재하는 자연스러운 군집 구조를 발견하는 데 유용하다. 계층적 군집 분석은 크게 두 가지 접근법으로 나눌 수 있다: 병합적 접근법(agglomerative approach)과 분할적 접근법(divisive approach)이다.

이 중 병합적 방법은 가장 널리 사용되는 형태로, 각 개체를 하나의 군집으로 시작하여 점차 유사한 군집끼리 결합해 가는 방식이다. 군집 간 거리를 계산하고, 가장 가까운 군집들을 반복적으로 병합한다. 결과는 덴드로그램(dendrogram)이라는 나무 모양의 그래프로 표현되어, 군집 간 결합 구조와 거리 수준을 시각적으로 보여준다.


먼저 거리 행렬의 예를 살펴보자.  5개의 개체들에 대하여 각각의 개체들 사이에 모든 거리가 존재하면 다음과 같은 거리 행렬(distance matrix)를 만들 수 있다.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-multivar-cluster-01
#| fig.cap: "거리 행렬"
#| fig.align: "center"
#| fig.width: 3
#| fig.height: 3

knitr::include_graphics(here("images", "cluster-01.png"))
```



여기서 주의할 점은 거리 행렬은 대칭행렬(symmetric matrix)이며, 거리는 주어진 상황에 따라서 적절한 거리 계산 방법을 선택하여 계산한다. 



주어진 거리행렬에 대하여 병합적 계층적 군집화 알고리즘의 기본 절차는 다음과 같다.

1. **초기화** 

   * $N$ 개의 개체(observation)를 하나의 독립된 군집으로 둔다.
   * 초기 상태에서 군집의 수는 개체의 수와 동일하다.

2. **거리 행렬 계산** 

   * 모든 개체(또는 군집) 간의 거리 또는 유사도를 계산한다.
   * 거리 행렬의 차원은 $N \times N$ 이다.
   * $i$번째 개체와 $j$번째 개체 사이의 거리를 $d_{ij}$로 표기한다.
   
3. **가장 가까운 두 군집 선택** 

   * 거리 행렬 $\pmb D = \{ d_{ij} \}$ 에서 가장 거리가 작은 두 군집을 찾는다.

4. **군집 병합** 

   * 선택된 두 군집을 하나의 군집으로 합친다.
   * 전체 군집의 수는 한 단계 줄어든다.

5. **거리 행렬 갱신** (Update Distance Matrix)

   * 새로 형성된 군집과 나머지 군집들 간의 거리를 다시 계산한다.
   * 이때 군집 간 거리를 정의하는 방식에 따라 다양한 방법이 존재한다

6. **반복** (Iterate)

   * 위의 과정을 군집의 수가 1이 될 때까지 반복한다.


새로운 군집을 형성할 때, 군집 간에 거리를 계산 방식을 연결 방법(Linkage Methods)이라고 하며 여러 가지가 있다.

이제 3 개의 군집 $u, v, w$ 가 있다고 하자. 군집 $u$ 와 $v$ 를 병합하여 새로운 군집 $[uv]$ 를 형성할 때, 새로운 군집 $[uv]$ 와 다른 군집 $w$ 사이의 거리를 계산하는 방법은 다음과 같다.



- 최단연결법 (Single linkage): 두 군집 사이의 최소 거리로 정의된다.

$$ d([uv], w) = \min(d(u, w), d(v, w)) $$

- 최장연결법 (Complete linkage): 두 군집 사이의 최대 거리로 정의된다.

$$ d([uv], w) = \max(d(u, w), d(v, w)) $$

- 평균연결법 (Average linkage): 두 군집 내 모든 쌍의 평균 거리로 정의된다.

$$ d([uv], w) = \frac{\sum_m \sum_n d(m,n)} { N_{[uv]} N_w} $$
위의 식에서 $m$은 군집 $[uv]$에 속한 개체, $n$은 군집 $w$에 속한 개체를 나타내며, $N_{[uv]}$와 $N_w$는 각각 군집 $[uv]$와 군집 $w$에 속한 개체의 수이다.


이제 @fig-multivar-cluster-01 에 제시된 거리 행렬을 이용하여 계층적 군집 분석을 수행하는 과정을 위에서 제시한 3 가지 방법으로 살펴보자.

### 최단 연결법

먼저 @fig-multivar-cluster-01 에 제시된 거리 행렬에서 가장 작은 거리를 찾는다. 이 경우, 개체 3과 개체 5 사이의 거리가 2 로 가장 작다. 따라서 개체 3과 개체 5를 병합하여 새로운 군집 [3,5]를 형성한다.

다음으로, 새로운 군집 [3,5]와 나머지 개체들(1, 2, 4) 사이의 거리를 계산한다. 최단연결법을 사용하므로, 군집 [3,5]와 개체 1 사이의 거리는 다음과 같이 계산된다.

$$
d([3,5], 1) = \min(d(3, 1), d(5, 1)) = \min(3, 11) = 3
$$
또한, 군집 [3,5]와 개체 2 사이의 거리는 다음과 같이 계산된다.

$$
d([3,5], 2) = \min(d(3, 2), d(5, 2)) = \min(7, 10) = 7
$$
이제 갱신된 거리 행렬은 다음과 같다.


```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-multivar-cluster-02
#| fig.cap: "최단 연결법 1차 작업에 의하여 갱신된 거리행렬"
#| fig.align: "center"
#| fig.width: 3
#| fig.height: 3

knitr::include_graphics(here("images", "cluster-02.png"))
```

이제 @fig-multivar-cluster-02 에 의한 거리 행렬에서 가장 작은 거리를 보이는 군집 $[3,5]$ 와 군집 $1$ 을 통합하고 다시 거리를 계산한다. 예를 들어 통합된 군집 $[1,3,5]$ 와 군집 $2$ 의 거리는 다음과 같이 계산된다.

$$
d([1,3,5], 2) = \min(d([3,5], 2), d(1, 2)) = \min(7,9)) = 7
$$

또한 통합된 군집 $[1,3,5]$ 와 군집 $4$ 의 거리는 다음과 같이 계산된다.

$$
d([1,3,5], 4) = \min(d([3,5], 4), d(1, 4)) = \min(8,6) = 6
$$

이제 다시 갱신된 거리 행렬은 다음과 같다.


```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-multivar-cluster-03
#| fig.cap: "최단 연결법 2차 작업에 의하여 갱신된 거리행렬"
#| fig.align: "center"
#| fig.width: 3
#| fig.height: 3

knitr::include_graphics(here("images", "cluster-03.png"))
```


다음으로 @fig-multivar-cluster-03 에 제시된 거리행렬에서 가장 작은 거리를 가진 군집 2 와 군집 4 를 병합한다. 이제 군집 $[1,3,5]$ 와 군집 $[2,4]$ 의 거리는 다음과 같이 계산할 수 있다.

$$
d([1,3,5], [2,4]) = \min(d([1,3,5], 2), d([1,3,5], 4)) = \min(7,6) = 6
$$

이제 최종 갱신된 거리 행렬은 다음과 같다.



```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-multivar-cluster-04
#| fig.cap: "최단 연결법 3차 작업에 의하여 갱신된 거리행렬"
#| fig.align: "center"
#| fig.width: 3
#| fig.height: 3

knitr::include_graphics(here("images", "cluster-04.png"))
```

이제 2 개의 군집만 존재하므로 두 개를 병합하면  최종적으로 모든 개체를 하나의 군집으로 병합된다. 

### 최장 연결법 

최장 연결법(Complete linkage)은 두 군집 사이의 최대 거리를 기준으로 군집 간 거리를 정의하는 방법이다. 이 방법은 군집 내의 모든 개체들이 서로 가까워지도록 하는 경향이 있다.

최장 연결법을 사용하여 @fig-multivar-cluster-01 에 제시된 거리 행렬을 이용하여 계층적 군집 분석을 수행하는 과정을 살펴보자.

먼저 거리 행렬에서 가장 작은 거리를 찾는다. 이 경우, 개체 3과 개체 5 사이의 거리가 2로 가장 작다. 따라서 개체 3과 개체 5를 병합하여 새로운 군집 [3,5]를 형성한다.

다음으로, 새로운 군집 [3,5]와 나머지 개체들(1, 2, 4) 사이의 거리를 계산한다.최장 연결법을 사용하므로, 군집 [3,5]와 개체 1 사이의 거리는 다음과 같이 계산된다.

$$
d([3,5], 1) = \max(d(3, 1), d(5, 1)) = \max(3, 11) = 11
$$
또한, 군집 [3,5]와 개체 2 사이의 거리는 다음과 같이 계산된다.

$$
d([3,5], 2) = \max(d(3, 2), d(5, 2)) = \max(7, 10) = 10
$$
이제 갱신된 거리 행렬은 다음과 같다.


```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-multivar-cluster-05
#| fig.cap: "최장 연결법 1차 작업에 의하여 갱신된 거리행렬"
#| fig.align: "center"
#| fig.width: 3
#| fig.height: 3

knitr::include_graphics(here("images", "cluster-05.png"))
```

다음으로 @fig-multivar-cluster-05 에 의한 거리행렬에서 가장 작은 거리를 보이는 군집 2 와 군집 4 을 통합하고 다시 거리를 계산한다. 예를 들어 통합된 군집 $[3,5]$ 와 군집 $[2,4]$ 의 거리는 다음과 같이 계산된다.


$$
d([2,4],[3,5]) = \max(d(2,[3,5]), d(4,[3,5])) = \max(10,9)) = 10
$$

또한 군집 $[2,4]$ 와 군집 1 의 거리는 다음과 같다.

$$
d([2,4],1) = \max(d(2,1), d(4,1)) = \max(9,6)) = 9
$$

이제 모든 거리를 갱신하면 다음과 같은 거리행렬을 얻는다.


```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-multivar-cluster-06
#| fig.cap: "최장 연결법 2차 작업에 의하여 갱신된 거리행렬"
#| fig.align: "center"
#| fig.width: 3
#| fig.height: 3

knitr::include_graphics(here("images", "cluster-06.png"))
```

다음으로 @fig-multivar-cluster-06 에 제시된 거리행렬에서 가장 작은 거리를 보이는 군집  $[2,4]$ 와 군집 1 을 병합한다. 이제 군집 $[1,2,4]$ 와 군집 $[3,5]$ 의 거리는 다음과 같이 계산할 수 있다.

$$
d([1,2,4], [3,5]) = \max(d(1,[3,5]), d([2,4],[3,5])) = \max(11,10) = 11
$$


이제 최종 갱신된 거리 행렬은 다음과 같다.


```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-multivar-cluster-07
#| fig.cap: "최장 연결법 3차 작업에 의하여 갱신된 거리행렬"
#| fig.align: "center"
#| fig.width: 3
#| fig.height: 3

knitr::include_graphics(here("images", "cluster-07.png"))
```


마지막으로 군집 $[1,2,4]$ 와 $[3,5]$ 를 통합하면 된다.

### 평균연결법

마지막으로 평균연결법(Average linkage)은 두 군집 내 모든 쌍의 평균 거리로 군집 간 거리를 정의하는 방법이다. 이 방법은 군집 내의 개체들 간의 거리를 균형 있게 반영하여 군집을 형성하는 경향이 있다.


먼저 @fig-multivar-cluster-01 에 제시된 거리 행렬에서 가장 작은 거리를 찾는다. 이 경우, 개체 3과 개체 5 사이의 거리가 2 로 가장 작다. 따라서 개체 3과 개체 5를 병합하여 새로운 군집 [3,5]를 형성한다.

다음으로, 새로운 군집 [3,5]와 나머지 개체들(1, 2, 4) 사이의 거리를 계산한다. 평균 연결법을 사용하므로, 군집 [3,5]와 개체 1 사이의 거리는 다음과 같이 계산된다.

$$
d([3,5], 1) = mean(d(3, 1), d(5, 1)) = mean(3, 11) = \frac{3+11}{2} = 7
$$
또한, 군집 [3,5]와 개체 2 사이의 거리는 다음과 같이 계산된다.

$$
d([3,5], 2) = mean(d(3, 2), d(5, 2)) = mean(7,10) = \frac{7+10}{2} = 8.5
$$
이제 갱신된 거리 행렬은 다음과 같다.


```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-multivar-cluster-08
#| fig.cap: "평균 연결법 1차 작업에 의하여 갱신된 거리행렬"
#| fig.align: "center"
#| fig.width: 3
#| fig.height: 3

knitr::include_graphics(here("images", "cluster-08.png"))
```

이제 @fig-multivar-cluster-08 에 의한 거리행렬에서 가장 작은 거리를 보이는 군집 2 와 군집 4을 통합하고 다시 거리를 계산한다. 예를 들어 통합된 군집 $[2,4]$ 와 군집 $[3,5]$ 의 거리는 다음과 같이 계산된다.


$$
d([2,4],[3,5]) = mean (d_{2,3}, d_{2,5}, d_{4,3}, d_{4,5}) = mean(7,10,8,9) = \frac{7+10+8+9}{4} = 8.5
$$

평균 연결법은 위와 같이 거리를 두 든 조합을 고려하여 평균으로 거리를 츠측정하는 방법으로 군집 간의 거리를 계산한다. 이 과정을 반복하여 최종적으로 모든 개체가 하나의 군집으로 통합될 때까지 진행한다.

### 와드 방법 


와드 방법(Ward’s method)는 가장 자주 사용되는 방법으로 군집 내 분산의 증가량을 최소화하는 방식으로 군집을 병합한다. 군집이 결합될 때 발생하는 군집 내 제곱합(Within-Cluster Sum of Squares)의 증가를 최소화하는 방향으로 군집을 합친다.


### 요약과 예제

다음은 각 연결법의 방법과 특징을 요약한 표이다.


| 방법                           | 정의                 | 특징                           |
| ---------------------------- | ------------------ | ---------------------------- |
| 최단연결법   | 두 군집 사이의 최소 거리     | 긴 “사슬형(chain-like)” 군집 생성 가능 | 
| 최장연결법  | 두 군집 사이의 최대 거리     | 조밀하고 균일한 군집 형성                  |
| 평균연결법  | 두 군집 내 모든 쌍의 평균 거리 | 균형 잡힌 결과, 이상치에 덜 민감     
| 와드 방법    | 군집 내 제곱합 증가량 최소화   | 분산 기반, 매우 안정적이고 자주 사용됨       |


분할적 계층적 군집 방법(Divisive Hierarchical Methods)은 병합적 계층적 군집 방법(Agglomerative Hierarchical Methods)에 비해 상대적으로 덜 사용되어 왔다. 군집분석의 성능은 군집의 형태, 군집들이 서로 얼마나 겹쳐 있는가에 크게 의존한다.


- 거리 행렬 만들기

```{r}
x <- c(9,3,6,11,7,5,10,9,2,8)
dd <- matrix(0,5,5)
k<-1
for (i in 1:4) {
  for ( j in (i+1):5) {
    dd[i,j] = x[k]
    dd[j,i] = x[k]
    k<-k+1
  }
  dd[i,i]=0
}
d <- as.dist(dd)
d
```

- 최단연결법

```{r}
re1 <- hclust(d, method="single")
plot(re1)
```

- 최장연결법

```{r}
re2 <- hclust(d, method="complete")
plot(re2)
```

- 평균연결법

```{r}
re3 <- hclust(d, method="average")
plot(re3)
```

- 와드 방법(Ward's method)

```{r}
re4 <- hclust(d, method="ward.D2")
plot(re4)
```

#### 신체치수 자료

 20명의 가슴, 허리, 엉덩이, 성별  측정 자료에 계층적 군집분석을 적용해 보자.

1. 자료의 생성

```{r}
measure <-
structure(list(V1 = 1:20, V2 = c(34L, 37L, 38L, 36L, 38L, 43L, 
40L, 38L, 40L, 41L, 36L, 36L, 34L, 33L, 36L, 37L, 34L, 36L, 38L, 
35L), V3 = c(30L, 32L, 30L, 33L, 29L, 32L, 33L, 30L, 30L, 32L, 
24L, 25L, 24L, 22L, 26L, 26L, 25L, 26L, 28L, 23L), V4 = c(32L, 
37L, 36L, 39L, 33L, 38L, 42L, 40L, 37L, 39L, 35L, 37L, 37L, 34L, 
38L, 37L, 38L, 37L, 40L, 35L)), .Names = c("V1", "V2", "V3", 
"V4"), class = "data.frame", row.names = c(NA, -20L))
measure <- measure[,-1]
names(measure) <- c("chest", "waist", "hips")
measure$gender <- gl(2, 10)
levels(measure$gender) <- c("male", "female")
head(measure)
```

2. 거리행렬의 생성

```{r}
dm <- dist(measure[, c("chest", "waist", "hips")])
round(dm, 2)
```

3. 최단연결법(single linkage)과 덴드로그램

```{r}
re1 <- hclust(dm, method="single")
plot(re1)
```

4. 최장연결법(complete linkage)과 덴드로그램

```{r}
re2 <- hclust(dm, method="complete")
plot(re2)
```

5. 평균연결법(average linkage)과 덴드로그램

```{r}
re3 <- hclust(dm, method="average")
plot(re3)
```



#### 2개의 군집으로 나누었을 때 그림들

- 자료가 남자10명, 여자 10영으로 구성되어있기 때문에 2개의 군집으로 나누어 진다고 예상

- 덴드로그램 높이(y축, 거리)의 변화를 보고 2개의 군집으로 나눌 수 있는 기준(cut point)를 결정

- 예를 들어 최단거리법에서는 cut point = 3.8

- 자료에 주성분 분석을 실시하여 첫 번째와 두 번째 주성분을 이용해 삼포도를 그리고 군집분석의 결과를 나타낸다
 (1 과 2는 다른 군집)
 
- 최단거리법의 문제점: 연쇄현상(chaining)이 나타난다.

- 연쇄현상(chaining)은 두 개의 군집이 연결되어 있을 때 군집을 분리하지 못하는 현상  

```{r, fig.width=10, fig.height=10}
#| echo: false
#| message: false
#| warning: false


body_pc <- princomp(dm, cor = TRUE)
layout(matrix(1:6, nr = 2), height = c(1, 1))
plot(cs <- hclust(dm, method = "single"), main = "Single")
abline(h = 3.8, col = "red")
xlim <- range(body_pc$scores[,1])
plot(body_pc$scores[,1:2], type = "n", xlim = xlim, ylim = xlim,
     xlab = "PC1", ylab = "PC2")
lab <- cutree(cs, h = 3.8)
text(body_pc$scores[,1:2], labels = lab, cex=1.2)
plot(cc <- hclust(dm, method = "complete"), main = "Complete")
abline(h = 10, col = "red")
plot(body_pc$scores[,1:2], type = "n", xlim = xlim, ylim = xlim,
     xlab = "PC1", ylab = "PC2")
lab <- cutree(cc, h = 10)  
text(body_pc$scores[,1:2], labels = lab, cex=1.2)     
plot(ca <- hclust(dm, method = "average"), main = "Average")
abline(h = 7.8, col = "red")
plot(body_pc$scores[,1:2], type = "n", xlim = xlim, ylim = xlim,
     xlab = "PC1", ylab = "PC2")
lab <- cutree(ca, h = 7.8)                             
text(body_pc$scores[,1:2], labels = lab, cex=1.2)     
```

- 실제로 남자와 여자가 어떻게 배치되는지 보자.
- 최장연결법과 평균연결법은 남자와 여자를 잘 구분한다.

```{r}
#| echo: false
#| message: false
#| warning: false


plot(body_pc$scores[,1:2], type = "n", xlim = xlim, ylim = xlim,
     xlab = "PC1", ylab = "PC2")
lab <- cutree(cs, h = 3.8)
text(body_pc$scores[,1:2], labels = measure$gender, cex=1.0)
```

####  제트 전투기 군집화


- 22개 미국 전투기에 대한 6개 변수 

다음은  22개 미국 전투기에 대한 6개 변수들이며 군집분석에는 두 번째부터 다섯번째까지 4개의 변수를 이용하며 최장연결법을 적용한다.

+ `FED`: 처음비행날짜
+ `SPR`: 출력
+ `RGF`: 비행범위 요인 
+ `PLF`: 탑재량
+ `SLF`: 무게요인
+ `CAR`: 항모착륙가능여부

자료를 다음과 같이 만든다.

```{r}
jet <-
structure(list(V1 = c(82L, 89L, 101L, 107L, 115L, 122L, 127L,
137L, 147L, 166L, 174L, 175L, 177L, 184L, 187L, 189L, 194L, 197L,
201L, 204L, 255L, 328L), V2 = c(1.468, 1.605, 2.168, 2.054, 2.467,
1.294, 2.183, 2.426, 2.607, 4.567, 4.588, 3.618, 5.855, 2.898,
3.88, 0.455, 8.088, 6.502, 6.081, 7.105, 8.548, 6.321), V3 = c(3.3,
3.64, 4.87, 4.72, 4.11, 3.75, 3.97, 4.65, 3.84, 4.92, 3.82, 4.32,
4.53, 4.48, 5.39, 4.99, 4.5, 5.2, 5.65, 5.4, 4.2, 6.45), V4 = c(0.166,
0.154, 0.177, 0.275, 0.298, 0.15, 0, 0.117, 0.155, 0.138, 0.249,
0.143, 0.172, 0.178, 0.101, 0.008, 0.251, 0.366, 0.106, 0.089,
0.222, 0.187), V5 = c(0.1, 0.1, 2.9, 1.1, 1, 0.9, 2.4, 1.8, 2.3,
3.2, 3.5, 2.8, 2.5, 3, 3, 2.64, 2.7, 2.9, 2.9, 3.2, 2.9, 2),
    V6 = c(0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 1L,
    0L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L)), .Names = c("V1", "V2",
"V3", "V4", "V5", "V6"), class = "data.frame", row.names = c(NA,
-22L))                                                   
colnames(jet) <- c("FFD", "SPR", "RGF", "PLF", "SLF", "CAR")
rownames(jet) <- c("FH-1", "FJ-1", "F-86A", "F9F-2", "F-94A", "F3D-1", "F-89A",
                   "XF10F-1", "F9F-6", "F-100A", "F4D-1", "F11F-1",
                   "F-101A", "F3H-2", "F-102A", "F-8A", "F-104B",
                   "F-105B", "YF-107A", "F-106A", "F-4B", "F-111A")
jet$CAR <- factor(jet$CAR, labels = c("no", "yes"))
head(jet)
```

- 계층적 군집분석

```{r}
X <- scale(jet[, c("SPR", "RGF", "PLF", "SLF")],
           center = FALSE, scale = TRUE) 
dj <- dist(X)
plot(cc <- hclust(dj), main = "Jets clustering")
cc
```

- 2개의 주성분을 이용한 군집분석의 시각화

```{r}
pr <- prcomp(dj)$x[, 1:2]
plot(pr, pch = (1:2)[cutree(cc, k = 2)],
     col = c("black", "darkgrey")[jet$CAR], 
     xlim = range(pr) * c(1, 1.5))
legend("topright", col = c("black", "black", 
                           "darkgrey", "darkgrey"), 
       legend = c("1 / no", "2 / no", "1 / yes", "2 / yes"), 
       pch = c(1:2, 1:2), title = "Cluster / CAR", bty = "n")
```

