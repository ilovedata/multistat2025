[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "다변량통계학-2025년 2학기",
    "section": "",
    "text": "Preface\n이 책은 2025년 다변량통계학에 대한 온라인 교재입니다.\n\n\n\n\n\n\n표기법\n\n\n\n이 책에서 사용된 기호, 표기법, 프로그램의 규칙과 쓰임은 다음과 같습니다.\n\n스칼라(scalar)와 일변량 확률변수는 일반적으로 보통 글씨체의 소문자로 표기한다. 특별한 이유가 있는 경우 대문자로 표시할 것이다.\n벡터, 행렬, 다변량 확률벡터는 굵은 글씨체로 표기한다.\n통계 프로그램은 R을 이용하였다. 각 예제에 사용된 R 프로그램은 코드 상자를 열면 나타난다.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "qmd/multivar-dist.html",
    "href": "qmd/multivar-dist.html",
    "title": "1  확률벡터와 다변량 정규분포",
    "section": "",
    "text": "1.1 예제- 국민체력100\n다변량 자료(multivariate data)는 두 개 이상의 변수를 측정한 자료를 말합니다. 예를 들어, 학생들의 키와 몸무게, 시험 점수와 공부 시간, 나이와 소득 등이 다변량 자료에 해당합니다. 다변량 자료는 변수들 간의 관계를 분석하고 이해하는 데 중요한 역할을 합니다. 다변량 자료를 효과적으로 표현하고 분석하기 위해 다양한 그래프와 통계 기법이 사용됩니다. 이 장에서는 다변량 자료의 표현 방법과 분포를 이해하는 데 필요한 기본 개념과 도구들을 소개합니다.\n국민체력100은 국민의 체력증진과 건강증진을 위해 개발된 종합적인 체력측정 프로그램이다. 이 프로그램은 다양한 연령대와 성별에 맞춘 체력측정 항목을 포함하고 있으며, 이를 통해 개인의 체력 상태를 평가하고 개선할 수 있는 기회를 제공한다.\n이번 장에서는 청소년(13-18세) 남여 3000명에 대하여 2024년에 국민체력100 사업에서 측정한 자료를 예제로 사용하여 다변량 자료를 표현하는 방법들과 분포를 배울것이다.\n먼저 측정항목에 대한 설명에 대한 자료를 보자.\nload(here(\"data\", \"physical100.RData\"))\nls()\n\n[1] \"physical100_df\"      \"physical100_df_info\"\n먼저 데이터프레임 selected_var_df 에는 측정한 항목의 영문 변수이름(varname_eng), 종목의 설명(varname_kor), 측정분야(category_kor) 그리고 측정단위(unit) 가 다음과 같이 저장되어 있다.\nvarname_engvarname_korcategory_korunitheight신장신체구성cmweight체중신체구성kgbody_fat_pct체지방율신체구성grip_left악력_좌근력kggrip_right악력_우근력kgsit_forward앉아윗몸앞으로굽히기유연성cmillinois일리노이민첩성초hang_time청소년체공시간순발력초twall_timeTWALL_시간협응력초twall_errorsTWALL_실수협응력회twall_scoreTWALL_결과값협응력초bmiBMI신체구성rel_grip상대악력근력%abs_grip절대악력근력kg\n다음으로 청소년 3000명의 측정 자료의 일부는 다음과 같다.\nsexageheightweightbody_fat_pctgrip_leftsit_forward남성15166.568.026.331.922.1여성13166.445.522.020.010.2남성13163.244.711.722.0-2.0여성14156.944.726.917.3-5.0남성17175.778.116.752.218.5여성16167.274.537.125.912.0여성16162.057.337.121.6-4.5여성17169.175.039.821.60.1여성15160.956.833.125.1-8.0남성13162.857.618.039.628.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>확률벡터와 다변량 정규분포</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-dist.html#확률벡터와-기본-성질",
    "href": "qmd/multivar-dist.html#확률벡터와-기본-성질",
    "title": "1  확률벡터와 다변량 정규분포",
    "section": "1.2 확률벡터와 기본 성질",
    "text": "1.2 확률벡터와 기본 성질\n\n1.2.1 일변량 확률변수\n일변량 확률변수(random variable) \\(X\\)가 확률밀도함수 \\(f(x)\\)를 가지는 분포를 따를때 기대값과 분산은 다음과 같이 정의된다.\n\\[\n\\begin{aligned}\nE(X) & = \\int x f(x)  dx = \\mu\\\\\nV(X) & = E[ X-E(X)]^2=\\int (x-\\mu)^2 f(x) dx =\\sigma^2\n\\end{aligned}\n\\]\n새로운 확률변수 \\(Y\\)가 확률변수 \\(X\\)의 다음과 같은 선형변환으로 표시된다면 (\\(a\\)와 \\(b\\)는 실수)\n\\[ Y = aX+b \\]\n일변량 확률변수 \\(X\\)의 기대값(평균)과 분산은 다음과 같이 계산된다.\n\\[\n\\begin{aligned}\nE(Y) & = E(aX+b) \\\\\n& = \\int (ax+b) f(x) dx \\\\\n& = a \\int x f(x) dx + b \\\\\n& = a E(X) + b\\\\\n& = a \\mu + b\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nV(Y) & = Var(aX+b) \\\\\n& = E[aX+b -E(aX+b)]^2 \\\\\n& = E[a(X-\\mu)]^2 \\\\\n& = a^2 E(X-\\mu)^2\\\\\n& = a^2 \\sigma^2\n\\end{aligned}\n\\]\n\n\n1.2.2 다변량 확률벡터\n이제 하나의 학률변수가 아는 2개 이상의 확률변수들을 모아놓은 확률벡터(random vector)를 생각해 보자. 다음과 같이 벡터로 표현된 확률벡터 \\(\\pmb X\\)가 \\(p\\) 차원의 다변량 분포(multivariate distribution)를 따른다고 하고 결합확률밀도함수 \\(f(\\pmb x) =f(x_1,x_2,\\dots,x_p)\\)를 를 가진다고 하자.\n\\[\n\\pmb X =\n  \\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n..  \\\\\nX_p\n\\end{bmatrix}\n\\]\n다변량 확률벡터의 평균 벡터(mean vector)는 다음과 같이 주어진다. 확률벡터의 평균 벡터는 구성하는 각 확률변수의 평균으로 주어진다.\n\\[\n\\pmb E(\\pmb X) =\n  \\begin{bmatrix}\nE(X_1) \\\\\nE(X_2) \\\\\nE(X_3) \\\\\n..  \\\\\nE(X_p)\n\\end{bmatrix}\n=\n  \\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n..  \\\\\n\\mu_p\n\\end{bmatrix}\n=\\pmb \\mu\n\\]\n다음으로 공분산(covariance)과 상관계수(correlation coefficient)에 대해서 알아보자. 우리는 여러 개의 확률 변수의 관계를 분석하는 분석을 하려고 하는데, 이 경우 가장 많이 사용되는 통계량이 두 개의 변수들의 선형적 관계를 나타내는 상관계수이다. 두 확률변수 \\(X_k\\) 와 \\(X_l\\) 의 상관계수 \\(\\rho_{jk}\\) 는 다음과 같이 정의된다.\n\\[\n\\rho_{jk} = \\frac{Cov(X_j, X_k)}{ \\sqrt{V(X_j) V(X_k)}} = \\frac{\\sigma_{jk}}{\\sqrt{\\sigma_{jj}\n  \\sigma_{kk}}}, \\quad j,k=1,2,\\dots,p\n\\]\n위의 상관계수의 공식에서 \\(\\sigma_{jj} \\equiv \\sigma^2_j\\) 와 \\(\\sigma_{kk} = \\sigma^2_k\\) 는 각각 확률변수 \\(X_i\\) 와 \\(X_j\\) 의 분산이며, 공분산은 다음과 같이 정의된다.\n\\[\n\\begin{aligned}\nCov(X_j, X_k) & = E [(X_j - E(X_j))(X_k - E(X_k))] \\\\\n& = E(X_j  X_k) - E(X_j)E(X_k)\n\\end{aligned}\n\\]\n위의 식을 보면 각각의 확률 변수가 평균에서 차이가 나는 두 개의 편차, 즉 \\(X_j - E(X_j)\\), \\(X_k - E(X_k)\\) 의 곱에 대한 기대값으로 두 확률 변수가 평균에서 얼마나 같은 방향 또는 반대 방향으로 함께 움직이는 경향이 있는지 그 정도를 수치화한 값이다. 두 확률변수의 공분산의 값이 양의 값으로 커지면 두 확률 변수의 변화가 같은 방향으로 나타난다는 의미이며, 반대로 음의 값으로 커지면 두 확률 변수의 변화가 반대 방향으로 나타난다는 의미이다.\n참고로 공분산은 단위가 확률 변수의 단위에 영향을 받기 떄문에 크기 자체만으로 비교가 직관적이지 않다는 단점이 있다. 반면에 상관 계수는 공분산을 각 확률 변수의 표분편차로 나누어 얻은 값이므로 단위에 영향을 받지 않아서 상대적인 비교가 가능하다.\n상관계수는 -1 과 1 사이의 값을 가지며 1에 가까울수록 두 개의 변수가 같은 방향으로 움직이는 확률적 경향이 강해지며 반대로 -1 에 가까워질수록 반대의 방향을 움직이는 경향이 강해진다.\n여기서 중요한 점은 상관계수(또는 공분산)은 두 확률 변수의 선형적 관계(linear relationship)을 나타내는 통계량으로 비선형적 관계를 파악하는데는 한계가 있을 수 있다.\n이제 확률 벡터의 모든 변수에 대한 분산과 공분산을 다음과 같은 공분산 행렬로 나타낼 수 있다.\n\\[\n\\begin{aligned}\nV(\\pmb X) & = Cov(\\pmb X) = E (\\pmb X-\\pmb \\mu) (\\pmb X-\\pmb \\mu)^t \\\\\n& = E(\\pmb X \\pmb X^t)-\\pmb \\mu \\pmb  \\mu^t\\\\\n& =\n  \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1p} \\\\\n\\sigma_{12} & \\sigma_{22} & \\dots & \\sigma_{2p} \\\\\n& \\dots & \\dots & \\\\\n\\sigma_{1p} & \\sigma_{2p} & \\dots & \\sigma_{pp} \\\\\n\\end{bmatrix} \\\\\n& = \\pmb \\Sigma\n\\end{aligned}\n\\]\n여기서 \\(\\sigma_{jj}=V(X_j)\\), \\(\\sigma_{jk} = Cov(X_j, X_k)=Cov(X_k, X_j)\\) 이다. 따라서 공분산 행렬 \\(\\pmb \\Sigma\\)는 대칭행렬(symmetric matrix)이다.\n더 나아가 확률 벡터의 모든 변수에 대한 상관계수을 다음과 같은 상관계수 행렬(correlation matrix) \\(\\pmb R\\) 로 나타낼 수 있다.\n\\[\n\\begin{aligned}\ncor(\\pmb X) & =\n  \\begin{bmatrix}\n1 & \\rho_{12} & \\dots & \\rho_{1p} \\\\\n\\rho_{12} & 1 & \\dots & \\rho_{2p} \\\\\n& \\dots & \\dots & \\\\\n\\rho_{1p} & \\rho_{2p} & \\dots & 1 \\\\\n\\end{bmatrix} \\\\\n& = \\pmb R\n\\end{aligned}\n\\]\n위의 상관계수 행렬에서 대각원소는 모두 1 임이 유의하자.\n새로운 확률벡터 \\(\\pmb Y\\)가 확률벡터 \\(\\pmb X\\) 의 선형변환라고 하자.\n\\[ \\pmb Y = \\pmb A  \\pmb X + \\pmb b \\]\n단 여기서 \\(\\pmb A\\) 는 \\(p \\times p\\) 실수 행렬이고 \\(\\pmb b\\) 는 \\(p \\times 1\\) 실수 벡터이다.\n확률벡터 \\(\\pmb Y\\)의 기대값(평균벡터)과 공분산은 다음과 같이 계산된다.\n\\[\n\\begin{aligned}\nE(\\pmb Y ) &= E(\\pmb A \\pmb X+ \\pmb b) \\\\\n&= \\pmb A E(\\pmb X)+ \\pmb b \\\\\n&= \\pmb A \\pmb \\mu+ \\pmb b \\\\\nV(\\pmb Y) &= Var(\\pmb A \\pmb X+ \\pmb b) \\\\\n&= E[\\pmb A \\pmb X+ \\pmb b -E(\\pmb A \\pmb X+ \\pmb b)] [\\pmb A \\pmb X+ \\pmb b -E(\\pmb A \\pmb X+ \\pmb b)]^t \\\\\n&= E[\\pmb A \\pmb X -  \\pmb A \\pmb \\mu] [\\pmb A \\pmb X -  \\pmb A \\pmb \\mu]^t \\\\\n&= E[\\pmb A (\\pmb X - \\pmb \\mu)] [\\pmb A (\\pmb X - \\pmb \\mu)]^t \\\\\n&=  E [\\pmb A(\\pmb X - \\pmb \\mu) (\\pmb X - \\pmb \\mu)^t  \\pmb A^t ] \\\\\n&= \\pmb A E [(\\pmb X - \\pmb \\mu) (\\pmb X - \\pmb \\mu)^t] \\pmb A^t \\\\\n&= \\pmb A \\pmb \\Sigma \\pmb A^t\n\\end{aligned}\n\\]\n\n\n1.2.3 표본 통계량\n이제 확률 표본(sample)을 이용하여 평균벡터, 공분산, 상관계수를 추정하는 간단한 방법에 대해서 알아보자.\n확률 벡터 \\(\\pmb X\\) 가 평균이 \\(\\pmb \\mu\\) 이고 공분산이 \\(\\pmb \\Sigma\\) 인 다변량 분포 \\(F\\) 를 따른다고 가정하자. 만약 확률 표본 \\(\\pmb X_1, \\pmb X_2, \\dots, \\pmb X_n\\) 이 독립적으로 다변량 분포 \\(F\\) 에서 임의로 추출되었다면\n\\[\n\\pmb X_i =\n\\begin{bmatrix}\nX_{i1} \\\\\nX_{i2} \\\\\nX_{i3} \\\\\n..  \\\\\nX_{ip}\n\\end{bmatrix}\n\\quad i=1,2,\\dots,n\n\\]\n다음과 같이 표본 통계량을 이용하여 평균벡터, 공분산, 상관계수를 추정할 수 있다.\n먼저 다음과 같은 표본평균 벡터 \\(\\bar {\\pmb X}\\) 는 평균벡터 \\(\\pmb \\mu\\) 의 불편추정량(unbiased estimator)이다.\n\\[\n\\bar {\\pmb X} =\n  \\begin{bmatrix}\n\\bar X_1  \\\\\n\\bar X_2  \\\\\n\\bar X_3  \\\\\n..  \\\\\n\\bar X_p  \\\\\n\\end{bmatrix}\n=\n  \\begin{bmatrix}\n\\sum_{i=1}^n X_{i1} / n  \\\\\n\\sum_{i=1}^n X_{i2} / n \\\\\n\\sum_{i=1}^n X_{i3} / n \\\\\n..  \\\\\n\\sum_{i=1}^n X_{ip} / n\n\\end{bmatrix}\n=\n\\hat {\\pmb \\mu}\n\\]\n여기서 \\(X_{ij}\\) 는 \\(i\\)번째 표본벡터 \\(\\pmb X_i =(X_{i1} X_{i2} \\dots X_{ip})^t\\)의 \\(j\\)번째 확률변수이다.\n또한 아래에 주어진 표본 공분산 행렬 \\(\\pmb S\\) 은 공분산 행렬 \\(\\pmb \\Sigma\\) 의 추정량이다.\n\\[\n\\pmb S\n=\n  \\begin{bmatrix}\ns_{11} & s_{12} & \\dots & s_{1p} \\\\\ns_{12} & s_{22} & \\dots & s_{2p} \\\\\n& \\dots & \\dots & \\\\\ns_{1p} & s_{2p} & \\dots & s_{pp} \\\\\n\\end{bmatrix}\n=\n\\hat {\\pmb \\Sigma}\n\\] 위에서 \\(s_{jj} \\equiv s^2_j\\) 는 확률변수 \\(X_j\\) 의 표본 분산이며 \\(s_{jk}\\) 는 \\(X_j\\) 와 \\(X_k\\) 의 표본 공분산이며 다음과 같이 계산된다.\n\\[\ns_{jj} = \\frac{1}{n-1} \\sum_{i=1}^n (X_{ij} - \\bar X_j)^2, \\quad\ns_{jk} = \\frac{1}{n-1} \\sum_{i=1}^n (X_{ij} - \\bar X_j)(X_{ik} - \\bar X_k), \\quad j,k=1,2,\\dots,p\n\\]\n마지막으로 아래에 주어진 표본 상관계수 행렬 \\(\\pmb R\\) 은 상관계수 행렬 \\(\\pmb R\\) 의 추정량이다.\n\\[\n\\hat {\\pmb R} =\n  \\begin{bmatrix}\n1 & r_{12} & \\dots & r_{1p} \\\\\nr_{12} & 1 & \\dots & r_{2p} \\\\\n& \\dots & \\dots & \\\\\nr_{1p} & r_{2p} & \\dots & 1 \\\\\n\\end{bmatrix}\n\\]\n여기서 \\(r_{jk}\\) 는 확률변수 \\(X_j\\) 와 \\(X_k\\) 의 표본 상관계수이며 다음과 같이 계산된다.\n\\[\nr_{jk} = \\frac{s_{jk}}{\\sqrt{s_{jj} s_{kk}}}, \\quad j,k=1,2,\\dots,p\n\\]\n\n\n1.2.4 예제-국민체력100\n이제 위에서 샇펴본 국민체력100 자료에서 청소년 남자 자료를 이용하여 평균벡터, 공분산 행렬, 상관계수 행렬의 표본 통계량을 계산해 보자.\n먼저 표본 평균 벡터를 계산해 보자. 주어진 변수가 많으니 키(height), 몸무게(weight), 체지방률(body_fat_pct), 악력(grip_left), 앉아윗몸앞으로굽히기(sit_forward), 청소년체공시간(hang_time) 6개 변수만 선택하여 계산해 보자.\n\n# 국민체력100 자료에서 남자만 선택하여 데이터프레임 df 생성\ndf &lt;- physical100_df %&gt;% \n  filter(sex == \"남성\") %&gt;%\n  select(height, weight, body_fat_pct, grip_left, sit_forward,hang_time)\n\n#  패키지 dplyr의 summarise()와 across() 함수를 사용하여 각 열의 평균 계산\nsample_mean_vector &lt;- df %&gt;%\n  summarise(across(everything(), \\(x) mean(x, na.rm = TRUE))) %&gt;%\n  unlist()\n\nsample_mean_vector\n\n      height       weight body_fat_pct    grip_left  sit_forward    hang_time \n 172.0451492   69.7115470   20.9993923   36.0728729    7.8976630    0.5593964 \n\n\n다음으로 표본 공분산 행렬을 계산해 보자.\n\ncor(df)\n\n                    height      weight body_fat_pct   grip_left   sit_forward\nheight        1.0000000000  0.50994161  -0.03019400  0.45225668  0.0008938941\nweight        0.5099416134  1.00000000   0.69156075  0.46152254  0.0111910927\nbody_fat_pct -0.0301940037  0.69156075   1.00000000 -0.01208826 -0.1434596849\ngrip_left     0.4522566754  0.46152254  -0.01208826  1.00000000  0.2605654264\nsit_forward   0.0008938941  0.01119109  -0.14345968  0.26056543  1.0000000000\nhang_time     0.1884978140 -0.14265212  -0.48446817  0.34559521  0.2889235491\n              hang_time\nheight        0.1884978\nweight       -0.1426521\nbody_fat_pct -0.4844682\ngrip_left     0.3455952\nsit_forward   0.2889235\nhang_time     1.0000000\n\n\n마지막으로 표본 상관계수 행렬을 계산해 보자.\n\ncor(df)\n\n                    height      weight body_fat_pct   grip_left   sit_forward\nheight        1.0000000000  0.50994161  -0.03019400  0.45225668  0.0008938941\nweight        0.5099416134  1.00000000   0.69156075  0.46152254  0.0111910927\nbody_fat_pct -0.0301940037  0.69156075   1.00000000 -0.01208826 -0.1434596849\ngrip_left     0.4522566754  0.46152254  -0.01208826  1.00000000  0.2605654264\nsit_forward   0.0008938941  0.01119109  -0.14345968  0.26056543  1.0000000000\nhang_time     0.1884978140 -0.14265212  -0.48446817  0.34559521  0.2889235491\n              hang_time\nheight        0.1884978\nweight       -0.1426521\nbody_fat_pct -0.4844682\ngrip_left     0.3455952\nsit_forward   0.2889235\nhang_time     1.0000000\n\n\n표본 상관계수 행렬을 보면 다양한 상관관계가 나타나는데 이러한 관계를 더 자세하게 보기위하여 산점도 행렬(scatterplot matrix)로 시각화 하면 더 유용한 정보를 얻을 수 있다.\n\npairs(df, pch=19, col='blue', cex=0.1)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>확률벡터와 다변량 정규분포</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-dist.html#다변량-정규분포",
    "href": "qmd/multivar-dist.html#다변량-정규분포",
    "title": "1  확률벡터와 다변량 정규분포",
    "section": "1.3 다변량 정규분포",
    "text": "1.3 다변량 정규분포\n일변량 확률변수 \\(X\\)가 평균이 \\(\\mu\\) 이고 분산이 \\(\\sigma^2\\)인 정규분포를 따른다면 다음과 같이 나타내고 \\[ X \\sim N(\\mu, \\sigma^2 ) \\] 확률밀도함수 \\(f(x)\\) 는 다음과 같이 주어진다.\n\\[ f(x) = (2 \\pi \\sigma^2)^{-1/2} \\exp \\left ( - \\frac{(x-\\mu)^2}{2} \\right ) \\]\n\n1.3.1 확률 밀도 함수\n\\(p\\)-차원 확률벡터 \\(\\pmb X\\)가 평균이 \\(\\pmb \\mu\\) 이고 공분산이 \\(\\pmb \\Sigma\\)인 다변량 정규분포를 따른다면 다음과 같이 나타내고 \\[ \\pmb X \\sim N_p(\\pmb \\mu, \\pmb \\Sigma ) \\] 확률밀도함수 \\(f(\\pmb x)\\) 는 다음과 갇이 주어진다.\n\\[ f(\\pmb x) = (2 \\pi)^{-p/2} | \\pmb \\Sigma|^{-1/2}\n   \\exp \\left ( - \\frac{(\\pmb x-\\pmb \\mu) \\pmb \\Sigma^{-1}(\\pmb x-\\pmb \\mu)^t}{2} \\right ) \\]\n예를 들어 \\(2\\)-차원 확률벡터 \\(\\pmb X=(X_1, X_2)^t\\)가 평균이 \\(\\pmb \\mu=(\\mu_1,\\mu_2)^t\\) 이고 공분산 \\(\\pmb \\Sigma\\)가 다음과 같이 주어진\n\\[\n\\pmb \\Sigma =\n  \\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} \\\\\n\\sigma_{12} & \\sigma_{22}\n\\end{bmatrix}\n\\]\n이변량 정규분포를 따른다면 확률밀도함수 \\(f(\\pmb x)\\)에서 \\(\\exp\\)함수의 인자는 다음과 같이 주어진다. \\[\n\\begin{aligned}\n&(\\pmb x-\\pmb \\mu) \\pmb \\Sigma^{-1}(\\pmb x-\\pmb \\mu)^t\n= \\\\\n&-\\frac{1}{2 (1-\\rho^2)}\n\\left [\n  \\left ( \\frac{(x_1-\\mu_1)^2}{\\sigma_{11}} \\right )\n  +\\left ( \\frac{(x_2-\\mu_2)^2}{\\sigma_{22}} \\right )\n  -2 \\rho \\left ( \\frac{(x_1-\\mu_1)}{\\sqrt{\\sigma_{11}}} \\right )\n  \\left ( \\frac{(x_2-\\mu_2)}{\\sqrt{\\sigma_{22}}} \\right )\n  \\right ]\n\\end{aligned}\n\\]\n그리고 \\(p=2\\) 인 경우 확률밀도함수의 상수부분은 다음과 같이 주어진다.\n\\[ (2 \\pi)^{-p/2} | \\pmb \\Sigma|^{-1/2} = \\frac{1}{ 2 \\pi \\sqrt{\\sigma_{11} \\sigma_{22} (1-\\rho^2)}} \\]\n여기서 \\(\\rho = \\sigma_{12} / \\sqrt{\\sigma_{11} \\sigma_{22}}\\)\n\n\n\n\n\n\n다변량 정규분포에서 독립과 공분산\n\n\n\n다변량 정규분포에서 공분산이 0인 두 확률 변수는 독립이다. \\[ \\sigma_{ij} = 0 \\leftrightarrow X_i \\text{ and } X_j \\text{ are independent} \\]\n참고로 정규분포가 아닌 다른 분포의 경우 공분산이 0인 두 확률 변수는 독립이 아닐 수 있다.\n\n\n\n\n1.3.2 예제-국민체력100\n이제 위에서 샇펴본 국민체력100 자료에서 청소년 남자의 키(height)와 몸무게(weight) 가 이변량 정규분포를 따른다고 가정하고 확률밀도 함수를 그려보자.\n\n# 필요한 패키지 로드\nlibrary(mvtnorm)\nlibrary(plotly)\n\n# 국민체력100 자료에서 남자만 선택하여 데이터프레임 df 생성, 키와 몸무게 변수만 선택와\ndf &lt;- physical100_df %&gt;% \n  filter(sex == \"남성\") %&gt;%\n  select(height, weight)\n\n#  패키지 dplyr의 summarise()와 across() 함수를 사용하여 각 열의 평균 계산\nsample_mean_vector &lt;- df %&gt;%\n  summarise(across(everything(), \\(x) mean(x, na.rm = TRUE))) %&gt;%\n  unlist()\nsample_mean_vector\n\n   height    weight \n172.04515  69.71155 \n\n# 표본 공분산 행렬 계산\nsample_cov_matrix &lt;- cov(df, use = \"complete.obs\")\nsample_cov_matrix\n\n         height    weight\nheight 47.51166  54.80339\nweight 54.80339 243.09372\n\n# 이변량 정규분포의 확률밀도함수 계산\n# 키와 몸무게의 평균에서 표분편차 3배의 범위의 값을 100개로 나누어 x,y 축 생성\nx1_seq &lt;- seq(sample_mean_vector[1]-3* sqrt(sample_cov_matrix[1,1]), sample_mean_vector[1]+3* sqrt(sample_cov_matrix[1,1]), length.out = 100)\nx2_seq &lt;- seq(sample_mean_vector[2]-3* sqrt(sample_cov_matrix[2,2]), sample_mean_vector[2]+3* sqrt(sample_cov_matrix[2,2]), length.out = 100)\ngrid &lt;- expand.grid(height = x1_seq, weight = x2_seq)\n\n# 확률밀도함수 계산 (z축의 값)\ngrid$z &lt;- dmvnorm(grid, mean = sample_mean_vector, sigma = sample_cov_matrix)\n\n# z를 행렬로 변환 (surface plot용)\nz_matrix &lt;- matrix(grid$z, nrow = length(x1_seq), ncol = length(x2_seq))\n\n다음은 위에서 얻어진 확률밀도 함수를 3차원 surface plot으로 나타낸 것이다.\n\n# 이변량 정규분포의 확률밀도 함를 3D Surface Plot\nplot_ly(\n  x = x1_seq, \n  y = x2_seq, \n  z = z_matrix\n) %&gt;% add_surface() %&gt;%\n  layout(\n    title = \"Bivariate Normal PDF (Surface)\",\n    scene = list(\n      xaxis = list(title = \"키\"),\n      yaxis = list(title = \"몸무게\"),\n      zaxis = list(title = \"Density\")\n    )\n  )\n\n\n\n\n\n아래 그림은 표본으로 부터 얻어진 확률밀도 함수를 2차원 등고선(contour)으로 나타낸 그림이다.\n\nplot_ly(\n  x = x1_seq,\n  y = x2_seq,\n  z = z_matrix\n) %&gt;%\n  add_contour(\n    contours = list(\n      coloring = \"heatmap\",\n      showlabels = TRUE\n    )\n  ) %&gt;%\n  layout(\n    title = \"Bivariate Normal PDF (Contour)\",\n    xaxis = list(title = \"키\"),\n    yaxis = list(title = \"몸무게\")\n  )\n\n\n\n\n\n\n\n1.3.3 조건부 분포\n다변량 정규분포 \\(N(\\pmb \\mu, \\pmb \\Sigma)\\)를 따르는 확률벡터 \\(\\pmb X\\)를 다음과 같이 두 부분으로 나누면\n\\[\n  \\pmb X =\n    \\begin{bmatrix}\n  \\pmb X_1 \\\\\n  \\pmb X_2\n  \\end{bmatrix}, \\quad\n  \\pmb X_1 =\n    \\begin{bmatrix}\n  \\pmb X_{11} \\\\\n  \\pmb X_{12} \\\\\n  \\pmb \\vdots \\\\\n  \\pmb X_{1p}\n  \\end{bmatrix}, \\quad\n  \\pmb X_2=\n    \\begin{bmatrix}\n  \\pmb X_{21} \\\\\n  \\pmb X_{22} \\\\\n  \\pmb \\vdots \\\\\n  \\pmb X_{2q}\n  \\end{bmatrix}\n  \\]\n각각 다변량 정규분포를 따르고 다음과 같이 나타낼 수 있다.\n\\[\n  \\begin{bmatrix}\n  E(\\pmb X_1) \\\\\n  E(\\pmb X_2)\n  \\end{bmatrix}\n  =\n    \\begin{bmatrix}\n  \\pmb \\mu_1 \\\\\n  \\pmb \\mu_2\n  \\end{bmatrix}\n  , \\quad\n  \\begin{bmatrix}\n  V(\\pmb X_1) & Cov(\\pmb X_1, X_2) \\\\\n  Cov(\\pmb X_2 X_1) & V(\\pmb X_2)\n  \\end{bmatrix}\n  =\n    \\begin{bmatrix}\n  \\pmb \\Sigma_{11} & \\pmb \\Sigma_{12} \\\\\n  \\pmb \\Sigma^t_{12} & \\pmb \\Sigma_{22}\n  \\end{bmatrix}\n  \\]\n\\[  \\pmb X =\n    \\begin{bmatrix}\n  \\pmb X_1 \\\\\n  \\pmb X_2\n  \\end{bmatrix}\n  \\sim\n  N_{p+q} \\left (\n    \\begin{bmatrix}\n    \\pmb \\mu_1 \\\\\n    \\pmb \\mu_2\n    \\end{bmatrix}\n    ,\\begin{bmatrix}\n    \\pmb \\Sigma_{11} & \\Sigma_{12} \\\\\n    \\pmb \\Sigma^t_{12} & \\Sigma_{22}\n    \\end{bmatrix}\n    \\right )\n  \\]\n확률벡터 \\(\\pmb X_2 = \\pmb x_2\\)가 주어진 경우 \\(\\pmb X_1\\)의 조건부 분포는 \\(p\\)-차원 다변량 정규분포를 따르고 평균과 공분산은 다음과 같다.\n\\[\n  E(\\pmb X_1 | \\pmb X_2 = \\pmb x_2 ) = \\pmb \\mu_1 + \\pmb \\Sigma_{12} \\pmb \\Sigma^{-1}_{22} (\\pmb \\mu_2 - \\pmb x_2), \\quad\n  V(\\pmb X_1 | \\pmb X_2 = \\pmb x_2 )  = \\pmb \\Sigma_{11} -\\pmb \\Sigma_{12} \\pmb \\Sigma^{-1}_{22} \\pmb \\Sigma^t_{12}\n  \\]\n만약 \\(X_2 = x_2\\)가 주어졌을 때 \\(X_1\\)의 조건부 분포는 정규분포이고 평균과 분산은 다음과 같이 주어진다.\n\\[\n  E( X_1 |  X_2 =  x_2 ) =  \\mu_1 +  \\frac{\\sigma_{12}}{\\sigma_{22}} ( \\mu_2 -  x_2)  = \\mu_1 +  \\rho \\frac{\\sqrt{\\sigma_{11}}}{\\sqrt{\\sigma_{22}}} ( \\mu_2 -  x_2) \\]\n\\[\n  V( X_1 |  X_2 =  x_2 )  =  \\sigma_{11} - \\frac{\\sigma^2_{12}}{\\sigma_{22}}  = \\sigma_{11}(1-\\rho^2)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>확률벡터와 다변량 정규분포</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-vis.html",
    "href": "qmd/multivar-vis.html",
    "title": "2  다변량 자료의 시각화",
    "section": "",
    "text": "2.1 산점도 그림\n일반적으로 많이 사용하는 회귀모형(regression model) 은 반응변수와 이에 영향을 주는 설명변수들의 관계를 분석하는 모형이다. 따라서 주로 회귀모형에서 시각화 방법은 반응변수와 설명변수의 관계를 나타내는 그림이며 주로 2개의 변수의 관계를 파악하는 산점도 그림 (scatter plot) 이 많이 사용된다.\n다변량 분석은 여러 개의 확률 변수들의 관계를 분석하는 통계 기법이다. 회귀모형과 다른 점은 주로 예측에 관심이 있는 반응변수가 없는 것이며 따라서 분석의 특성상 3개 이상의 변수의 관계를 한 그림에 시각화하는 방밥이 필요하다.\n이 장에서는 다변량 분석에서 여러 개의 변수들의 관계를 동시에 시각화 방법들을 알아보려고 한다.\n앞에서 언급한 것과 같이 3개의 변수를 산점도에 나타내려면 x 축, y 축 과 더불어서 점의 특성(색깔, 모양 등)을 이용할 수 있다.\nR 패키지 gapminder 에 포함된 gapminder 데이터는 전 세계 여러 국가의 경제, 보건 지표를 연도별로 기록한 공개 자료이다. 원본은 [Gapminder 재단]{https://www.gapminder.org/} 에서 제공한다. gapminder 데이터는 1952년부터 2007년까지 5년 단위로 측정한 각 국가별 경제 수준과 건강 상태를 시계열로 비교 가능하게 정리한 자료이다.\nhead(gapminder::gapminder)\n\n# A tibble: 6 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.\n2 Afghanistan Asia       1957    30.3  9240934      821.\n3 Afghanistan Asia       1962    32.0 10267083      853.\n4 Afghanistan Asia       1967    34.0 11537966      836.\n5 Afghanistan Asia       1972    36.1 13079460      740.\n6 Afghanistan Asia       1977    38.4 14880372      786.\n이제 gapminder 데이터에서 2007년 자료를 이용하여 1인당 국민소득(gdpPercap)과 기대수명(lifeExp)의 관계를 산점도로 나타내고, 대륙(continent)에 따라 점의 크기로 다르게 나타내는 그림을 그려보자. 이렇게 점의 크기를 변수의 값에 따라 변하는 산점도를 버블 차트(bubble chart) 라고 한다.\ngapminder::gapminder %&gt;%\n  filter(year == 2007) %&gt;%\n  ggplot(aes(x = gdpPercap, y = lifeExp, size=pop)) +\n  geom_point(alpha = 0.5, color=\"blue\") +\n  labs(title = \"1인당 국민소득과 기대수명의 관계 (2007년)\",\n       x = \"1인당 국민소득\",\n       y = \"기대수명\",\n       size = \"인구수\") +\n  theme(text = element_text(family = \"noto\")) # 한글 폰트 설정(lib(showtext) 패키지 필요)\n위의 그림을 보면 1인당 국민소득과 기대수명의 관계가 선형적이지 않음을 알 수 있다. 따라서 x 축을 로그 스케일로 변환하여 다시 그려보자.\ngapminder::gapminder %&gt;%\n  filter(year == 2007) %&gt;%\n  ggplot(aes(x = gdpPercap, y = lifeExp, size=pop)) +\n  geom_point(alpha = 0.5, color=\"blue\") + # alpha는 점의 투명도\n  scale_x_log10() + # x축을 로그 스케일로 변환\n  labs(title = \"1인당 국민소득과 기대수명의 관계 (2007년)\",\n       x = \"1인당 국민소득 (로그 스케일)\",\n       y = \"기대수명\",\n       size = \"인구수\") +\n  theme(text = element_text(family = \"noto\"))\n위의 그림에서 나라가 속한 대륙에 따라 점의 색깔을 다르게 나타내어 보자. 많은 경우 자료는 특정 그룹별로 분석하는 경우가 많기 떄문에 그룹을 시각적으로 나타내는 것이 중요하다.\ngapminder::gapminder %&gt;%\n  filter(year == 2007) %&gt;%\n  ggplot(aes(x = gdpPercap, y = lifeExp, size=pop, color=continent)) +\n  geom_point(alpha = 0.5) +\n  scale_x_log10() + # x축을 로그 스케일로 변환\n  labs(title = \"1인당 국민소득과 기대수명의 관계 (2007년)\",\n       x = \"1인당 국민소득 (로그 스케일)\",\n       y = \"기대수명\",\n       size = \"인구수\",\n       color = \"대륙\") +\n  theme(text = element_text(family = \"noto\"))\n이렇게 그룹화하여 시각화 하는 경우 그룹마다 산점도를 따로 나타내는 방법으로 facet 기능을 사용할 수 있다. facet 기능은 ggplot2 패키지에서 제공하는 기능으로 facet_wrap() 함수와 facet_grid() 함수가 있다.\ngapminder::gapminder %&gt;%\n  filter(year == 2007) %&gt;%\n  ggplot(aes(x = gdpPercap, y = lifeExp, size=pop)) +\n  geom_point(alpha = 0.5, color=\"blue\") +\n  scale_x_log10() + \n  labs(title = \"1인당 국민소득과 기대수명의 관계 (2007년)\",\n       x = \"1인당 국민소득 (로그 스케일)\",\n       y = \"기대수명\",\n       size = \"인구수\") +\n  facet_wrap(~ continent) + # 대륙별로 산점도 따로 표시\n  theme(text = element_text(family = \"noto\"))\n위의 그림에서 아시아에 속한 나라들만 선택해서 점의 크기를 인구에 비례하게 하고 또한 나라의 이름을 표시해보자.\ngapminder::gapminder %&gt;%\n  filter(year == 2007, continent == \"Asia\") %&gt;%\n  ggplot(aes(x = gdpPercap, y = lifeExp, size=pop, label=country)) +\n  geom_point(alpha = 0.5, color=\"blue\") +\n  geom_text(vjust = -1, size=3) + # 나라 이름 표시\n  scale_x_log10() + # x축을 로그 스케일로 변환\n  labs(title = \"1인당 국민소득과 기대수명의 관계 (2007년, 아시아 국가)\",\n       x = \"1인당 국민소득 (로그 스케일)\",\n       y = \"기대수명\",\n       color = \"대륙\") +\n  theme(text = element_text(family = \"noto\"))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>다변량 자료의 시각화</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-vis.html#산점도-그림",
    "href": "qmd/multivar-vis.html#산점도-그림",
    "title": "2  다변량 자료의 시각화",
    "section": "",
    "text": "변수명\n설명\n\n\n\n\ncountry\n국가 이름\n\n\ncontinent\n대륙 이름\n\n\nyear\n연도\n\n\nlifeExp\n기대수명 (average life expectancy)\n\n\npop\n인구 (population)\n\n\ngdpPercap\n1인당 국민소득 (gross domestic product per capita)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>다변량 자료의 시각화</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-vis.html#상관계수-행렬",
    "href": "qmd/multivar-vis.html#상관계수-행렬",
    "title": "2  다변량 자료의 시각화",
    "section": "2.2 상관계수 행렬",
    "text": "2.2 상관계수 행렬\n다변량 분석에서 상관계수는 여러 개의 변수들의 관계를 파악하는 가장 기본적인 통계량으로서 시각화를 이용하면 더 쉽게 이해할 수 있다.\n상관계수 행렬(correlation matrix)은 여러 변수들 간의 상관계수를 한눈에 볼 수 있도록 정리한 그림이다. 상관계수 행렬을 시각화하는 방법으로는 히트맵(heatmap)이나 페어 플롯(pair plot) 등이 있다.\n상관계수 행렬에 대한 예제는 앞 장에서 살펴본 국민체력100 자료를 이용하고자 한다.\n먼저 가장 감단한 방법인 pair() 함수를 이용하여 페어 플롯을 그려보자. pair() 함수는 R에 기본으로 포함된 함수로서 여러 변수들 간의 산점도와 히스토그램을 한눈에 볼 수 있도록 그려준다.\n\nload(here(\"data\", \"physical100.RData\"))\n# 국민체력100 자료에서 남자만 선택하여 데이터프레임 df 생성\ndf &lt;- physical100_df %&gt;% \n  filter(sex == \"남성\") %&gt;%\n  select(height, weight, body_fat_pct, grip_left, sit_forward,hang_time)\n\n# pair() 함수를 이용하여 상관계수 행렬 그리기\npairs(df, pch=19, cex=0.1)\n\n\n\n\n\n\n\n\n위와 같은 상관계수 산점도 행렬에 다음과 같이 회귀 직선을 추가하여 변수들 간의 관계를 더 명확히 나타낼 수 있다. 다음 코드는 panel 인수를 이용하여 각 산점도에 회귀 직선을 추가하는 방법을 보여준다.\n\npairs(\n  df,\n  panel = function(x, y) {\n    points(x, y, pch=19, cex=0.1)\n    abline(lm(y ~ x), col = \"red\", lwd = 1.5)  # 선형회귀 직선\n  }\n)\n\n\n\n\n\n\n\n\n위의 상관계수 행렬 그림에 좀 더 유용한 정보를 추가하는 다양한 방법들이 있다. 예를 들어, GGally 패키지의 ggpairs() 함수를 이용하면 상관계수 행렬에 상관계수 값과 각 변수에 대한 히스토그램을 추가하여 볼수 있다.\n\nggpairs(df,\n        upper = list(continuous = wrap(\"cor\", size = 4)), # 상관계수 표시\n        lower = list(continuous = wrap(\"points\", alpha=0.3, size=0.5)), # 산점도\n        diag = list(continuous = wrap(\"barDiag\", fill=\"lightblue\"))) + # 히스토그램\n  theme(text = element_text(family = \"noto\"))\n\n\n\n\n\n\n\n\n최근에는 상관계수 행렬을 히트맵(heatmap)으로 나타내기도 한다. 히트맵은 색깔로 값의 크기를 나타내는 그림으로서 상관계수 행렬을 히트맵으로 나타내면 변수들 간의 관계를 쉽게 파악할 수 있다.\n특히, 변수의 개수가 매우 많은 경우 산점도를 이용한 상관계수 행렬은 시각적으로 분석이 어렵기 때문에 히트맵으로 나타내는 것이 더 유용할 수 있다. 패키지 pheatmap 의 pheatmap() 함수를 이용하여 상관계수 행렬을 히트맵으로 나타내보자. pheatmap() 함수의 유용한 점은 상관계수가 큰 변수끼리 군집화(clustering)하여 시각화할 수 있다는 점이다\n자료는 국민체력100 자료에서 남자에 대한 모든 변수를 이용한 자료를 이용한다.\n\n# 국민체력100 자료에서 남자만 선택하여 데이터프레임 df 생성\ndf &lt;- physical100_df %&gt;% \n  filter(sex == \"남성\") %&gt;%\n  select(-sex)\n\n# 상관계수 행렬 계산\ncor_mat &lt;- cor(df, use=\"pairwise.complete.obs\") # 결측치가 있는 경우 pairwise로 계산\n\n# 히트맵 그리기\npheatmap::pheatmap(cor_mat,  display_numbers = TRUE, number_format = \"%.2f\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>다변량 자료의 시각화</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-test.html",
    "href": "qmd/multivar-test.html",
    "title": "3  다변량 가설 검정",
    "section": "",
    "text": "3.1 t-검정\n이번 장에서는 다변량 벡터의 자료에 대한 가설 검정법을 간단히 학습한다. 다변량에서 평균에 대한 검정은 일변량 벡터에서의 t-겁정의 개념을 확장하여 이해하는 것이 중요하다. 일변량 확률 분포에서 두 그룹의 평균을 비교하는 t-겁정 방법을 다변량으로 확장하는 방법을 단계별로 살펴보자.\n참고로 이 장에서는 두 그룹에 대한 분산 또는 공분산이 같다고 가정한다. 공분산이 다른 경우는 아래 방법들을 확장해서 적용할 수 있지만 좀 더 복잡한 통계적 추론이 필요하다.\n기초통계학에서 나오는 가장 기본적이고 자주 쓰이는 가설검정 방법은 두 집단의 평균의 차이를 검정하는 t-검정(t-test)이다.\n두 집단이 평균이 다르고 분산이 동일한 정규분포 \\(N(\\mu_1, \\sigma^2)\\), \\(N(\\mu_2, \\sigma^2)\\)를 따른다고 가정하고 다음과 같이 각각 \\(n_1, n_2\\)개의 독립 표본을 얻었다고 하자.\n\\[\nX_{1}, X_{2}, \\dots, X_{n_1} \\sim N(\\mu_1, \\sigma^2), \\quad Y_{1}, Y_{2}, \\dots, Y_{n_2} \\sim N(\\mu_2, \\sigma^2)\n\\]\n위의 가설을 다음과 같은 t-통계량을 이용하여 검정할 수 있다.\n\\[\nt_0 =\\frac {\\bar X -\\bar Y } {  S_p \\sqrt{1/n_1 + 1/n_2}}\n\\tag{3.1}\\]\n여기서 \\(\\bar X\\), \\(\\bar Y\\)은 각 그룹의 표본 평균이다. 또한 \\(S_p^2\\) 은 두 집단의 공통분산 추정량(pooled variance estimator)이며 다음과 같이 계산한다.\n\\[ \\hat \\sigma^2 = S_p^2 =  \\frac { \\sum_{i=1}^{n_1} (X_{i} -\\bar X)^2 + \\sum_{i=1}^{n_2} (Y_{i} -\\bar Y)^2 } { n_1 + n_2 -2} \\]\n식 3.1 의 t-검정 통계량의 분자는 집단 간의 평균의 차이를 나타낸다. 즉 \\(\\bar X-\\bar Y\\)는 두 집단의 표본 평균의 차이를 추정하는 양이고 그 차이가 크면 클수록 두 집단의 모평균의 차이 \\(\\mu_1 - \\mu_2\\)가 크다는 것을 의미한다.\nt-검정 통계량의 분모는 두 집단의 공통분산 추정량 \\(\\hat \\sigma^2 =S_p^2\\)에 비례한다. 즉 집단 내의 변동을 반영하는 \\(S_p^2\\) 이 크면 클수록 t-검정 통계량은 그 크기가 작아져서 두 그룹 간에 차이가 있다는 증거가 약해진다.\n정리해보면 t-검정 통계량은 집단 간의 변동(between-group variation)을 집단 내의 변동(within-group variation) 으로 나누어준 값이다. 다른 말로 급간 변동과 급내 변동을 사용하기도 한다.\n이제 t-검정 통계량을 제곱하면 다음과 같이 표현할 수 있다.\n\\[  \nt_0^2 =\\frac { (\\bar X -\\bar Y)^2 } { S_p^2 (1/n_1 + 1/n_2)} = \\frac{\\text{between-group variation}} {\\text{within-group variation}}\n\\tag{3.2}\\]\n통계학에서 등장하는 평균에 대한 겁정 통계량은 식 식 3.2 과 같이 그룹간 변동과 그룹내 변동의 비(ratio)로 이루어 진 경우가 많다. 이제 더 나아가 검정 통계량의 다른 해석으로 통계적 거리의 의미를 살펴보자.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>다변량 가설 검정</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-test.html#통계적-거리",
    "href": "qmd/multivar-test.html#통계적-거리",
    "title": "3  다변량 가설 검정",
    "section": "3.2 통계적 거리",
    "text": "3.2 통계적 거리\n다변량 벡터의 평균에 대한 가설 검정을 위해서 다변량 벡터의 통계적 거리(statistical distance)의 개념에 대해서 알아보자.\n먼저 일변량 벡터의 가설검정에 나타나는 t-검정 통계량의 형태를 식 3.2 으로 나타내면 다음과 같은 사실을 알 수 있다.\n\n분자는 두 그룹의 평균에 대한 추정량의 공간적 거리, 즉 \\(\\bar X  - \\bar Y\\) 로서 두 그룹의 평균이 유클리디안 공간(Euclidean distance)에서 얼마나 떨어져 있는 가를 나타낸다. 검정 통계량에서는 거리의 제곱을 사용하였다.\n분모는 두 그룹의 거리에 대한 통계적 불확실성을 반영한다. 이는 추정량의 분산 \\(S^2_p\\) 를 통해서 나타내며, \\(S^2_p\\) 가 커지면 공간적인 거리가 동일해도 통계적인 의미에서의 거리는 줄어드는 것이다.\n\n이제 두 p-차원 확률 벡터 \\(\\pmb X\\) 와 \\(\\pmb Y\\)에 대한 공간적 거리 \\(d( \\pmb X, \\pmb Y)\\) 는 다음과 같이 정의된다.\n\\[\nd( \\pmb X, \\pmb Y)^2 =  (\\pmb X - \\pmb Y)^t (\\pmb X - \\pmb Y)\n\\] 이제 두 확률 벡터의 차이 \\(\\pmb X - \\pmb Y\\)의 불확실성을 나타내는 공분산 행렬을 \\(\\pmb \\Sigma\\) 하면, 확률 벡터의 통계적 거리(statistical distnace 또는 Mahalanobis distance) 는 다음과 같이 정의 된다.\n\\[\nd( \\pmb X, \\pmb Y | {\\pmb \\Sigma})^2 =   (\\pmb X - \\pmb Y)^t  {\\pmb \\Sigma}^{-1} (\\pmb X - \\pmb Y)\n\\tag{3.3}\\]\n여기서 \\(\\Sigma^{-1}\\) 은 공분산 행렬 \\(\\Sigma\\) 의 역행렬(inverse matrix)이다.\n식 3.3 의 통계적 거리는 일변량에서 사용되는 t-검정 통계량의 형태를 다변량 확률변수에 대해서 확장할 수 있는 방법을 제공해 준다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>다변량 가설 검정</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-test.html#호텔링의-t2-검정",
    "href": "qmd/multivar-test.html#호텔링의-t2-검정",
    "title": "3  다변량 가설 검정",
    "section": "3.3 호텔링의 \\(T^2\\) 검정",
    "text": "3.3 호텔링의 \\(T^2\\) 검정\n이제 다변량 벡터의 평균에 대한 검정을 위해서 위에서 정의한 통계적 거리를 이용하여 호텔링의 \\(T^2\\) 검정(Hotelling’s \\(T^2\\) test)을 살펴보자.\n확률 벡터 \\(\\pmb X\\) 과 \\(\\pmb Y\\) 가 평균이 각각 \\(\\pmb \\mu_1\\), \\(\\pmb \\mu_2\\) 이고 공분산이 \\(\\pmb \\Sigma\\) 인 p-차원 다변량 정규 분포를 따른다고 가정하자.\n\\[ \\pmb X \\sim N_p(\\pmb \\mu_1, \\pmb \\Sigma), \\quad \\pmb Y \\sim N_p(\\pmb \\mu_2, \\pmb \\Sigma) \\]\n호텔링의 \\(T^2\\) 검정은 두 그룹의 다변량 평균 벡터가 같은지에 대한 가설검정 방법이다. 즉 다음과 같은 가설을 검정한다.\n\\[\nH_0: \\pmb \\mu_1 = \\pmb \\mu_2\n\\]\n이제 가설 검정을 위하여 두 그룹에서 각각 \\(n_1, n_2\\)개의 다변량 표본이 관측되었다고 하자.\n\\[\n\\pmb X_1, \\pmb X_2, \\dots, \\pmb X_{n_1} \\sim_{IID} N(\\pmb \\mu_1, \\pmb \\Sigma), \\quad \\pmb Y_1, \\pmb Y_2, \\dots, \\pmb Y_{n_2} \\sim_{IID} N(\\pmb \\mu_2, \\pmb \\Sigma)\n\\]\n평균 벡터에 대한 추정량은 각 표본 평균 \\(\\bar {\\pmb X}\\) 과 \\(\\bar {\\pmb Y}\\) 이고 공분산 행렬의 합동 추정량 \\(\\pmb {S}_p\\) 을 다음과 같이 정의한다.\n\\[\n\\pmb S_p = \\hat {\\pmb \\Sigma} =\n\\frac{1}{n_1 + n_2 -2} \\left( \\sum_{i=1}^{n_1} (\\pmb X_i - \\bar {\\pmb X})(\\pmb X_i - \\bar {\\pmb X})^t + \\sum_{i=1}^{n_2} (\\pmb Y_i - \\bar {\\pmb Y})(\\pmb Y_i - \\bar {\\pmb Y})^t \\right)\n\\tag{3.4}\\]\n두 그룹의 평균벡터가 같은지에 대한 검정을 위하여 호텔링의 \\(T^2\\) 검정 통계량은 식 3.3 의 형태로 다음과 같이 정의된다.\n\\[\n\\begin{aligned}\nT^2 & = (\\bar {\\pmb X} - \\bar {\\pmb Y})^t \\left [ \\left ( \\frac{1}{n_1} + \\frac{1}{n_2} \\right )  \\pmb S_p \\right ]^{-1} (\\bar {\\pmb X} - \\bar {\\pmb Y}) \\\\\n& = \\frac{n_1 n_2}{n_1 + n_2} (\\bar {\\pmb X} - \\bar {\\pmb Y})^t \\pmb S_p^{-1} (\\bar {\\pmb X} - \\bar {\\pmb Y})\n\\end{aligned}\n\\tag{3.5}\\]\n위의 호텔링의 \\(T^2\\) 통계량은 두 그룹의 평균 벡터 \\(\\bar {\\pmb X}\\) 와 \\(\\bar {\\pmb Y}\\) 의 차이에 대한 통계적 거리(statistical distance)를 나타낸다. 즉 두 그룹의 평균 벡터의 차이 \\(\\bar {\\pmb X} - \\bar {\\pmb Y}\\) 에 대한 제곱항을 그 차이의 불확실성을 나타내는 공분산 행렬 \\(\\pmb S\\) 의 역행렬로 나누어준 값이다. 일변량에서 t-검정과 동일하게 식 3.5 의 값이 커지면 귀무가설에 반하는 정도가 커지는 것이다.\n호텔링 통계량 \\(T^2\\) 은 귀무가설이 참인 경우, 즉 \\(\\pmb \\mu_1 = \\pmb \\mu_2\\) 일때 자유도가 각각 \\(p\\) 와 \\(n_2+n_2-p-1\\) 을 가지는 F-분포를 따른다. 이 때 \\(p\\) 는 확률 벡터의 차원이다.\n\\[\n\\frac{n_1 + n_2 - p -1}{(n_1 + n_2 -2)p} T^2 \\sim F_{p, n_1 + n_2 - p -1} \\quad \\text{ if } H_0: \\pmb {\\mu_1} = \\pmb {\\mu_2} \\text{ is true}\n\\] 따라서 유의수준 \\(\\alpha\\) 에서 귀무가설을 검정하기 위해서는 다음과 같이 F-분포에서 기각역을 구하여 \\(T^2\\) 값과 비교한다.\n\\[\n\\text{Reject } H_0 \\quad  \\text{ if } \\frac{n_1 + n_2 - p -1}{(n_1 + n_2 -2)p} T^2 &gt; F_{\\alpha; p, n_1 + n_2 - p -1}\n\\] 또는 다음과 같이 계산한 p-값(p-value) 가 유의수준 \\(\\alpha\\) 보다 작으면 귀무가설을 기각한다.\n\\[\n\\text{p-value } = P \\left ( F_{p, n_1 + n_2 - p -1} &gt; \\frac{n_1 + n_2 - p -1}{(n_1 + n_2 -2)p} T^2 \\right )\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>다변량 가설 검정</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-test.html#예제-두-그룹의-평균벡터-검정",
    "href": "qmd/multivar-test.html#예제-두-그룹의-평균벡터-검정",
    "title": "3  다변량 가설 검정",
    "section": "3.4 예제: 두 그룹의 평균벡터 검정",
    "text": "3.4 예제: 두 그룹의 평균벡터 검정\n이 예제에서는 다변량 통계학에서 가장 자주 사용되는 피셔의 아이리스(Fisher’s Iris) 자료를 이용하여 두 그룹의 평균벡터에 대한 검정을 배워보자.\nR에 내장된 iris(Fisher’s Iris) 자료는 1930년대 식물학자 Edgar Anderson 가 채집하고 측정한 붓꽃(iris) 데이터를 통계학자 R. A. Fisher(1936) 가 선형판별분석(Linear Discrimination Anslysis) 예제로 분석하면서 널리 알려졌다. 총 3개 종(Setosa, Versicolor, Virginica) 에서 각각 50개 표본으로 구성된 균형 자료(balanced data)이며 붓꽃의 특성을 나타내는 4개의 변수(단위: cm)로 구성되어 있다.\n\nSepal.Length: 꽃받침 길이\nSepal.Width : 꽃받침 너비\nPetal.Length: 꽃잎 길이\nPetal.Width : 꽃잎 너비\nSpecies: 범주형(세 종: setosa, versicolor, virginica)\n\n\ndata(iris)\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n이제 iris 자료에서 versicolor 와 virginica 두 종(각 50개 표본, \\(p=4\\) 변수)로 두 종에 대한 평균벡터가 동등한지 검정을 R 프로그램으로 수행해보자. 먼저 두 개의 종만 포함하는 자료를 만들고 표본 통계량을 구해보자.\n\n# 패키지\n#install.packages(c(\"Hotelling\", \"biotools\"), dependencies = TRUE)\n##library(Hotelling)\n#library(biotools)\n\ndf &lt;- iris %&gt;% \n      filter(Species %in% c(\"versicolor\", \"virginica\"))\n\ndf$Species &lt;- droplevels(df$Species)  # 두 수준만\n\nhead(df)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          7.0         3.2          4.7         1.4 versicolor\n2          6.4         3.2          4.5         1.5 versicolor\n3          6.9         3.1          4.9         1.5 versicolor\n4          5.5         2.3          4.0         1.3 versicolor\n5          6.5         2.8          4.6         1.5 versicolor\n6          5.7         2.8          4.5         1.3 versicolor\n\n# 각 그룹의 표본 크기\nn1 &lt;- sum(df$Species==\"versicolor\")\nn2 &lt;- sum(df$Species==\"virginica\")\np &lt;- ncol(df)-1  # 변수 개수\nn1; n2; p\n\n[1] 50\n\n\n[1] 50\n\n\n[1] 4\n\n\n다음으로 두 그룹에 대한 평균 벡터와 두 그룹의 평균의 차이를 나타내는 벡터를 구해보자.\n\n# 그룹별 4개의 변수에 대한 평균 \nmean_vec &lt;- df %&gt;% group_by(Species) %&gt;%\n       summarise(across(Sepal.Length:Petal.Width, list(mean=mean), .names=\"{col}_{fn}\"))\nmean_vec\n\n# A tibble: 2 × 5\n  Species  Sepal.Length_mean Sepal.Width_mean Petal.Length_mean Petal.Width_mean\n  &lt;fct&gt;                &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;\n1 versico…              5.94             2.77              4.26             1.33\n2 virgini…              6.59             2.97              5.55             2.03\n\nmean_x &lt;- mean_vec %&gt;%\n  filter(Species==\"versicolor\") %&gt;%\n  select(Sepal.Length_mean, Sepal.Width_mean, Petal.Length_mean, Petal.Width_mean) %&gt;%\n  as.matrix()\n\nmean_y &lt;- mean_vec %&gt;%\n  filter(Species==\"virginica\") %&gt;%\n  select(Sepal.Length_mean, Sepal.Width_mean, Petal.Length_mean, Petal.Width_mean) %&gt;%\n  as.matrix()\n\nmean_diff &lt;- mean_x - mean_y\nmean_diff\n\n     Sepal.Length_mean Sepal.Width_mean Petal.Length_mean Petal.Width_mean\n[1,]            -0.652           -0.204            -1.292             -0.7\n\n\n이제 두 그룹에 대한 공분산 행렬을 구하고 합동 분산 추정량을 구해보자.\n\n# 그룹별 4개의 변수에 대한 공분산 행렬을 List 형식으로 저장\ncov_tbl &lt;- df %&gt;%\n  group_by(Species) %&gt;%\n  summarise(cov = list(cov(across(where(is.numeric)))), .groups = \"drop\")\ncov_tbl\n\n# A tibble: 2 × 2\n  Species    cov          \n  &lt;fct&gt;      &lt;list&gt;       \n1 versicolor &lt;dbl [4 × 4]&gt;\n2 virginica  &lt;dbl [4 × 4]&gt;\n\n# versicolor 종의 공분산 행렬 꺼내기 (마지막에 .[[1]] 은 앞의 개체에서 첫 번째 요소를 추출하는 명령)\ncov_x &lt;- cov_tbl %&gt;% filter(Species == \"versicolor\") %&gt;% pull(cov) %&gt;% .[[1]]\ncov_x\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.26643265  0.08518367   0.18289796  0.05577959\nSepal.Width    0.08518367  0.09846939   0.08265306  0.04120408\nPetal.Length   0.18289796  0.08265306   0.22081633  0.07310204\nPetal.Width    0.05577959  0.04120408   0.07310204  0.03910612\n\n# virginica 종의 공분산 행렬 꺼내기\ncov_y &lt;- cov_tbl %&gt;% filter(Species == \"virginica\") %&gt;% pull(cov) %&gt;% .[[1]]\ncov_y\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.40434286  0.09376327   0.30328980  0.04909388\nSepal.Width    0.09376327  0.10400408   0.07137959  0.04762857\nPetal.Length   0.30328980  0.07137959   0.30458776  0.04882449\nPetal.Width    0.04909388  0.04762857   0.04882449  0.07543265\n\n# 합동 공분산 행렬\nSp &lt;- ((n1-1) * cov_x  + (n2-1) * cov_y  ) / (n1 + n2 - 2)\nSp\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   0.33538776  0.08947347   0.24309388  0.05243673\nSepal.Width    0.08947347  0.10123673   0.07701633  0.04441633\nPetal.Length   0.24309388  0.07701633   0.26270204  0.06096327\nPetal.Width    0.05243673  0.04441633   0.06096327  0.05726939\n\n\n이제 다음과 같이 위에서 구한 표본 통계량을 이용하여 Hotelling \\(T^2\\) 을 다음과 같이 구할 수 있다.\n\n# 평균벡터와 공분산 행렬의 차원 확인 \ndim(mean_diff); dim(Sp)\n\n[1] 1 4\n\n\n[1] 4 4\n\n# Hotelling T^2 통계량 계산\nT2 &lt;- (n1*n2/(n1+n2)) * mean_diff %*% solve(Sp) %*% t(mean_diff)\nT2\n\n         [,1]\n[1,] 355.4721\n\n\n이제 기각역을 다음과 같이 구하고 위의 호텔링의 \\(T^2\\) 통계량과 비교해보자\n\n# 유의수준\nalpha &lt;- 0.05\n# F-분포의 임계값\nF_crit &lt;- qf(1-alpha, df1 = p, df2 = n1 + n2 - p - 1)\nF_crit\n\n[1] 2.467494\n\n\n호텔링의 \\(T^2\\) 통계량의 값 355.4721452 이 기각역 2.4674936 보다 크므로 귀무가설을 기각한다. 즉, 두 종 versicolor와 virginica의 평균벡터가 통계적으로 유의하게 다르다고 할 수 있다.\n위에서 직접구한 것과 동일한 결과를 주는 R 패키지 Hotelling의 hotelling.test() 함수를 사용하여 검정을 수행해보자. 이 함수는 등공분산 가정을 전제로 한다.\n\nlibrary(Hotelling)\nres &lt;- hotelling.test(Sepal.Length + Sepal.Width + Petal.Length + Petal.Width ~ Species, data = df, var.equal = TRUE)\nres\n\nTest stat:  355.47 \nNumerator df:  4 \nDenominator df:  95 \nP-value:  0",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>다변량 가설 검정</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-cov.html",
    "href": "qmd/multivar-cov.html",
    "title": "4  공분산 행렬의 추정",
    "section": "",
    "text": "4.1 공분산 행렬의 정의\n공분산행렬은 다변량 분석에서 여러 변수들 간의 관계를 살펴볼 수 있는 가장 기본적이고 중요한 요소이다. 공분산행렬의 구조를 이해하고 적절히 추정하는 것은 다변량 데이터 분석에서 매우 중요하다. 본 장에서는 공분산 행렬의 다양한 구조적 형태와 이를 추정하는 방법들을 간단히 소개한다.\n먼저 \\(p\\)-차원 확률벡터 \\(\\pmb X\\) 가 다변량 정규분포를 따른다고 가정하자.\n\\[\n\\pmb X  =\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\n\\vdots \\\\\nX_p\n\\end{bmatrix}\n\\sim N_p({\\pmb \\mu}, {\\pmb \\Sigma})\n\\]\n공분산 행렬 \\(\\Sigma\\) 는 다음과 같이 정의되며\n\\[\n\\pmb \\Sigma = \\mathrm{Var}(\\pmb{X}) = \\pmb{E}\\big[(\\pmb{X}- \\pmb \\mu)(\\pmb{X}-\\pmb  \\mu)^\\top\\big], \\quad\n\\pmb{X} \\in \\RR^p\n\\] 다음과 같은 성질을 가지고있다.\n\\[\n\\pmb{a}^{\\top} {\\pmb \\Sigma} \\pmb{a} \\ge 0 \\quad \\text{ for all } \\pmb{a} \\in \\RR^p\\]\n이제 다변량 정규분포를 따르는 예제 데이터를 생성하고, 표본 공분산 행렬을 계산해 보자. 평균는 모두 0 이고 다음과 같은 공분산행렬을 가지는 6차원 다변량 정규분포를 고려한다. 다변량 정규분포에서 100개의 표본을 임의로 추출한 다음 표본 공분산 행렬을 구해보자.\n\\[\n\\Sigma = \\begin{bmatrix}\n  1.0 &  0.4 &  0.2 &  0.5 &  0.1 & -0.2 \\\\\n  0.4 &  1.0 &  0.3 &  0.4 & -0.3 &  0.01 \\\\\n  0.2 &  0.3 &  1.0 &  0.3 &  0.2 & -0.1 \\\\\n  0.5 &  0.4 &  0.3 &  1.0 &  0.4 & -0.2 \\\\\n  0.1 & -0.3 &  0.2 &  0.4 &  1.0 &  0.2 \\\\\n-0.2 &  0.01 & -0.1 & -0.2 &  0.2 &  1.0\n\\end{bmatrix}\n\\]\n# 예제 데이터 (다변량 정규)\n\n# 공분산 행렬\nSigma_true &lt;- matrix(c(\n  1.0,  0.4,  0.2,  0.5,  0.1, -0.2,\n  0.4,  1.0,  0.3,  0.4, -0.3,  0.01,\n  0.2,  0.3,  1.0,  0.3,  0.2, -0.1,\n  0.5,  0.4,  0.3,  1.0,  0.4, -0.2,\n  0.1, -0.3,  0.2,  0.4,  1.0,  0.2,\n -0.2,  0.01, -0.1, -0.2,  0.2,  1.0\n), 6, 6, byrow = TRUE)\n\nSigma_true\n\n     [,1]  [,2] [,3] [,4] [,5]  [,6]\n[1,]  1.0  0.40  0.2  0.5  0.1 -0.20\n[2,]  0.4  1.00  0.3  0.4 -0.3  0.01\n[3,]  0.2  0.30  1.0  0.3  0.2 -0.10\n[4,]  0.5  0.40  0.3  1.0  0.4 -0.20\n[5,]  0.1 -0.30  0.2  0.4  1.0  0.20\n[6,] -0.2  0.01 -0.1 -0.2  0.2  1.00\n\n# 고유값 확인- 공분산행렬의 양반정치 점검\neigen(Sigma_true)$values  \n\n[1] 2.1507035 1.3633588 1.0108471 0.8176237 0.4860156 0.1714512\n\n# 다변량 정규분포에서 표본 생성\nset.seed(123) # 재현 가능성\nX &lt;- rmvnorm(n = 100, mean = c(0, 0, 0, 0, 0, 0), sigma = Sigma_true)\ncolnames(X) &lt;- c(\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\")\n\n# 표본의 일부 보기\nhead(X)\n\n              x1           x2         x3          x4         x5         x6\n[1,] -0.60697291 -0.016012988  1.3739779 -0.09555296  0.5483513  1.6489336\n[2,]  0.06807651 -1.514672553 -0.7643606 -0.39888978  1.2909460  0.4933002\n[3,]  0.98823956  0.285412440 -0.1300729  2.00665652  0.5701723 -2.0653381\n[4,]  0.46989680 -0.333468735 -1.1591075 -0.42188577 -1.0883766 -0.8543093\n[5,] -0.99065257 -1.222874588  0.3448429 -0.68099634 -0.4556374  1.0022332\n[6,]  0.58628469 -0.005235656  1.0113511  1.05145508  1.2414359  0.5797083\n\n# 표본 공분산 행렬\nS_hat &lt;- cov(X)\nS_hat\n\n            x1          x2         x3         x4          x5          x6\nx1  0.88057500  0.33490828  0.1872037  0.3207651 -0.05887987 -0.32665921\nx2  0.33490828  0.88314376  0.3453332  0.2363243 -0.37813166 -0.06704301\nx3  0.18720366  0.34533319  1.0006850  0.1587787  0.10083233 -0.01456920\nx4  0.32076508  0.23632432  0.1587787  0.8100167  0.28207315 -0.44437452\nx5 -0.05887987 -0.37813166  0.1008323  0.2820731  0.82579538  0.04660305\nx6 -0.32665921 -0.06704301 -0.0145692 -0.4443745  0.04660305  1.06484897\n\n# 추정량과 실제 공분산 행렬 비교: 각 분산과 공분산의 상대적인 오차(%)\nround(100 * (S_hat - Sigma_true) / (Sigma_true), 2)\n\n        x1      x2     x3     x4      x5      x6\nx1  -11.94  -16.27  -6.40 -35.85 -158.88   63.33\nx2  -16.27  -11.69  15.11 -40.92   26.04 -770.43\nx3   -6.40   15.11   0.07 -47.07  -49.58  -85.43\nx4  -35.85  -40.92 -47.07 -19.00  -29.48  122.19\nx5 -158.88   26.04 -49.58 -29.48  -17.42  -76.70\nx6   63.33 -770.43 -85.43 122.19  -76.70    6.48",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>공분산 행렬의 추정</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-cov.html#공분산-행렬의-정의",
    "href": "qmd/multivar-cov.html#공분산-행렬의-정의",
    "title": "4  공분산 행렬의 추정",
    "section": "",
    "text": "대각원소는 각 변수의 분산, 비대각원소는 변수 간의 공분산을 나타낸다.\n대칭 행렬(symmeric matrix): \\(\\pmb \\Sigma^\\top = \\pmb \\Sigma\\)\n양반정치 행렬(semi-positive matrix):\n\n\n\n양반정치를 확인하는 방법은 공분산 행렬의 고유값이 모두 0 보다 같거나 커야한다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>공분산 행렬의 추정</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-cov.html#공분산-행렬의-형태",
    "href": "qmd/multivar-cov.html#공분산-행렬의-형태",
    "title": "4  공분산 행렬의 추정",
    "section": "4.2 공분산 행렬의 형태",
    "text": "4.2 공분산 행렬의 형태\n\n일반형 공분산\n\np-차원 공분산 행렬의 일반적인 구조(general 또는 unstructured) 는 다음과 같으며 대칭 행렬이기 떄문에 모수(parameter)의 개수는 \\(p(p-1)/2\\) 개이다.\n\\[\n\\pmb \\Sigma =\n\\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1p} \\\\\n\\sigma_{12} & \\sigma_{22} & \\cdots & \\sigma_{2p} \\\\\n\\vdots      & \\vdots      & \\ddots & \\vdots      \\\\\n\\sigma_{1p} & \\sigma_{2p} & \\cdots & \\sigma_{pp}\n\\end{bmatrix}\n\\tag{4.1}\\]\n\n독립과 등분산\n\n다변량 정규분포에서 모든 변수 \\(X_1, X_2, \\dots, X_p\\) 가 독립인 경우 공분산이 0인 것과 동일하기 떄문에 다음과 같은 대각행렬의 형태를 가지게 된다.\n\\[\n\\pmb \\Sigma =\n\\begin{bmatrix}\n\\sigma_1^2 & 0 & \\cdots & 0 \\\\\n0 & \\sigma_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_p^2\n\\end{bmatrix}\n\\]\n이 경우 다음과 같이 각 확률변수가 독립적으로 분산이 다른 일변량 정규분포를 따른다고 할 수 있다.\n\\[\nX_i \\sim_{indep} N(\\mu_i, \\sigma_i^2), \\quad i=1,2,\\dots,p\n\\] 더 나아가 모든 변수의 분산이 동일한 경우(등분산)에는 다음과 같은 구형(spherical) 형태의 공분산 행렬을 가진다.\n\\[\n\\pmb \\Sigma =\n\\begin{bmatrix}\n\\sigma^2 & 0 & \\cdots & 0 \\\\\n0 & \\sigma^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma^2\n\\end{bmatrix}\n=\n\\sigma^2 {\\pmb I}_p\n\\]\n\n균등 상관 구조\n\n특별한 구조를 고려하는 경우, 공분산 행렬의 형태로 가장 자주 사용되는 구조가 균등 상관 구조(Compound Symmetry, CS) 이다. 이 형태는 분산이 모두 동일하고 공분산도 모두 동일한 형태이며 분산과 공분산은 다르게 설정된다. 즉 모든 변수의 분산이 \\(\\sigma^2\\) 이고 모든 변수 쌍들의 공분산이 \\(\\rho\\sigma^2\\) 인 경우이다. 따라서 \\(\\rho\\) 는 상관 계수이다. 따라서 모든 변수의 상관계수도 동일한 형태를 가지는 구조이다.\n\\[\ncor(X_i, X_j) = \\rho \\quad \\text{ for all } i,j\n\\]\n\\[\n\\pmb \\Sigma_{\\text{CS}} =\n\\begin{bmatrix}\n\\sigma^2 & \\rho\\sigma^2 & \\cdots & \\rho\\sigma^2 \\\\\n\\rho\\sigma^2 & \\sigma^2 & \\cdots & \\rho\\sigma^2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho\\sigma^2 & \\rho\\sigma^2 & \\cdots & \\sigma^2\n\\end{bmatrix}\n= \\sigma^2 \\big [ (1-\\rho) \\pmb  I_p + \\rho \\pmb  J_p \\big ]\n\\tag{4.2}\\]\n위의 식에서 \\(\\pmb J_p\\) 는 모든 원소가 1인 \\(p\\times p\\) 행렬이다.\n\n4.2.1 AR(1) 구조\n공분산 행렬의 또 다른 구조적 형태로는 AR(1) 구조가 있다. 이 형태는 시계열 자료에서 자주 사용되는 구조로서, 인접한 변수들 간의 상관관계가 멀어질수록 지수적으로 감소하는 특징을 가진다.\n\\[\ncor(X_i, X_j) =  \\rho^{|i-j|}\n\\]\nAR(1) 구조의 공분산 행렬은 다음과 같은 형태를 가진다.\n\\[\n\\pmb \\Sigma_{AR} =\n\\begin{bmatrix}\n\\sigma^2 & \\rho\\sigma^2 & \\rho^2\\sigma^2 & \\cdots & \\rho^{p-1}\\sigma^2 \\\\\n\\rho\\sigma^2 & \\sigma^2 & \\rho\\sigma^2 & \\cdots & \\rho^{p-2}\\sigma^2 \\\\\n\\rho^2\\sigma^2 & \\rho\\sigma^2 & \\sigma^2 & \\cdots & \\rho^{p-3}\\sigma^2 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho^{p-1}\\sigma^2 & \\rho^{p-2}\\sigma^2 & \\rho^{p-3}\\sigma^2 & \\cdots & \\sigma^2\n\\end{bmatrix}\n\\]\n따라서 AR(1) 구조를 가진 다변량 확률 벡터의 상관 계수 행렬은 다음과 같이 나타낼 수 있다.\n\\[\n\\pmb R =\n\\begin{bmatrix}\n1 & \\rho & \\rho^2 & \\cdots & \\rho^{p-1} \\\\\n\\rho & 1 & \\rho & \\cdots & \\rho^{p-2} \\\\\n\\rho^2 & \\rho & 1 & \\cdots & \\rho^{p-3} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho^{p-1} & \\rho^{p-2} & \\rho^{p-3} & \\cdots & 1\n\\end{bmatrix}\n\\]\n\n\n4.2.2 블록 대각 구조\n공분산 행렬의 또 다른 구조적 형태로는 블록 대각(Block Diagonal) 구조가 있다. 이 형태는 변수들이 여러 개의 그룹으로 나누어져 있고, 각 그룹 내에서는 변수들 간에 상관관계가 존재하지만, 그룹 간에는 상관관계가 없는 경우에 적합하다. 블록 대각 구조의 공분산 행렬은 다음과 같은 형태를 가진다.\n\\[\n\\pmb \\Sigma =\n\\begin{bmatrix}\n\\pmb  \\Sigma_{1} & \\pmb  0  & \\pmb  0\\\\\n\\pmb  0 & \\pmb  \\Sigma_{2} & \\pmb  0 \\\\\n\\pmb  0 & \\pmb  0 & \\pmb  \\Sigma_{3}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>공분산 행렬의 추정</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-cov.html#공분산의-추정",
    "href": "qmd/multivar-cov.html#공분산의-추정",
    "title": "4  공분산 행렬의 추정",
    "section": "4.3 공분산의 추정",
    "text": "4.3 공분산의 추정\n\n4.3.1 표본 공분산 행렬\n만약 \\(n\\) 개의 표본 \\(\\pmb X_1, \\pmb X_2, \\dots, \\pmb X_n\\) 이 관측되었다고 하자.\n만약 분포의 가정에서 공분산이 제약이 없는 일반적인 형태 식 4.1 라고 한다면 다음과 같은 표본 공분산 행렬을 이용하여 추정한다. 이 추정량은 불편추정량(unbiased estimator)이고 동시에 \\(n\\) 이 충분히 크면 최대가능도 추정량과 동일하다고 볼 수 있다.\n\\[\n\\hat{\\Sigma} = \\frac{1}{n-1}\\sum_{i=1}^n (\\pmb{X}_i-\\bar{\\pmb{X}})(\\pmb{X}_i-\\bar{\\pmb{X}})^\\top\n\\tag{4.3}\\]\n특별한 구조를 가진 공분산 행렬은 최대가능도 추정을 이용하여 추정할 수 있다.\n\n\n4.3.2 예제: 반복측정자료\n이 장에서는 의학통계에 자주 사용되는 반복측정자료 형태의 다변량 확률벡터에 대하여 균등 상관 구조 형태의 공분산 행렬을 추정하는 예제에 대해서 살펴 보자.\n확률변수 \\(X_i\\) 는 \\(i\\) 시점에서 순서대로 5번 관측한 자료이며 한 명의 개체가 각 시점에서 반응값을 측정한다고 하자. 따라서 반복으로 측정한 확률 변수 \\(X_1, X_2, \\dots, X_5\\) 는 독립이 아니다.\n이제 확률 벡터 \\(\\pmb X = (X_1, X_2, \\dots, X_5)^t\\) 가 다변량 정규분포를 따른다고 가정하자. 5-차원의 다변량 정규 확률 벡터를 고려하고 각 변수의 평균은 시간을 나타내는 시점 (\\(i\\))에 비례하게 다음과 같이 정의한다.\n\\[\n\\mu_i = E(X_i) = 0.5 + 0.1 (i - 1)\n\\]\n공분산 행렬은 식 4.2 의 균등 상관 구조를 가지며 분산은 모두 1 이고 상관계수는 0.6으로 가정한다. 표본의 개수는 100 개이다.\n먼저 주어진 평균벡터와 공분산 행렬에 대하여 분포를 정의하고 가상의 자료를 임의로 추출하는 R 코드를 고려한다.\n\nset.seed(121)\nn  &lt;- 100          # 표본 수\np  &lt;- 5            # 확률 벡터의 차원 \n\n# 각 개체의 id 와 시간 변수 생성\nid &lt;- factor(rep(1:n, each = p))\ntime &lt;- rep(1:p, times = n)\nmu &lt;- 0.5 + 0.1 * (rep(1:p, times = 1) - 1)  # 평균 벡터\n\n#   p- 차원 CS 공분산을 만드는 함수 \nmake_Sigma_CS &lt;- function(p, sigma2 = 1, rho = 0.3) {\n  if (rho &lt;= -1/(p - 1) || rho &gt;= 1) {\n    stop(\"rho must be in (-1/(p-1), 1) to ensure positive definiteness.\")\n  }\n  J &lt;- matrix(1, p, p)         # 모든 원소가 1인 행렬\n  Sigma &lt;- sigma2 * ((1 - rho) * diag(p) + rho * J)\n  return(Sigma)\n}\n\n# 균등 상관 구조의 공분산 행렬\nSigma_true &lt;- make_Sigma_CS(p, sigma2 = 1, rho = 0.6)\nSigma_true\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  1.0  0.6  0.6  0.6  0.6\n[2,]  0.6  1.0  0.6  0.6  0.6\n[3,]  0.6  0.6  1.0  0.6  0.6\n[4,]  0.6  0.6  0.6  1.0  0.6\n[5,]  0.6  0.6  0.6  0.6  1.0\n\n\n이제 100 개의 표본을 다변량 정규분포에서 임의로 추출하고 wide 형식의 자료로 변환해 보자.\n\nXmat &lt;- rmvnorm(n = n, mean = mu, sigma = Sigma_true) # 다변량 표본 추출\ncolnames(Xmat) &lt;- paste0(\"X\", 1:p)\n\ndf_wide &lt;- as.data.frame(Xmat) # wide 형식의 자료\nhead(df_wide)\n\n          X1          X2         X3          X4          X5\n1  0.1412804  0.47132670  0.5836000  0.55150916  0.25141129\n2  1.9259813  1.19236353  1.0678925  0.50890233  1.88213477\n3  1.4160815  0.08403608  0.4753524  0.47093252  1.27335173\n4  1.1549582 -0.30705522  1.4909395  0.08919076  1.23971146\n5  1.2631167  1.31918169  1.2023846  1.90247109  1.83556681\n6 -0.6336252  0.47159709 -0.6432958 -0.37359215 -0.06331619\n\n\n추출된 표본을 이용하여 식 4.3 에 주어진 표본 공분산 행렬을 계산해 보자. 표본 공분산 행렬은 제약조건이 없는 형태로 나타나기 때문에 모든 분산과 공분산이 각각 다르게 추정된다.\n\ncov(df_wide)\n\n          X1        X2        X3        X4        X5\nX1 0.7180133 0.4216849 0.3886342 0.3553769 0.4661159\nX2 0.4216849 0.8704348 0.4896497 0.4478777 0.4441199\nX3 0.3886342 0.4896497 0.8766630 0.4369850 0.5013652\nX4 0.3553769 0.4478777 0.4369850 0.7887407 0.4043893\nX5 0.4661159 0.4441199 0.5013652 0.4043893 0.9236021\n\n\n이제 균등 상관 구조를 가지는 공분산을 추정할 수 있는 방법을 알아보자. 먼저 자료는 측정 시간을 변수로 하는 긴 형식의 자료로 생성한다.\n\n# pivot_longer 함수를 이용하여 긴 형식의 자료를 생성  \ndf_long &lt;- df_wide %&gt;%\n  mutate(id = factor(1:n)) %&gt;%\n  pivot_longer(cols = starts_with(\"X\"), \n               names_to = \"time\", \n               values_to = \"y\",\n               names_prefix = \"X\") %&gt;%\n  mutate(time = as.integer(time))\nhead(df_long, 10)\n\n# A tibble: 10 × 3\n   id     time     y\n   &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;\n 1 1         1 0.141\n 2 1         2 0.471\n 3 1         3 0.584\n 4 1         4 0.552\n 5 1         5 0.251\n 6 2         1 1.93 \n 7 2         2 1.19 \n 8 2         3 1.07 \n 9 2         4 0.509\n10 2         5 1.88 \n\n\n공분산 추정을 위하여 nlme 패키지에 있는 다변량 확률변수의 회귀식을 추정하는 함수 nlme 를 사용하여고 하며, 균등 상관 구조를 가지는 공분산을 가정한다.\n다음 추정된 다변량 회귀모형의 결과를 보면 식 4.2 의 균등 상관 구조에서 분산 \\(\\sigma\\) 의 추정값은 \\(0.8362 = (0.9144)^2\\), 상관계수 \\(\\rho\\) 의 추정값은 \\(0.52078\\) 로 나타난다. 참고로 아래 결과에서 rho 는 공통 상관계수의 추정값,Residual standard error 는 표준편차 추정값이다.\n\nfit_gls_cs &lt;- gls(y ~ time, data = df_long,\n                  correlation = corCompSymm(form = ~ 1 | id))\n\nsummary(fit_gls_cs) \n\nGeneralized least squares fit by REML\n  Model: y ~ time \n  Data: df_long \n       AIC      BIC    logLik\n  1163.245 1180.088 -577.6226\n\nCorrelation Structure: Compound symmetry\n Formula: ~1 | id \n Parameter estimate(s):\n      Rho \n0.5207752 \n\nCoefficients:\n                Value  Std.Error  t-value p-value\n(Intercept) 0.3750164 0.09360729 4.006274   1e-04\ntime        0.0817480 0.02001772 4.083779   1e-04\n\n Correlation: \n     (Intr)\ntime -0.642\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-2.76158379 -0.71458351  0.04453439  0.76834584  3.26193595 \n\nResidual standard error: 0.9144187 \nDegrees of freedom: 500 total; 498 residual\n\n# rho 는 공통 상관계수, Residual standard error 는 표준편차 추정값\n\n위에서 주어진 결과를 가지고 공분산 행렬의 추정값을 다음과 같이 구할 수 있다.\n\ngetVarCov(fit_gls_cs) \n\nMarginal variance covariance matrix\n        [,1]    [,2]    [,3]    [,4]    [,5]\n[1,] 0.83616 0.43545 0.43545 0.43545 0.43545\n[2,] 0.43545 0.83616 0.43545 0.43545 0.43545\n[3,] 0.43545 0.43545 0.83616 0.43545 0.43545\n[4,] 0.43545 0.43545 0.43545 0.83616 0.43545\n[5,] 0.43545 0.43545 0.43545 0.43545 0.83616\n  Standard Deviations: 0.91442 0.91442 0.91442 0.91442 0.91442",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>공분산 행렬의 추정</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-discrim.html",
    "href": "qmd/multivar-discrim.html",
    "title": "5  판별분석",
    "section": "",
    "text": "5.1 분포와 판별규칙\n의사결정은 주어진 유한개의 선택들 중에서 하나를 고르는 것이다. 예를 들어 은행에서 대출 신청자에게 돈을 빌려줄지 말지, 이동 시에 택시를 탈지, 버스를 탈지 또는 지하철을 탈지 결정해야 한다. 인간은 의사결정을 할 때 어느 정도의 자신만의 규칙에 따라 움직이며(아닌 경우도 많지만..) 여러 번의 시행 착오 등을 거쳐서 좋은 선택을 위해 규칙을 바꾸기도 한다. 또한 유한개의 선택들은 대체로 두 가지의 선택이 있으며 둘 중 하나를 선택하는 경우가 많다.\n예를 들어, 신용평가에서 은행은 과거 경험을 통해 두 부류의 고객이 있음을 알고 있다. 즉, 대출을 아무 문제 없이 상환하는 안전 고객과 상환에 어려움을 겪은 위험 고객이다. 새로운 고객이 대출을 신청할 때, 은행은 대출을 해줄지 말지를 결정해야 한다. 은행의 과거 기록은 두 부류의 고객에 대한 다수의 특성값들를 제공하며, 여기에는 나이, 급여, 혼인 여부, 대출 금액 등과 같은 다양한 정보가 포함된다. 새로운 고객은 새로운 특성값을 가지고 있으며 판별 규칙(Discrimination rule)은 이 새로운 고객을 두 집단 중 하나로 분류해야 하는 규칙을 말하는 것이다.\n판별분석(Discriminant analysis)은 이러한 의사결정에서 통계적 분포과 방법을 사용하여 자료에 기반한 규칙을 정하는 방법이다. 기본적으로 전체 집단(모집단)이 두 개 이상의 집단들로 나누어져 있다고 생각하고 그 집단들에 대한 확률적 가정(분포 가정)을 고려한다. 같은 집단에 속하는 개체들은 유사하며 다른 집단에 속한 개체들은 그 특성이 다르다고 가정하고 이러한 집단의 특성을 확률적 분포로 나타낸다.\n예를 들어 이동시 자가용을 타는 사람들과 대중교통을 타는 사람들은 소득의 분포가 다르다고 가정할 수 있다. 이러한 가정 아래 새로운 개체를 어느 집단에 배정하는지에 대한 규칙을 자료를 이용하여 정하는 방법이 판별분석이며 다양한 분류방법(classification)의 출발점이다. 유의할 점은 규칙을 정할 때 어떤 규칙이 좋은 것인지에 대한 기준을 생각해야 한다. 동일한 상황에서 두 가지 규칙을 비교할 수 있어야 더 나은 의사결정을 할 수 있다.\n판별분석에서는 다음과 같은 개념들을 생각헤야 한다.\n이 장에서는 두 개의 집단(population) \\(P_1\\) 과 \\(P_2\\) 을 고려한다. 예를 들어 은행에서는 전체 고객을 안전 고객과 위험 고객, 두 집단으로 나눌 수 있다.\n각 집단에 속하는 개체들의 특성(확률벡터) \\(\\pmb X\\) 은 속하는 집단에 따라서 각각 분포 \\(F_i(\\pmb x)\\), \\(i=1,2\\) 를 따른다고 가정한다. 또한 \\(f_i(\\pmb x)\\)를 분포 \\(F_i\\)의 확률밀도함수라고 하자.\n판별 규칙(discrimination rule) 은 새로운 개체의 특성 \\(\\pmb X = \\pmb x\\) 가 주어졌을 때 이 개체가 어느 집단에 속하는지 결정하는 방법이다. 이러한 규칙은 개체의 특성삾이 가질 수 있는 전체 공간(표본공간)을 겹치지 않는 두 집합 \\(R_1\\) 과 \\(R_2\\) 로 나누고 관측된 값이 \\(R_1\\) 에 속하면 새로운 개체를 \\(P_1\\) 에 베정하고, 반대로 \\(R_2\\) 에 속하면 \\(P_2\\)에 배정하는 것으로 주어진다.\n\\[\n\\text { where }  R_1 \\cup R_2 = \\RR^p, \\quad R_1 \\cap R_2 = \\emptyset\n\\tag{5.1}\\]\n즉, \\(R_1\\) 과 \\(R_2\\) 는 전체 공간을 겹치지 않게 나누는 두 집합이다. 판별 규칙은 다음과 같이 쓸 수 있다. \\[\n\\pmb x \\in R_1 \\Rightarrow \\text{개체를 } P_1 \\text{에 배정} \\\\\n\\pmb x \\in R_2 \\Rightarrow \\text{개체를 } P_2 \\text{에 배정}\n\\tag{5.2}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>판별분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-discrim.html#분포와-판별규칙",
    "href": "qmd/multivar-discrim.html#분포와-판별규칙",
    "title": "5  판별분석",
    "section": "",
    "text": "5.1.1 판별 오류와 비용\n이제 \\(i\\) 집단에 속한 개체가 주어진 판별 규칙에 따라서 \\(j\\) 집단에 배정될 사건과 확률을 생각헤 보자. 먼저 \\(i\\) 집단에 속한 개체가 \\(j\\) 집단에 배정될 사건을 \\(A(j|i) \\equiv A_{ji}\\) 라고 정의한다. 따라서 각 개체가 자신이 속한 집단에 배정된 사건, \\(A_{11}\\) 과 \\(A_{22}\\) 가 발생하면 판별규칙이 잘 적용된 경우이다. 반대로 개체가 자신이 속하지 않은 다른 집단에 배정된 사건, \\(A_{12}\\) 과 \\(A_{21}\\) 가 발생하면 오류가 발생한 경우이며 이러한 경우 오류에 의한 비용 \\(c_{12}\\), \\(c_{21}\\) 이 발생한다.\n예를 들어 은행에서 대출 신청자가 우량 고객 \\(P_1\\) 에 속하는데 불량 고객 \\(P_2\\) 로 잘못 판별되어 대출을 거절하는 경우, 은행은 대출 이익을 얻지 못하는 손실 \\(c_{21}\\) 이 발생한다. 반대로 불량 고객 \\(P_2\\) 가 우량 고객 \\(P_1\\) 으로 잘못 판별되어 대출을 해주는 경우, 은행은 대출금을 상환받지 못하는 더 큰 손실 \\(c_{12}\\) 이 발생한다. 따라서 두 가지 오류에 대한 비용은 일반적으로 다르며, 이러한 비용을 고려하여 판별 규칙을 정하는 것이 중요하다.\n\n\n5.1.2 최대가능도 규칙\n이제 위에서 정의한 사건과 비용을 이용하여 판별 분석에서 다루는 중요한 확률을 생각해보자. 먼저 첫 집단 \\(P_1\\) 에 속한 개체가 주어진 판별 규칙에 따라서 두 번째 집단 \\(P_2\\) 에 배정될 사건확률은 다음과 같다.\n\\[\np_{21} \\equiv P(A_{21}) = \\int_{R_2} f_1(\\pmb x) d\\pmb x\n\\tag{5.3}\\]\n마찬가지로 \\(P_2\\) 에 속한 개체가 \\(P_1\\) 에 배정될 확률은 다음과 같다.\n\\[\np_{12} \\equiv P(A_{12}) = \\int_{R_1} f_2(\\pmb x) d\\pmb x\n\\tag{5.4}\\]\n위에서 정의한 두 확률 \\(p_{12}\\) 와 \\(p_{21}\\) 은 판별에서 오류가 발생한 확률이다.\n판별 규칙을 만드는 것은 식 5.1 과 식 5.2 에서 정의한 두 집합 \\(R_1\\) 과 \\(R_2\\) 를 정하는 것이다. 따라서 잘못된 분류를 범할 확률 식 5.3 과 식 5.4 은 판별 규칙에 따라서 달라진다. 즉, \\(R_1\\) 과 \\(R_2\\) 를 다르게 정하면 오류 확률도 달라진다.\n판별 규칙은 당연히 두 오류의 확률을 가장 작게 만드는 방향으로 정해져야 한다. 이제 앞에서 정의한 두 그룹의 분포를 이용하여 오류의 확률을 가장 작게 만드는 판별 규칙을 어떻게 정할 수 있는지 알아보자. 다음과 같이 두 오류를 범할 확률을 최소로 하는 판별 규칙을 찾으려 한다\n\\[\n\\min_{R_1, R_2} \\{ p_{12} + p_{21} \\} = \\min_{R_1, R_2} \\left [ \\int_{R_1} f_2(\\pmb x) d\\pmb x + \\int_{R_2} f_1(\\pmb x) d\\pmb x \\right ]\n\\tag{5.5}\\]\n위의 식은 두 오류 확률의 합을 최소로 하는 판별 규칙을 찾는 문제이다. 두 종류의 오류를 범할 확률의 합을 다시 살펴보면 다음과 같이 쓸 수 있다.\n\\[\n\\begin{aligned}\np_{12} + p_{21} & = \\int_{R_1} f_2(\\pmb x) d\\pmb x + \\int_{R_2} f_1(\\pmb x) d\\pmb x \\\\\n& = \\int_{R_1} f_2(\\pmb x) d\\pmb x + 1 - \\int_{R_1} f_1(\\pmb x) d\\pmb x \\\\\n& = 1 + \\int_{R_1} \\{ f_2(\\pmb x) - f_1(\\pmb x) \\} d\\pmb x\n\\end{aligned}\n\\tag{5.6}\\]\n참고로 위의 식은 전채 공간에서의 확률밀도함수의 적분이 1임을 이용하였다.\n\\[\n\\int_{R_1} f_1{\\pmb x} d \\pmb x + \\int_{R_2} f_1(\\pmb x) d\\pmb x = 1\n\\]\n이제 식 5.6 의 적분값을 최소로 하는 \\(R_1\\) 을 찾으면 된다. 이 적분값을 최소로 하기 위해서는 적분하는 영역 \\(R_1\\) 에서 적분하는 함수 \\(f_2(\\pmb x) - f_1(\\pmb x)\\) 의 값이 음수인 부분만 포함되도록 하면 된다. 즉, 다음과 같은 조건을 만족하는 \\(R_1\\) 을 찾으면 된다.\n\\[\nR_1 = \\{ \\pmb x | f_2(\\pmb x) - f_1(\\pmb x) &lt; 0 \\} = \\{ \\pmb x | f_1(\\pmb x) &gt; f_2(\\pmb x) \\}\n\\tag{5.7}\\]\n만약 \\(R_1\\) 을 식 5.7 과 같이 정하면 \\(R_2\\) 는 자동으로 다음과 같이 정해진다.\n\\[\nR_2 = \\{ \\pmb x | f_2(\\pmb x) \\ge f_1(\\pmb x) \\}\n\\tag{5.8}\\]\n위의 유도에서 판별 규칙을 만들 때 중요한 요인은 두 확률밀도함수의 비(ratio)이다. 두 확률밀도함수의 비가 1 보다 큰 값을 가지는 표본 \\(\\pmb x\\) 의 영역과 1 보다 작은 값을 가지는 표본 \\(\\pmb x\\) 의 영역이 각각 \\(R_1\\) 과 \\(R_2\\)가 되는 것이다.\n\\[  \\frac{f_1(\\pmb x)}{f_2(\\pmb x)} \\]\n앞에서 살펴보았듯이, 판별 함수를 만드는 가장 기본적인 방법은 최대 가능도 규칙(Maximum Likelihood Rule)이다. 이 방법은 새로운 개체의 특성 \\(\\pmb X = \\pmb x\\) 가 주어졌을 때, 각 집단에서 이 특성이 관측될 가능도(likelihood function)를 계산하여 더 큰 집단에 개체를 배정하는 방법이다. 즉, 다음과 같이 판별 규칙을 정의한다.\n\\[\n\\pmb x \\in R_1 \\text{ if } f_1(\\pmb x) &gt; f_2(\\pmb x), \\quad \\text{ and } \\quad \\pmb x \\in R_2 \\text{ if } f_1(\\pmb x) \\le f_2(\\pmb x)\n\\]\n\n\n5.1.3 베이지안 규칙\n만약 각 집단에 속할 사전 확률(prior probability)이 각각 \\(\\pi_1\\) , \\(\\pi_2\\) 로 주어졌다고 하자. 사전확률은 관측하는 확률벡터의 분포를 고려하기 전에, 어떤 개체가 각 집단에 속할 가능성을 확률을 나타낸 것이다.\n예를 들어 앞에서 생각한 예제에서 은행의 대출 신청자가 우량 고객 집단에 속할 가능성과 물량 고객 집단에 속할 가능성이 다를 수 있다. 대부분의 사람들이 대출을 계획대로 상환하였다면 임의의 고객이 우량 고객일 가능성이 불량 고객일 가능성보다 매우 크다고 할 수 있다. 이러한 사전 정보를 고려하여 판별 규칙을 정하는 임의의 고객이 우량 고객 집단 \\(P_1\\) 에 속할 사전 확률을 \\(\\pi_1\\) 이라고 하고 불량 고객 \\(P_2\\) 에 속할 사전 확률이 \\(\\pi_2\\) 라고 할 수 있는 것이다.\n\\[ \\pi_1  = P(\\text{고객} \\in  P_1), \\quad \\pi_2 = P(\\text{고객} \\in P_2), \\quad \\pi_1 + \\pi_2 =1 \\]\n사전확률을 고려하는 경우, 베이즈 판별 규칙은 다음과 같이 정의된다. 즉, 새로운 개체의 특성 \\(\\pmb X = \\pmb x\\) 가 주어졌을 때, 각 집단에서 이 특성이 관측될 가능도(likelihood function)에 사전 확률을 곱한 값을 계산하여 더 큰 집단에 개체를 배정하는 방법이다. 이 경우 베이즈 판별 규칙(Bayes Discrimination Rule)은 다음과 같이 정의된다.\n\\[\n\\pmb x \\in R_1 \\text{ if } \\pi_1 f_1(\\pmb x) &gt; \\pi_2 f_2(\\pmb x), \\quad \\text{ and } \\quad \\pmb x \\in R_2 \\text{ if } \\pi_1 f_1(\\pmb x) \\le \\pi_2 f_2(\\pmb x)\n\\]\n위의 규칙에서 가능도 함수와 사전 확률의 곱을 사후 확률(posterior probability)이라고 한다. 사후 확률은 관측된 표본의 값이 주어진 경우 개체가 집단에 속할 확률을 의미한다.\n\\[\nP (P_i | \\pmb X = \\pmb x) \\propto \\pi_i f_i(\\pmb x), \\quad i=1,2\n\\]\n\n\n5.1.4 최적 판별 규칙\n이제 판별에서 발생하는 오류에 대한 비용도 함께 고려할 수 있는 최적의 규칙을 고려해 보자. 앞 절에서 정의한 오류에 대한 비용 \\(c_{12}\\) 와 \\(c_{21}\\) 을 고려할 때 최적 판별 규칙은 다음에서 정의한 기대 오류비용 (Expected cost of misclassification; ECM) 을 최소로 하는 규칙이다.\n\\[\n\\text{ECM} = c_{12} p_{12} \\pi_2 + c_{21} p_{21} \\pi_1\n\\tag{5.9}\\]\n기대 오류비용(ECM)을 최소로 하는 판별 규칙을 유도하면 다음과 같은 최적 판별 규칙을 얻을 수 있다.\n\\[\nR_1 = \\left \\{ \\pmb x | \\frac{f_1(\\pmb x)}{f_2(\\pmb x)} &gt; \\left [ \\frac{c_{12}}{c_{21}} \\right ] \\left [ \\frac{\\pi_2}{\\pi_1} \\right ] \\right \\}, \\quad R_2 = \\left \\{ \\pmb x | \\frac{f_1(\\pmb x)}{f_2(\\pmb x)} \\le \\left [ \\frac{c_{12}}{c_{21}} \\right ] \\left [ \\frac{\\pi_2}{\\pi_1} \\right ] \\right \\}\n\\tag{5.10}\\]\n식 5.10 에서 알 수 있듯이, 최대가능도 규칙과 베이즈 판별 규칙은 기대 오류비용을 최소화하는 최적 규칙의 특별한 경우이다. 즉, 비용이 동일한 경우 \\(c_{12} = c_{21}\\) 에는 베이즈 판별 규칙과 동일하다. 또한 사전 확률도 동일한 경우 \\(\\pi_1 = \\pi_2\\) 에는 최대 가능도 판별 규칙과 동일하다.\n\n예제 5.1 (두 정규 분포의 판별 규칙) 두 집단 \\(P_1\\) 과 \\(P_2\\) 에 속한 개체들의 특성 \\(\\pmb X\\) 가 각각 다음과 같은 분산이 동일한 일변량 정규 분포를 따른다고 하자.\n\\[\nX | P_1 \\sim N( \\mu_1,  \\sigma^2), \\quad  X | P_2 \\sim N(\\mu_2, \\sigma^2), \\quad \\mu_1 &lt; \\mu_2\n\\] 이제 두 정규분포 확률밀도함수의 비, 최대가능도 함수의 비율을 고려해 보자.\n\\[\n\\frac{f_1(x)}{f_2(x)} =  \\exp \\left \\{ - \\left ( \\frac{(x - \\mu_1)^2}{2\\sigma^2} \\right ) - \\left ( - \\frac{(x - \\mu_2)^2}{2\\sigma^2} \\right ) \\right \\}\n\\]\n위의 식에서 두 정규분포의 비가 1 보다 큰 표본의 영역 \\(R_1\\) 을 구하면 다음과 같다.\n\\[\n\\begin{aligned}\n&   \\frac{f_1(x)}{ f_2(x)} &gt; 1 \\\\\n& \\Longleftrightarrow   -\\frac{1}{2\\sigma^2}(x-\\mu_1)^2  + \\frac{1}{2\\sigma^2}(x-\\mu_2)^2 &gt; \\log(1) \\\\\n& \\Longleftrightarrow   2x\\mu_1 -\\mu_1^2 -2x \\mu_2 + \\mu_2^2 &gt; 0 \\\\\n& \\Longleftrightarrow   2x(\\mu_1 - \\mu_2)   &gt; \\mu_1^2 - \\mu_2^2  \\\\\n& \\Longleftrightarrow   x &lt; \\frac{\\mu_1+\\mu_2}{2}\n\\end{aligned}\n\\]\n따라서 두 집단의 평균의 중간에 있는 값 \\((\\mu_1 + \\mu_2)/2\\) 보다 관측한 값 \\(x\\) 가 작으면 그룹 \\(P_1\\) 에 속한다고 결정한다.\n\\[\nR_1 = \\left \\{ x | x &lt; \\frac{\\mu_1 + \\mu_2}{2} \\right \\}, \\quad R_2 = \\left \\{ x | x \\ge \\frac{\\mu_1 + \\mu_2}{2} \\right \\}\n\\]\n\\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>판별분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-discrim.html#다변량-정규분포와-판별-규칙",
    "href": "qmd/multivar-discrim.html#다변량-정규분포와-판별-규칙",
    "title": "5  판별분석",
    "section": "5.2 다변량 정규분포와 판별 규칙",
    "text": "5.2 다변량 정규분포와 판별 규칙\n앞 절에서는 개체의 특성을 나타내는 확률변수가 일변량 정규분포인 경우의 예제를 보았다. 일반적인 경우 개체의 특성을 나타내는 특성값은 두 개 이상인 경우가 흔하다. 따라서 이제는 개체의 특성이 여러 개의 확률변수, 즉 확률벡터로 구성되어 있다고 가정하자.\n\\(p\\)차원 확률벡터 \\(\\pmb X\\) 를 고려하며 분포는 다변량 정규분포를 가정한다. 정규분포의 가정은 강한 가정이지만 단순하고 다루기 쉬운 분포이며 실제로 사용하기 용이하다. (하지만 조심해서 가정을 검토해야함)\n두 개의 다변량정규분포를 따르는 두 집단 \\(P_1, P_2\\)으로 나누어져 있다고 가정하고 두 분포를 고려하자. 처음에는 문제를 쉽게 하기 위하여 평균은 다르고 공분산은 같다고 가정하자\n\\[\nP_1 : ~N_p (\\pmb \\mu_1, \\pmb \\Sigma) , \\quad  P_2 :~ N(\\pmb \\mu_2, \\pmb \\Sigma)   \\quad \\text{ where } \\pmb \\mu_1 \\ne \\pmb \\mu_2\n\\]\n참고로 다변량 정규 분포의 확률 밀도 함수는 다음과 같다.\n\\[\nf(\\pmb x) = (2 \\pi)^{-p/2} |\\pmb \\Sigma |^{-1/2} \\exp \\left [\n-\\frac{1}{2} (\\pmb x -\\pmb \\mu)^t \\pmb \\Sigma^{-1} (\\pmb x -\\pmb \\mu) \\right ]\n\\]\n식 5.9 에서 기대 오류비용 ECM 을 최소하는 판별규칙 식 5.10 과 같이 구할 수 있으며 이제 다변량 정규분포의 확률밀도함수를 식 5.10 에 넣고 판별함수를 유도해 보자.\n\\[\n\\begin{aligned}\n\\frac{f_1(\\pmb x)}{f_2(\\pmb x)} & =\n\\exp \\left [ -\\frac{1}{2} (\\pmb x -\\pmb \\mu_1)^t \\pmb \\Sigma^{-1} (\\pmb x -\\pmb \\mu_1) +\n\\frac{1}{2} (\\pmb x -\\pmb \\mu_2)^t \\pmb \\Sigma^{-1} (\\pmb x\n-\\pmb \\mu_2)\\right ] \\\\\n& = \\exp \\left [ (\\pmb \\mu_1-\\pmb \\mu_2)^t \\pmb \\Sigma^{-1} \\pmb x -\n\\frac{1}{2} \\pmb \\mu_1^t \\pmb \\Sigma^{-1} \\pmb \\mu_1\n+\\frac{1}{2} \\pmb \\mu_2^t \\pmb \\Sigma^{-1} \\pmb \\mu_2) \\right ]  \\\\\n&= \\exp \\left [ (\\pmb \\mu_1-\\pmb \\mu_2)^t \\pmb \\Sigma^{-1} \\pmb x -\n\\frac{1}{2} (\\pmb \\mu_1-\\pmb \\mu_2)^t \\pmb \\Sigma^{-1} (\\pmb \\mu_1+\\pmb \\mu_2) \\right ]\n\\end{aligned}\n\\]\n이제 ECM을 최소화하는 규칙은 개체의 특성이 \\(\\pmb x =\\pmb x_0\\) 로 주어진 경우 다음을 만족하면 \\(P_1\\)으로 분류한다.\n\\[\n(\\pmb \\mu_1-\\pmb \\mu_2)^t \\pmb \\Sigma^{-1} \\pmb x_0 -\n\\frac{1}{2} (\\pmb \\mu_1-\\pmb \\mu_2)^t \\pmb \\Sigma^{-1} (\\pmb \\mu_1+\\pmb \\mu_2)\n&gt; \\log \\left ( \\frac{c_{12}}{c_{21}} \\frac{\\pi_2}{\\pi_1} \\right )\n\\]\n실제로 \\(\\pmb \\mu_1\\), \\(\\pmb \\mu_2\\), \\(\\pmb \\Sigma\\)은 알 수 없기 때문에 표본 자료를 이용하여 추정을 한다.\n\\[ \\hat {\\pmb \\mu}_1 = \\bar {\\pmb x}_1 \\quad \\text{and} \\quad \\hat {\\pmb \\mu}_2 = \\bar {\\pmb x}_2  \\]\n\\[ \\hat {\\pmb \\Sigma} = \\pmb S_{p} = \\frac{(n_1-1) \\pmb S_1+(n_2-1)\n\\pmb S_2}{ n_1+n_2-2 } \\]\n여기서 \\(n_1\\), \\(n_2\\)는 표본에서 두 그룹에 대한 표본의 크기이며 \\(\\bar {\\pmb x}_1\\), \\(\\bar {\\pmb x}_2\\)는 각각의 표본평균 벡터이다. \\(\\pmb S_{p}\\)는 두 개의 그룹에서 구한 공분산 행렬의 추정치를 결합한 합동 공분산 추정량이다.\n위에서 구한 모수들의 추정치를 모집단으로 부터 구한 판별함수에 넣으면 다음과 같은 표본을 이용한 판별함수가 얻어진다.\n\\[\n(\\bar {\\pmb x}_1-\\bar {\\pmb x}_2)^t \\pmb S_{p}^{-1} \\pmb x_0 -\n\\frac{1}{2} (\\bar {\\pmb x}_1-\\bar {\\pmb x}_2)^t \\pmb S_{p}^{-1} (\\bar\n{\\pmb x}_1+\\bar {\\pmb x}_2) &gt;  \\log \\left ( \\frac{c(1|2)}{c(2|1)}\n\\frac{\\pi_2}{\\pi_1} \\right )\n\\tag{5.11}\\]\n위 식 5.11 의 표본 판별함수에서 아래와 같이 벡터 \\(\\pmb a\\)와 상수 \\(m\\)을 정의하면\n\\[\n\\begin{aligned}\n\\pmb a & =  \\pmb S_{p}^{-1} (\\bar {\\pmb x}_1-\\bar {\\pmb x}_2)   \\\\\nm &  = \\frac{1}{2} (\\bar {\\pmb x}_1-\\bar {\\pmb x}_2)^t\n\\pmb S_{p}^{-1} (\\bar {\\pmb x}_1+\\bar {\\pmb x}_2) = \\frac{1}{2}(\\pmb a^t\n\\bar {\\pmb x}_1 + \\pmb a^t \\bar {\\pmb x}_2)\n\\end{aligned}\n\\tag{5.12}\\]\nECM을 최소화하는 판별 규칙은 다음과 같이 특성값 \\(\\pmb x_0\\)의 선형함수로 나타낼 수 있다.\n\\[\n\\pmb a^t \\pmb x_0 &gt;  m + \\log \\left ( \\frac{c(1|2)}{c(2|1)} \\frac{\\pi_2}{\\pi_1} \\right )\n\\tag{5.13}\\]\n만약 비용이 같고 (\\(c_{12}=c_{21}\\)) 두 집단에 대한 사전확률이 같다면 (\\(\\pi_1=\\pi_2\\)) 판별함수는 다음과 같이 간단하게 나타낼 수 있다.\n\\[  \n\\pmb a^t \\pmb x_0     &gt; m\n\\tag{5.14}\\]\n마지막으로 두 모집단의 공분산이 다를 경우는 판별함수가 확률벡터 \\(\\pmb x\\)의 선형함수로 나타나지 않는다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>판별분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-discrim.html#fisher의-선형-판별함수",
    "href": "qmd/multivar-discrim.html#fisher의-선형-판별함수",
    "title": "5  판별분석",
    "section": "5.3 Fisher의 선형 판별함수",
    "text": "5.3 Fisher의 선형 판별함수\nFisher(1938)는 위에서 다룬 ECM을 최소화하는 판별함수를 유도하는 방법과 완전히 다른 규칙을 사용하여 ECM을 최소화하는 방법과 동일한 결과를 유도하였다.\nFisher는 다변량 확률벡터 \\(\\pmb x\\)를 일변량 변수 \\(y\\) 로 선형변환하여 차원을 축소하는 방법을 고려하였다. 차원을 축소할 때 축소된 차원의 특성값 \\(y\\)가 두 집단을 최대로 구별되게 하는 선형변환을 구하는 문제를 생각하였다. Fisher의 방법은 특성값 벡터 \\(\\pmb x\\)에 확률적 분포가정을 고려하지 않는다.\n특성값 벡터 \\(\\pmb x_1\\)을 집단 \\(P_1\\)의 특성값이라고 하고 \\(\\pmb x_2\\)을 집단 \\(P_2\\)의 특성벡터라고 하자. 이제 특성값 벡터 \\(\\pmb x_1\\)과 \\(\\pmb x_2\\)에 동시에 적용하는 선형변환 벡터를 \\(\\pmb a\\)라고 하고 각 집단의 일변량 특성값 \\(y_1\\)과 $ y_2$를 다음과 같이 정의하자.\n\\[\ny_1 = \\pmb a^t \\pmb x_1 \\quad \\text{and} \\quad y_2 = \\pmb a^t \\pmb x_2\n\\] 이제 두 집단에서 각각 \\(n_1\\), \\(n_2\\) 개의 표본을 독립적으로 추출하였다거 가정하자.\n\\[\ny_{11}, y_{12}, \\dots, y_{1 n_1} \\sim_{ind} P_1 \\quad \\text{and} \\quad y_{21}, y_{22}, \\dots, y_{2 n_2} \\sim_{ind} P_2\n\\] 각 집단의 평균벡터는 다음과 같이 일변량 평균으로 변환된다. \\[\n\\bar y_1 = \\pmb a^t \\bar {\\pmb x}_1 \\quad \\text{and} \\quad \\bar y_2 = \\pmb a^t \\bar {\\pmb x}_2\n\\]\nFisher의 방법은 변환된 일변량 변수 \\(y\\)가 두 집단 간의 차이를 최대로, 집단 내의 차이를 최소로 하도록 하는 선형변환 \\(\\pmb a\\)를 유도하는 것이다.\n두 집단 간의 차이를 최대로 하는 것은 두 평균의 차이 \\(|\\bar y_1 -\\bar y_2|\\)가 커지게 하는 것이고 집단 내의 차이를 최소로 하도록 하는 것은 집단내의 변동, 즉 집단내의 분산을 작게하는 것이다. 이러한 두 집단 간의 차이와 집단 내의 변동을 같이 반영할 수 있는 측도를 다음과 같이 생각하였다.\n\\[\n\\max_{\\pmb a} \\frac {| \\bar y_1 - \\bar y_2|} {s_y} =  \\max \\frac{ \\text{variation between groups}}{\\text{variation within group}}\n\\tag{5.15}\\]\n여기서 $s^2_y $는 두 집단의 특성값 \\(y\\)의 합동분산 추정량이다.\n\\[\ns^2_y =\\frac {\\sum_{j=1}^{ n_1}(y_{1j} -\\bar y_1)^2 + \\sum_{j=1}^{ n_2}(y_{2j} -\\bar y_2)^2 }{n_1+n_2-2}\n\\]\n이제 두 집단 간의 차이를 최대로, 집단 내의 변동를 최소로 하는 변환을 유도해보자. 식 5.15 에 제시된 값의 제곱이 가질 수 있는 상한(upper bound)을 유도하고 그 상한값을 취하는 선형변환 벡터 \\(\\pmb a\\)를 찾아보자.\n여기서 두 집단의 평균벡터의 차이 \\(\\pmb d\\)를 다음과 같이 정의한다.\n\\[\n\\pmb d =(\\bar {\\pmb x}_1 - \\bar {\\pmb x}_2)\n\\]\n이제 식 5.15 에 주어진 값의 제곱에 대한 상한을 유도해보자.\n\\[\n\\begin{aligned}\n\\frac {( \\bar y_1 - \\bar y_2)^2} {s^2_y}\n&=  \\frac{ ( \\pmb a^t \\bar {\\pmb x}_1 -  \\pmb a^t \\bar {\\pmb x}_2)^2}\n{ \\pmb a^t \\pmb S_{p}  \\pmb a } \\\\\n& =   \\frac{ [ \\pmb a^t (\\bar {\\pmb x}_1 - \\bar {\\pmb x}_2)]^2}\n{ \\pmb a^t \\pmb S_{p}  \\pmb a } \\\\\n& =   \\frac{ ( \\pmb a^t \\pmb d)^2}\n{ \\pmb a^t \\pmb S_{p}  \\pmb a } \\\\\n&=   \\frac{ ( \\pmb a^t  \\pmb S_{p}^{1/2} \\pmb S_{p}^{-1/2} \\pmb d)^2}\n{ \\pmb a^t \\pmb S_{p}  \\pmb a } \\\\\n& \\le   \\frac{ ( \\pmb a^t  \\pmb S_{p} \\pmb a) ( \\pmb d^t \\pmb S_{p}^{-1} \\pmb d)}\n{ \\pmb a^t \\pmb S_{p}  \\pmb a } \\\\\n&= \\pmb d^t \\pmb S^{-1}_{p} \\pmb d\n\\end{aligned}\n\\tag{5.16}\\]\n위에서 부등식은 코쉬-쉬바르쯔 부등식(Cauchy–Schwarz Inequality) 을 적용한 결과이며 부등식의 등호는 다음과 같은 경우에 성립한다.\n\\[\n\\pmb a = \\pmb S^{-1}_{p} \\pmb d = \\pmb S^{-1}_{p} (\\bar {\\pmb x}_1 - \\bar {\\pmb x}_2)\n\\tag{5.17}\\]\n\n\n\n\n\n\n코쉬-쉬바르쯔 부등식\n\n\n\n두 벡터 \\(\\pmb u\\)와 \\(\\pmb v\\)에 대하여 다음 부등식(Cauchy–Schwarz Inequality)이 성립한다.\n\\[\n|\\pmb u^t \\pmb v|^2 \\le (\\pmb u^t \\pmb u)(\\pmb v^t \\pmb v)\n\\tag{5.18}\\]\n부등식의 등호는 \\(\\pmb u\\)와 \\(\\pmb v\\)가 선형종속일 때, 즉 \\(\\pmb u = c \\pmb v\\) 인 경우 성립한다.\n식 5.16 에서 다음과 같이 벡터 \\(\\pmb u\\) 와 \\(\\pmb v\\) 를 적용하면 원하는 결과를 얻는다.\n\\[\n\\pmb u = \\pmb S_{p}^{1/2} \\pmb a \\quad \\text{and} \\quad \\pmb v = \\pmb S_{p}^{-1/2} \\pmb d\n\\]\n\n\n식 5.17 에서 유도한 선형변환 벡터 \\(\\pmb a\\)는 다변량 정규분포 가정하에서 ECM을 최소화하는 판별함수의 선형벡터 식 5.12 와 동일하다.\n더 나아가 예제 5.1 의 결과를 이용하면, 두 집단의 평균이 각각 \\(\\bar y_1\\) 과 \\(\\bar y_2\\) 이므로 새로운 관측값 \\(y_0 = \\pmb a^t \\pmb x_0\\) 의 값을 두 평균의 가운데 값, 즉 \\(m=(\\bar y_1 + \\bar y_2)/2\\)와 비교하여 판별할 수 있다. 즉, 개체의 특성값 \\(y_0 = \\pmb a^t \\pmb x_0\\)일 때 다음을 만족하면 집단 \\(P_1\\)으로 분류한다.\n\\[  \ny_0  &gt;  m\n\\]\n여기서\n\\[\ny_0= \\pmb a^t\\pmb x_0=  (\\bar {\\pmb x}_1-\\bar {\\pmb x}_2)^t \\pmb S_{p}^{-1} \\pmb x_0\n\\]\n\\[\nm=\\frac{1}{2} (\\bar {\\pmb x}_1-\\bar {\\pmb x}_2)^t \\pmb S_{p}^{-1} (\\bar\n{\\pmb x}_1+\\bar {\\pmb x}_2)= \\frac{1}{2} \\pmb a^t (\\bar\n{\\pmb x}_1+\\bar {\\pmb x}_2)  = \\frac{1}{2} {(\\bar y_1 +\\bar y_2} )\n\\]\n따라서 Fisher의 판별함수에 의한 분류 규칙은 다변량 정규분포 가정하에 ECM을 최소로 하는 규칙 식 5.14 과 동일하다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>판별분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-discrim.html#예제-잔디깎는-트렉터",
    "href": "qmd/multivar-discrim.html#예제-잔디깎는-트렉터",
    "title": "5  판별분석",
    "section": "5.4 예제: 잔디깎는 트렉터",
    "text": "5.4 예제: 잔디깎는 트렉터\n다음은 미국에서 한 소비용품 판매점이 잔디깍는 트랙터(lawn mower)의 소유 여부에 대한 가구의 소득과 집 크기의 관계를 알아보기 위하여 고객의 정보를 수집한 자료이다. 총 24개의 자료가 있고 트랙터를 소유하지 않는 집과 소유한 집이 각각 12개이다.\n변수 income 과 lotsize 는 각각 가구 소득과 집의 크기에 대한 변수이다. 트랙터를 소유한 상태를 나타내는 변수는 class 이며 값이 2 이면 소유하지 않는 상태(NO,그룹 1), 1 이면 소유한 상태(YES, 그룹 2) 를 나타낸다.\n\n# read data\ndf &lt;- read.csv(here(\"data\",\"lawn.csv\") , header=T, sep=\"\")\ndf$class &lt;- factor(df$class, levels=c(2,1), labels=c( \"NO\",\"YES\"))\nhead(df)\n\n  income lotsize class\n1   60.0    18.4   YES\n2   85.5    16.8   YES\n3   64.8    21.6   YES\n4   61.5    20.8   YES\n5   87.0    23.6   YES\n6  110.1    19.2   YES\n\n\n이제 트렉터 소유의 여부와 소득/집크기의 관계를 알아보기 위하여 이차원에 자료를 그림으로 나타내보자.\n\n# plot data wiyth different symbol by uisng ggplot2\n\ndf %&gt;% ggplot(aes(x=income,y=lotsize, shape=class, color=class)) + \n  geom_point(size=4, alpha = 0.7) +\n  theme_bw() + \n  labs(x=\"가구 소득\",y=\"집 면적 \",shape=\"소유 여부\", color = \"소유 여부\")\n\n\n\n\n\n\n\n\n이제 앞절에서 유도한 선형판별함수를 트렉터 자료에 적용하여 판별함수를 구해보자. 일단 표본 평균벡터와 합동 공분산 행렬을 계산한다.\n\n# 각 그룹의 표본 크기\nn1 &lt;- sum(df$class == \"NO\" )\nn2 &lt;- sum(df$class == \"YES\" )\n\nn1\n\n[1] 12\n\nn2\n\n[1] 12\n\n# 각 그룹의 평균벡터\nmean_vec &lt;- df %&gt;% \n   group_by(class) %&gt;%\n   summarise(across(c(income, lotsize), mean))\n\nmean_vec\n\n# A tibble: 2 × 3\n  class income lotsize\n  &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 NO      57.4    17.6\n2 YES     79.5    20.3\n\nmean_x1 &lt;- mean_vec %&gt;% dplyr::filter(class == \"NO\" ) %&gt;% \n  dplyr::select(income, lotsize) %&gt;% \n  as.matrix() %&gt;% t()\n\nmean_x2 &lt;- mean_vec %&gt;% dplyr::filter(class == \"YES\" ) %&gt;% \n  dplyr::select(income, lotsize) %&gt;% \n  as.matrix() %&gt;% t()\n\nmean_x1\n\n            [,1]\nincome  57.40000\nlotsize 17.63333\n\nmean_x2\n\n            [,1]\nincome  79.47500\nlotsize 20.26667\n\n# 각 그룹의 공분산 행렬\n\ncov_tbl &lt;- df %&gt;%\n  group_by(class) %&gt;%\n  summarise(cov = list(cov(across(c(income, lotsize)))), .groups = \"drop\")\n\ncov_x1 &lt;- cov_tbl %&gt;% filter(class == \"NO\" ) %&gt;% pull(cov) %&gt;% .[[1]]\ncov_x1\n\n            income   lotsize\nincome  200.705455 -2.589091\nlotsize  -2.589091  4.464242\n\ncov_x2 &lt;- cov_tbl %&gt;% filter(class == \"YES\" ) %&gt;% pull(cov) %&gt;% .[[1]]\ncov_x2\n\n           income    lotsize\nincome  352.64386 -11.818182\nlotsize -11.81818   4.082424\n\n# 합동 공분산 행렬\n#cov_p &lt;- ((n1-1)*cov_x1 + (n2-1)*cov_x2)/(n1+n2-2)\ncov_p &lt;- (n1*cov_x1 + n2*cov_x2)/(n1+n2)\ncov_p\n\n            income   lotsize\nincome  276.674659 -7.203636\nlotsize  -7.203636  4.273333\n\n\n이제 트렉터 자료에 적용하여 앞장에서 구한 최적의 판별 규칙을 유도해보자. 먼저 식 5.12 와 식 5.17 에 나타난 최적 변환 벡터 \\(\\pmb a\\) 와 상수 \\(m\\) 는 다음과 같이 구할 수 있다.\n\n# find discriminant function \na &lt;- solve(cov_p) %*%  (mean_x1 - mean_x2) \na\n\n              [,1]\nincome  -0.1002303\nlotsize -0.7851847\n\n\n\nm &lt;- t(mean_x1 - mean_x2) %*% solve(cov_p) %*% (mean_x1 + mean_x2)/2\nm\n\n          [,1]\n[1,] -21.73876\n\n\n따라서 새로운 특성값 벡터를 \\(\\pmb x_0= (x_{10},~x_{20})^t\\) 이라고 하면 ECM을 최소화하는 판별함수는 다음과 같이 주어진다. 즉, 다음 조건이 만족하면 그룹 1, 즉 트랙터를 사지 않는 그룹에 속한다.\n\\[\n(-0.1002303) \\times  (x_{10})  + (-0.7851847) \\times  (x_{20})  &gt; -21.7387617\n\\]\n만약 income 과 lotsize가 각각 70, 16 이라면 다음과 같이 \\(\\pmb a^t \\pmb x_0\\) 를 구할 수 있다.\n\nx_0 &lt;- matrix(c(70, 16),2,1)\nx_0\n\n     [,1]\n[1,]   70\n[2,]   16\n\nt(a) %*% x_0 \n\n          [,1]\n[1,] -19.57908\n\n\n다시 쓰면\n\\[\n(-0.1002303) \\times  (70)  + (-0.7851847) \\times  (16)  = -19.5790766\n\\] 따라서 -19.5790766 &gt; -21.7387617 이므로 그룹 1 (NO) 에 속한다고 판별한다.즉 트랙터를 소유하지 않는 집단에 속한다고 판별한다.\n\nt(a) %*% x_0 &gt; m\n\n     [,1]\n[1,] TRUE\n\n\n즉, income 과 lotsize가 각각 70, 16 이면 \\(\\pmb a^t \\pmb x_0 &gt; m\\) 을 만족하므로 트렉터를 소유하지 않을 그룹 (NO) 에 속한다고 판별한다.\n트랙터를 소유한 집단과 소유하지 않은 집단에서 소득과 집크기에 대하여 이변량 정규분포를 가정하고 각 집단의 평균벡터와 공통 공분산을 구하여 이변량 정규분포의 확률밀도함수를 이차원 평면에 표시해보았다. 또한 앞에서 구한 선형 판별함수의 경계선 \\(\\pmb a^t \\pmb x = m\\) 을 표시하였다\n\n# 등고선을 위한 격자점 생성\ngrid &lt;- expand.grid(\n  income = seq(min(df$income) - 1, max(df$income) + 1, length = 100),\n  lotsize = seq(min(df$lotsize) - 1, max(df$lotsize) + 1, length = 100)\n)\n# 각 집단의 이변량 정규분포 확률밀도함수 계산\ngrid$dx1 &lt;- dmvnorm(grid[,c(\"income\",\"lotsize\")], mean = mean_x1, sigma = cov_p)\ngrid$dx2 &lt;- dmvnorm(grid[,c(\"income\",\"lotsize\")], mean = mean_x2, sigma = cov_p)\n\nb0 &lt;- as.numeric(m/a[2])\nb1 &lt;- -as.numeric(a[1])/as.numeric(a[2])\n\n# 산점도와 등고선, 판별 경계선 그리기 \ndf %&gt;% ggplot(aes(x=income,y=lotsize, shape=class, color=class)) + \n  geom_point(size=4, alpha = 0.9) +\n  # 평균벡터 표기\n  geom_point(data = mean_vec, aes(x=income, y=lotsize, color=class),  shape=3, size=5, stroke=2) +\n  # 등고선 1\n  geom_contour(\n    data = grid,\n    aes(x = income, y = lotsize, z = dx1),\n    color = \"red\", linewidth = 0.2, bins = 6,\n    inherit.aes = FALSE\n  ) +\n  # 등고선 2\n  geom_contour(\n    data = grid,\n    aes(x = income, y = lotsize, z = dx2),\n    color = \"blue\", linewidth = 0.2, bins = 6,\n    inherit.aes = FALSE\n  ) +\n  # 판별 경계선\n  geom_abline(\n    intercept = b0, slope = b1,\n    color = \"black\", linewidth = 1\n  ) +\n  theme_bw() + \n  labs(x=\"가구 소득\",y=\"집 면적 \",shape=\"소유 여부\", color = \"소유 여부\")\n\n\n\n\n\n\n\n\n패키지 MAASS 의 함수 lda()을 이용하면 앞에서 구한 Fisher 의 선형판별함수를 쉽게 구할 수 있다.\n\nfisher_model &lt;- lda(class ~ income + lotsize, data = df, method=\"moment\")\nfisher_model\n\nCall:\nlda(class ~ income + lotsize, data = df, method = \"moment\")\n\nPrior probabilities of groups:\n NO YES \n0.5 0.5 \n\nGroup means:\n    income  lotsize\nNO  57.400 17.63333\nYES 79.475 20.26667\n\nCoefficients of linear discriminants:\n              LD1\nincome  0.0484468\nlotsize 0.3795228\n\n# 판별 벡터\nfisher_model$scaling\n\n              LD1\nincome  0.0484468\nlotsize 0.3795228\n\n\n참고로 함수 lda 의 결과값인 Coefficients of linear discriminants 는 식 5.17 에서 구한 최적 변환 백터 \\(\\pmb a\\) 와 같은 방향의 벡터이지만 길이가 다른 점에 유의하자.\n\n# lda 함수의 결과와 비교\na/a[1]\n\n            [,1]\nincome  1.000000\nlotsize 7.833806\n\nfisher_model$scaling/fisher_model$scaling[1]\n\n             LD1\nincome  1.000000\nlotsize 7.833806\n\n\n이제 위에서 고려한 값, 즉 income 과 lotsize가 각각 70, 16 이라면 다음과 같이 새로운 데이터프레임을 만들고 predict 함수를 사용하여 앞에서 구한 판별 결과와 동일한 결과를 얻을 수 있다.\n\nnew_data &lt;- data.frame(income = 70, lotsize = 16)\npredict(fisher_model, newdata = new_data)\n\n$class\n[1] NO\nLevels: NO YES\n\n$posterior\n         NO       YES\n1 0.8965703 0.1034297\n\n$x\n        LD1\n1 -1.043894",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>판별분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-pca.html",
    "href": "qmd/multivar-pca.html",
    "title": "6  주성분 분석",
    "section": "",
    "text": "6.1 이변량 확률벡터의 변환\n다변량 데이터를 다룰 때 문제점 중 하나는 단순히 변수가 너무 많아 데이터에 대한 유익한 초기 평가를 성공적으로 수행하기 어렵다는 점이다. 변수들 사이의 복잡한 상관관계는 분석에 어려움을 더한다. 또한 변수가 너무 많으면 연구자가 데이터에 적용하고자 하는 다른 다변량 기법에도 문제를 일으킬 수 있다.빅데이터가 흔한 시데에 데이터는 넘쳐나지만, 그 속에서 진짜 중요한 정보를 찾아내는 일은 점점 더 어려워지고 있다.\n주성분 분석(PCA, Principal Component Analysis)은 단순히 하나의 통계 기법을 넘어, 데이터를 이해하는 새로운 방식을 제시한다. 주성분 분석은 다변량 자료의 변수들 속에 숨어 있는 공통된 구조를 발견하고, 이를 소수의 새로운 변수로 변환하는과정이다. 수십 개의 변수를 그대로 바라보면 혼란스러울 뿐이다. PCA는 이 변수들을 새로운 좌표계로 옮겨, 가장 큰 변동을 담아내는 방향으로 새로운 변수를 찾아낸다. 첫 번째 주성분이 데이터의 전체 흐름을 포착하고, 두 번째 주성분이 남은 변동을 설명한다. 이렇게 서로 직교하는 변수들이 순차적으로 만들어지면서, 데이터가 가지고 있는 변동을 2-3 개의 새로운 변수로 단순하게 많은 부분을 설명할 수 있다.\n주성분 분석의 목적은 분명하다. 첫째, 차원을 축소하여 핵심적인 소수의 변수들만 남기는 것이다. 복잡한 다변량을 2-3 개의 주성분으로 요약하면, 분석은 훨씬 단순해진다. 주성분 분석은 자료가 많은 수의 변수들(Variables)로 구성되어 있는 경우 적은 개수의 변수로 자료의 차원을 축소하는 것이 주요한 목적이다(Dimension Reduction). 또한 이렇게 차원을 축소하는 경우 원래 변수들이 가지고 있던 변동(Variation)을 차원이 축소된 경우에도 최대한 유지할 수 있도록 한다.\n둘째로 주성분 분석은 데이터의 시각화를 더욱 쉽게 수행할 수 있다. 수십 차원의 세계를 2차원, 3차원으로 투영해 보면, 군집은 또렷해지고 이상치는 드러난다. 셋째, 주어진 변수들의 복잡한 상관관계를 검토하고 분석할 필요가 없어진다. 서로 얽힌 변수들을 직교하는 주성분으로 바꿔놓으면, 다중공선성이라는 문제는 자연스레 사라진다.\n주성분 분석은 특정 학문에만 국한되지 않는다. 사회과학에서 복잡한 설문 문항을 요약하는 데, 생물학에서 유전자 발현 데이터의 패턴을 찾는 데, 경영학에서 고객 행동의 핵심 요인을 도출하는 데, 더 나아가 인공지능에서 이미지나 텍스트의 고차원 정보를 다루는 데까지, 그 쓰임새는 한계가 없다.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>주성분 분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-pca.html#이변량-확률벡터의-변환",
    "href": "qmd/multivar-pca.html#이변량-확률벡터의-변환",
    "title": "6  주성분 분석",
    "section": "",
    "text": "6.1.1 이변량 정규분포\n먼저 주성분분석의 기본개념을 이해하기 위하여 다음과 같은 평균 \\(\\pmb  \\mu\\)인 이차원 확률벡터 \\(\\pmb  X =(X_1, X_2)^t\\) 가 아래와 같이 공분산을 가지는 정규분포를 따른다고 하자.\n\\[\n\\pmb  X \\sim N_2( \\pmb  \\mu, \\pmb  \\Sigma), \\quad\nE(\\pmb  X) =  \\pmb  \\mu =\n\\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2\n\\end{bmatrix}, \\quad\n\\pmb  \\Sigma = V(\\pmb  X) =\n\\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} \\\\\n\\sigma_{12} & \\sigma_{22} \\\\\n\\end{bmatrix}\n\\]\n확률벡터 \\(\\pmb  X\\)의 상관계수행렬 \\(\\pmb  C\\) 는 다음과 같이 표시한다. \\[\n\\pmb  C =\n\\begin{bmatrix}\n1 & \\rho \\\\\n\\rho & 1 \\\\\n\\end{bmatrix},\n\\quad\n\\rho = \\frac{\\sigma_{12}}{ \\sqrt{\\sigma_{11} \\sigma_{22}}}\n\\]\n참고할 사항은 상관계수행렬은 표준화된 확률변수 \\(Z_i = (X_i - \\mu _i)/\\sqrt{\\sigma_{ii}}\\)의 공분산 행렬이다. 이유는 다음과 같이 보일 수 있다. 일단 표준화된 확률벡터 \\(\\pmb  Z = (Z_1, Z_2)^t\\)를 정의하자.\n\\[\n\\pmb  Z =\n\\begin{bmatrix}\n(X_1 - \\mu _1)/\\sqrt{\\sigma_{11}} \\\\\n(X_2 - \\mu _2)/\\sqrt{\\sigma_{22}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1/\\sqrt{\\sigma_{11}} & 0 \\\\\n0 & 1/\\sqrt{\\sigma_{22}} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n(X_1 - \\mu _1) \\\\\n(X_2 - \\mu _2)\n\\end{bmatrix}\n= \\pmb  D (\\pmb  X  - \\pmb  \\mu)\n\\] 여기서 \\(\\pmb  D\\)는 각 변수의 표준편차의 역수를 대각원소로 가지는 대각 행렬이다. 표준화된 확률벡터 \\(\\pmb  Z\\)의 공분산은 다음과 같이 유도된다.\n\\[\n\\begin{aligned}\nV(\\pmb  Z ) &=  E( [\\pmb  D (\\pmb  X  - \\pmb  \\mu)] [\\pmb  D (\\pmb  X  - \\pmb  \\mu)]^t ) \\\\\n&=  \\pmb  D E(  (\\pmb  X  - \\pmb  \\mu)  (\\pmb  X  - \\pmb  \\mu)^t ) \\pmb  D^t  \\\\\n&=  \\pmb  D \\Sigma \\pmb  D^t \\\\\n&=\n\\begin{bmatrix}\n1/\\sqrt{\\sigma_{11}} & 0 \\\\\n0 & 1\\sqrt{\\sigma_{22}} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} \\\\\n\\sigma_{12} & \\sigma_{22} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1/\\sqrt{\\sigma_{11}} & 0 \\\\\n0 & 1\\sqrt{\\sigma_{22}} \\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n1 & \\rho \\\\\n\\rho & 1 \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\n\n6.1.2 주성분의 기준과 생성방법\n일반적으로 주성분 분석에서는 확률벡터 \\(\\pmb  X\\)의 평균이 \\(\\pmb  0\\)이라고 가정한다. 주성분 분석은 원래 변수들의 위치(location, 즉 평균)에 영향을 받는 방법이 아니라 자료의 변동(variation, 즉 분산)을 최대로 유지하는 새로운 변수를 만드는 것이 목적이다. 따라서 평균이 \\(\\pmb  0\\)이 아닌 확률벡터 \\(\\pmb  X_*\\)도 그 평균을 뺀 벡터 \\(\\pmb  X\\)로 변환하여 평균을 \\(\\pmb  0\\)으로 만들고 주성분분석을 적용한다.\n\\[ \\text{Let } \\pmb  X = \\pmb  X_* - E(\\pmb  X_* )=\\pmb  X_* - \\pmb  \\mu_* ,\n\\text{ then } E(\\pmb  X)=0, ~~ V(\\pmb  X) = V(\\pmb  X_*)  \\] 아래부터는 특별한 언급이 없으면 확률벡터 \\(\\pmb  X\\)가 평균이 \\(\\pmb  0\\)이라고 가정한다.\n이제 평균이 0이고 공분산(상관계수행렬)이 다음과 같이 주어지는 이변량정규분포를 생각해보자. \\[\n\\pmb  \\Sigma =\n\\begin{bmatrix}\n2 & 1.5\\\\\n1.5 & 2\n\\end{bmatrix}\n\\tag{6.1}\\]\n참고로 두 변수의 상관계수는 \\(\\rho = 1.5/2 = 0.75\\) 이다\nR 의 패키지 중 mtvnorm 을 사용하면 다변량분포에 대한 다양한작업을 손쉽게 할 수 있다. 다음 R 프로그램은 위에 주어진 공분산행렬 을 가지는 이변량정규분포 화률밀도함수의 2차원 등고선그림이다.\n\nlibrary(mvtnorm)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n#### 평균과 공분산\nmu &lt;- c(0, 0)\nsigma &lt;- matrix(c(2, 1.5, 1.5, 2), nrow = 2)\n\n# grid 만들기\nx.points &lt;- seq(-4, 4, length.out = 100)\ny.points &lt;- seq(-4, 4, length.out = 100)\n\ngrid &lt;- expand.grid(x1 = x.points, x2 = y.points)\n\n#### 확률밀도 계산\ngrid$z &lt;- dmvnorm(grid[, c(\"x1\", \"x2\")], mean = mu, sigma = sigma)\n\n#### ggplot으로 contour 그리기\n\np &lt;- ggplot(grid, aes(x = x1, y = x2, z = z)) +\n  geom_contour(color = \"black\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\" ,linewidth = 1.5) +\n  geom_abline(slope = -1, intercept = 0, color = \"blue\",linewidth = 1.5) +\n  geom_abline(slope = 1.5, intercept = 0, color = \"black\") +\n  geom_abline(slope = 0.5, intercept = 0, color = \"black\") +\n  labs(x = \"X_1\", y = \"X_2\", title = \"이차원 정규분포의 확률빌도함수(rho = 0.75) \") \n\nprint(p)\n\n\n\n\n\n\n\n\n위의 그림에서 이차원 평면 상의 이차원 정규분포 밀도함수 등고선을 보면 분포의 퍼진 정도가 가장 큰 방향의 축이 원점을 지나고 기울기가 1 인 직선임을 알 수 있다(빨간 색 직선) 또한 이 직선과 직교하는 선은 원점을 지나고 기울기가 -1 인 직선임을 알 수 있다(파란 색 직선 )\n이제 각 확률변수 \\(X_i\\) 의 선형 변환으로 새로운 확률변수 \\(Z_i\\) 를 다음과 같이 정의한다.\n\\[ Z_1 = a_{11} X_1 + a_{12} X_2, \\quad Z_2 = a_{21} X_1 + a_{22} X_2 \\] 새로운 확률벡터 \\(\\pmb  Z =(Z_1, Z_2)^t\\) 는 다음과 같이 표시할 수 있다. \\[\n\\pmb  Z = \\pmb  A \\pmb  X \\text{ where }\n\\pmb  A =\n\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\\tag{6.2}\\]\n이렇게 새로운 확률 변수 \\(Z_i\\)를 만들 때 첫번째 변수 \\(Z_1\\) 를 첫번째 주성분(the 1st Principal Component)이라고 하며 원래 확률 벡터 \\(\\pmb  X\\)가 가지는 총 변동 중에 최대한 큰 변동을 가질 수 있도록 만들고(빨간 색 직선 방향) 두번째 변수 \\(Z_2\\) 를 두번째 주성분(the 2nd Principal Component) 이라고 부르고 첫번째 변수 \\(Z_1\\)과 공분산이 0 이면서 나머지 변동을 가질 수 있도록(파란색 직선 방향) 만드는 방법이 주성분(principal components)을 만드는 기준이다.\n\n\\(\\max V(Z_1)\\)\n\\(Cov(Z_1,Z_2) =0\\)\n\\(V(Z_1) + V(Z_2) = V(X_1) + V(X_2)\\)\n\n위의 조건을 만족하는 선형변환 \\(\\pmb  Z\\)는 공분산행렬 \\(\\pmb  \\Sigma\\)의 고유값과 고유벡터로 구할 수 있다. 식 6.1 에 주어진 공분산행렬은 양정치 행렬이므로 고유값(eigen value) \\(\\lambda_1 &gt; \\lambda_2&gt;0\\)가 모두 양수이고 각 고유치에 대응하는 정규 직교 고유벡터(orthonormal eigen vector)의 행렬 \\(\\pmb  P\\)을 이용하여 다음과 같은 분해가 가능하다.\n\\[\n\\Sigma = \\pmb  P \\pmb  \\Lambda \\pmb  P^t  \n\\tag{6.3}\\]\n여기서 식 6.1 에 대해서 식 6.3 에 주어진 행렬의 분해를 구해보면 아래와 같다.\n\\[\n\\pmb  \\Lambda =\n\\begin{bmatrix}\n\\lambda_1 & 0\\\\\n0 & \\lambda_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n7/2 & 0\\\\\n0 & 1/2\n\\end{bmatrix}\n, \\quad\n\\pmb  P =\n\\begin{bmatrix}\n1/\\sqrt{2} & -1/\\sqrt{2}\\\\\n1/\\sqrt{2}  & 1/\\sqrt{2}\n\\end{bmatrix}\n\\] 식 6.2 에서 정의된 선형변환을 다음과 같이 정의하면\n\\[  \n\\pmb  A = \\pmb  P^t \\text{ so that } \\pmb  Z = \\pmb  P^t \\pmb  X\n\\] 주성분은 다음과 같이 만들어 진다. \\[\nZ_1 = \\frac{1}{\\sqrt{2}} X_1 + \\frac{1}{\\sqrt{2}} X_2, \\quad Z_2 = -\\frac{1}{\\sqrt{2}} X_1 + \\frac{1}{\\sqrt{2}} X_2\n\\]\n이렇게 만들어진 주성분 벡터의 공분산 행렬을 구해보자\n\\[\n\\begin{aligned}\nV(\\pmb  Z) & = V(\\pmb  P^t \\pmb  X) \\\\\n& = \\pmb  P^t V(\\pmb  X) \\pmb  P \\\\\n& = \\pmb  P^t \\pmb  \\Sigma \\pmb  P \\\\\n& = \\pmb  P^t  \\pmb  P \\pmb  \\Lambda \\pmb  P^t  \\pmb  P  \\\\\n& = \\pmb  \\Lambda\n\\end{aligned}\n\\]\n위의 유도식에서 정규직교 고유벡터 \\(\\pmb  P\\)는 직교행렬이므로 (i.e. \\(\\pmb  P^t  \\pmb  P=\\pmb  I\\))\n\\[  \nV(\\pmb  Z)  =  \\pmb  \\Lambda = \\begin{bmatrix}\n7/2 & 0\\\\\n0 & 1/2\n\\end{bmatrix}\n\\] 따라서 첫번째 주성분의 분산은 가장 큰 고유치가 되고 두 번째 주성분의 분산은 두 번째 고유치가 되며 두 주성분의 공분산은 0이고 정규분포이므로 독립이다. \\[ \\pmb  Z  \\sim N_2( \\pmb  0 , \\pmb  \\Lambda) \\] 또한 다음의 관계가 성립한다.\n\\[ V(Z_1)=3.5,~~ V(Z_2)=0.5,~~ Cov(Z_1,Z_2) =0,~~  V(Z_1) + V(Z_2) = V(X_1) + V(X_2) \\]\n다음은 주성분 벡터 \\(\\pmb  Z\\)의 이변량정규분포의 2차원 등고선그림이다.\n\n# 평균 벡터와 공분산 행렬\nmu &lt;- c(0, 0)\nsigma1 &lt;- matrix(c(3.5, 0, \n                  0, 0.5), nrow = 2)\n\n# x, y 격자 만들기\nx.points &lt;- seq(-5, 5, length.out = 100)\ny.points &lt;- seq(-5, 5, length.out = 100)\ngrid &lt;- expand.grid(x = x.points, y = y.points)\n\n# 이변량 정규분포 밀도 계산\ngrid$z &lt;- dmvnorm(cbind(grid$x, grid$y), mean = mu, sigma = sigma1)\n\n# ggplot2 시각화\np &lt;- ggplot(grid, aes(x = x, y = y, z = z)) +\n  geom_contour(color = \"blue\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Z_1\", y = \"Z_2\", title = \"두 주성분의 분포\")\n\nprint(p)\n\n\n\n\n\n\n\n\n위와 같이 만든 첫번째 주성분은 원래 변수가 가지고 있는 총변동 \\(V(X_1)+V(X_2) = 4\\)의 85.7%를 설명한다.\n\\[\n\\frac{V(Z_1)}{V(X_1)+V(X_2)} = \\frac{V(Z_1)}{V(Z_1)+V(Z_2)}\n= \\frac{\\lambda_1}{\\lambda_1+\\lambda_2} = \\frac{3.5}{4} = 0.857\n\\]\n원래의 두 변수 \\(X_1\\)과 \\(X_2\\)를 모두 사용하지 않고 하나의 주성분 \\(Z_1\\)만으로서 분포의 전체 변동의 큰 부분(85.7%)을 설명할 수 있다. 이러한 논리가 주성분을 이용한 차원의 축소이다.\n\n\n6.1.3 상관계수행렬을 통한 주성분분석\n각 변수는 숫자로 나타나므로 측정할 경우 그 단위(unit)가 있다. 하지만 측정 단위들은 변수에 따라 또는 측정하는 사람에 따라 다를 수 있다. 예를 들어 키를 측정하는 경우 센티미터(cm)를 사용하고 몸무게를 측정하는 경우 킬로그램(Kg)를 사용한다. 또한 같은 변수인 키를 측정하는 경우에 센티미터(cm)대신 미터(m)를 사용할 수도 있다.\n공분산행렬은 각 변수의 측정단위에 따라 변하며 그에 따른 고유값과 고유벡터도 변한다. 즉 주성분분석은 변수의 측정 단위에 따라 변할 수 있다.\n예를 들어 식 6.1 을 공분산으로 가지는 확률벡터에서 첫번째 확률변수 \\(X_1\\)를 키라고 하고 그 측정단위가 미터(m)라고 가정하자. 만약 키의 측정단위를 센티미터로 바꾼다면 \\((100 \\times X_1)\\) 단위가 바뀐 확률벡터의 공분산행렬은 다음과 같이 변한다.\n\\[\n\\pmb  \\Sigma =\n\\begin{bmatrix}\n(10000)(2) & (100)(1.5)\\\\\n(100)(1.5) & 2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n20000 & 150\\\\\n150 & 2\n\\end{bmatrix}\n\\tag{6.4}\\]\n위에서 변환된 공분산행렬의 고유값과 고유벡터를 구해보면 다음과 같다.\n\nsigma &lt;- matrix(c(20000,150,150,2),nrow=2) # 공분산행렬\nsigma\n\n      [,1] [,2]\n[1,] 20000  150\n[2,]   150    2\n\neigen(sigma)\n\neigen() decomposition\n$values\n[1] 2.000113e+04 8.749508e-01\n\n$vectors\n             [,1]         [,2]\n[1,] -0.999971874  0.007500117\n[2,] -0.007500117 -0.999971874\n\n\n가장 큰 고유치는 \\(\\lambda_1= 20000\\)에 가깝고 다음 고유치는 거의 0이다 (\\(\\lambda_2 \\sim 0\\)) 또한 고유벡터는 거의 단위행렬에 가까우므로 첫번째 주성분은 키를 나타내는 원래 변수와 동일하고(부호만 바뀐다) 전체 변동을 대부분 설명하며 두번째 주성분은 거의 설명하는 변동이 없게된다.\n위와 같은 현상은 공분산행렬을 주성분분석에 사용하는 경우에 나타나는 큰 문제점이다. 많은 경우 변수들은 측정단위가 다르며 단위를 바꾸면 주성분이 크게 변한다. 이러한 분제점을 해결하기 위하여 많은 경우 각 변수를 표준화하여 사용하는 것이 좋다. 표준화하면 각 변수가 가지는 변동이 같게되며 서로의 관계는 상관계수로 파악할 수 있다. 앞에서 표준화된 변수들의 공분산행렬은 상관계수행렬임을 보였다. 따라서 특별한 이유가 없는 한 주성분분석은 상관계수 행렬을 사용한다.\n만약 두개의 확률벡터가 각각 식 6.1 과 식 6.4 를 공분산으로 갖는다고 가정하자. 두 공분산행렬은 다르지만 상관계수행렬은 같음을 알수 있다.\n\\[\n\\pmb  C =\\begin{bmatrix}\n1 & \\rho \\\\\n\\rho & 1 \\\\\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & 0.75 \\\\\n0.75 & 1 \\\\\n\\end{bmatrix}\n\\tag{6.5}\\]\n따라서 단위의 종류와 변환에 관계없이 같은 주성분을 가진다.\n\nsigma &lt;- matrix(c(1,0.75,0.75,1),nrow=2) # 상관계수행렬(상관계수=0.75)\neigen(sigma)\n\neigen() decomposition\n$values\n[1] 1.75 0.25\n\n$vectors\n          [,1]       [,2]\n[1,] 0.7071068 -0.7071068\n[2,] 0.7071068  0.7071068\n\n\n위에서 구한 고유치와 고유벡터를 사용한 주성분과 그 통계적 성질은 다음과 같다.\n\\[\nZ_1 = \\frac{1}{\\sqrt{2}} X_1 + \\frac{1}{\\sqrt{2}} X_2, \\quad Z_2 = -\\frac{1}{\\sqrt{2}} X_1 + \\frac{1}{\\sqrt{2}} X_2\n\\] \\[\nV(Z_1)=1.75,~~ V(Z_2)=0.25,~~ Cov(Z_1,Z_2) =0,~~  V(Z_1) + V(Z_2) = V(X_1) + V(X_2)=2\n\\]\n참고할 점은 상관계수행렬의 대각의 합은 변수의 개수(\\(p=2\\))와 같고 상관계수가 변한다 하더라도 고유벡터들는 변하지 않아 주성분은 동일하게 정의되지만 각각의 분산은 달라진다. 아래는 상관계수 \\(0.95\\)를 가지는 상관계수행렬의 고유치와 고유값이다.\n\nsigma &lt;- matrix(c(1,0.95,0.95,1),nrow=2) # 상관계수행렬(상관계수=0.95)\nsigma\n\n     [,1] [,2]\n[1,] 1.00 0.95\n[2,] 0.95 1.00\n\neigen(sigma)\n\neigen() decomposition\n$values\n[1] 1.95 0.05\n\n$vectors\n          [,1]       [,2]\n[1,] 0.7071068 -0.7071068\n[2,] 0.7071068  0.7071068\n\n\n\n\n6.1.4 표본자료를 이용한 주성분분석\n이변량 정규분포에서 \\(n\\)개의 자료 \\(\\pmb  X_1, \\pmb  X_2, \\dots, \\pmb  X_n\\)이 독립표본으로 추출되었다면 표본공분산과 표본상관계수를 추정하여 주성분을 만들 수 있다.\n\\[\n\\hat {\\pmb  \\Sigma } =\n\\begin{bmatrix}\n\\hat \\sigma_{11} & \\hat \\sigma_{12} \\\\\n\\hat  \\sigma_{12} & \\hat \\sigma_{22} \\\\\n\\end{bmatrix}\n\\quad\n\\hat{\\pmb  C} =\n\\begin{bmatrix}\n1 & \\hat \\rho \\\\\n\\hat \\rho & 1 \\\\\n\\end{bmatrix}\n\\]\n다음은 평균이 \\(\\pmb  0\\) 이고 다음을 공분산으로 가지는 이변량 정규분포에서 50개의 표본을 추출하고 표본상관계수를 추정하여 주성분을 만드는 R 프로그램이다. \\[\n\\pmb  \\Sigma  =\n\\begin{bmatrix}\n3 & -2 \\\\\n-2 & 2 \\\\\n\\end{bmatrix}\n\\]\n\nmu &lt;- c(0,0)  # 평균 벡터\nsigma &lt;- matrix(c(3,-2,-2,2),nrow=2) # 공분산행렬\nX &lt;- rmvnorm(100,mean=mu,sigma=sigma)\nhead(X)\n\n           [,1]       [,2]\n[1,]  4.6282705 -4.2243072\n[2,]  1.4045731 -0.8958122\n[3,]  0.4338715 -0.3901536\n[4,]  0.9394071 -1.7978767\n[5,] -2.1498722  1.7714280\n[6,]  3.1633948 -1.8869771\n\nC &lt;- cor(X)\nC\n\n           [,1]       [,2]\n[1,]  1.0000000 -0.8164816\n[2,] -0.8164816  1.0000000\n\neigen(C)\n\neigen() decomposition\n$values\n[1] 1.8164816 0.1835184\n\n$vectors\n           [,1]       [,2]\n[1,] -0.7071068 -0.7071068\n[2,]  0.7071068 -0.7071068\n\n\n위의 결과로 주성분은 다음과 같이 주어진다.\n\\[\nZ_1 = - 0.707 X_1 + 0.707 X_2 \\quad Z_2 = - 0.707 X_1 - 0.707 X_2\n\\]\n첫번째 주성분의 분산이 \\(V(Z_1)=\\lambda_1=1.816\\) 이므로 총변동 \\(p=2\\) 의 91%를 설명한다.\n원래 자료 \\((X_1, X_2)\\)의 산점도와 위의 식에 원래 자료를 넣어 계산된 주성분의 값 \\((Z_1,Z_2)\\) 산점도는 다음 그림과 같다.\n\nP &lt;- eigen(C)$vectors\nZ &lt;- X %*% P\nplot(X, xlab=\"X1\",ylab=\"X2\",ylim=c(-4,4),xlim=c(-4,4))\nabline(v=0);abline(h=0)\n\n\n\n\n\n\n\nplot(Z, xlab=\"Z1\",ylab=\"Z2\",ylim=c(-4,4),xlim=c(-4,4))\nabline(v=0);abline(h=0)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>주성분 분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-pca.html#주성분-분석의-기초이론",
    "href": "qmd/multivar-pca.html#주성분-분석의-기초이론",
    "title": "6  주성분 분석",
    "section": "6.2 주성분 분석의 기초이론",
    "text": "6.2 주성분 분석의 기초이론\n주성분분석은 서로 상관된 \\(p\\) 개의 변수들 \\(\\pmb X=(X_1,X_2,\\dots,X_p)^t\\)의 총 변동을 각 변수들의 선형결합으로 만든 서로 상관되지 않은 새로운 변수들인 \\(\\pmb Z^t=(Z_1,Z_2,\\dots,Z_p)\\)에 의해 설명하는 것이다.\n\\[\nZ_i = a_{i1} X_1 + a_{i2} X_2 + a_{i3} X_3 + \\dots + a_{ip} X_p,\\quad i=1,2,\\dots,p\n\\]\n주성분 분석의 목적은 원래의 관측변수들이 아닌 새로운 변수를 만들어서 원래 변수의 수보다 적은 수의 새로운 변수를 이용하여 원래 변수가 가지는 대부분의 변동을 설명하려는것이다. 즉, 차원의 축소(dimension reduction)가 그 목적이다.\n새로운 변수인 \\(Z_i\\)들을 주성분(principal component, PC)이라고 부르며 \\(X_i\\)들의 모든 선형결합중에서 서로 상관되지 않고 원래 자료의 변동을 가능한 한 많이 설명한다는 의미에서 중요성이 감소하는 순서되로 유도된다.\n즉 첫번째 주성분인 \\(Z_1\\)은 다른 주성분보다 분산이 가장 크다. 다음으로 두번째 주성분 \\(Z_2\\)는 \\(Z_1\\)과 상관관계가 없으며 남아있는 변동을 가능한 한 많이 설명하게 만든다. 이러한 작업을 계속 적용하여 \\(p\\)개의 주성분을 만들 수 있다.\n\\[\nV(Z_1) \\ge V(Z_2) \\ge \\dots \\ge V(Z_p) \\quad \\text{ and } \\quad Cov(Z_i, Z_j)=0,~~ i\\ne j\n\\]\n주성분 분석은 다음의 경우에 매우 우용하다.\n\n관측된 개수에 비하여 설명변수가 너무 많다.\n설명변수들이 높게 상관되어 있다.\n\n\n6.2.1 주성분의 정의\n\\(\\pmb X=(X_1, X_2,\\dots,X_p)^t\\)를 차원이 \\(p\\)인 확률벡터(random vector)라고 하자. \\[  E(\\pmb X) = \\pmb 0 \\quad \\text{and} \\quad V(X)=\\pmb \\Sigma \\] 새로운 \\(p\\)개의 주성분 \\(\\pmb Z=(Z_1,Z_2,\\dots,Z_p)^t\\)는 \\(X_i\\)들의 선형결합으로 만들어진다.\n\\[\n\\begin{aligned}\nZ_1 &= \\pmb a_1^t \\pmb X = a_{11} X_1 + a_{12} X_2 + a_{13} X_3 + \\dots + a_{1p} X_p  \\\\\nZ_2 &= \\pmb a_2^t \\pmb X = a_{21} X_1 + a_{22} X_2 + a_{23} X_3 + \\dots + a_{2p} X_p  \\\\\nZ_3 &= \\pmb a_3^t \\pmb X = a_{31} X_1 + a_{32} X_2 + a_{33} X_3 + \\dots + a_{3p} X_p  \\\\\n    & \\dots & \\\\\nZ_p &= \\pmb a_p^t \\pmb X = a_{p1} X_1 + a_{p2} X_2 + a_{p3} X_3 + \\dots + a_{pp} X_p  \\\\\n\\end{aligned}\n\\tag{6.6}\\]\n식 6.6 에서 정의된 주성분은 다음과 같은 통계적 성질을 가지고 있다.\n\\[\nE(Z_i) = \\pmb a_i^t \\pmb 0= \\pmb 0 \\quad \\text{and} \\quad V(Z_i)= \\pmb a_i^t \\pmb \\Sigma \\pmb a_i\n\\quad \\text{and} \\quad Cov(Z_i,Z_j)= \\pmb a_i^t \\pmb \\Sigma \\pmb a_j\n\\]\n만약 주성분을 만드는 계수 벡터 \\(\\pmb a_i\\)들을 다음과 같이 행렬로 표시하면\n\\[\n\\pmb A =\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1p} \\\\\na_{21} & a_{22} & \\dots & a_{2p} \\\\\n       & \\dots & \\dots           \\\\\na_{p1} & a_{p2} & \\dots & a_{pp} \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\pmb a_1^t \\\\\n\\pmb a_2^t \\\\\n\\dots \\\\\n\\pmb a_1^t \\\\\n\\end{bmatrix}\n\\tag{6.7}\\]\n식 6.6 의 주성분 벡터 \\(\\pmb Z\\)는 다음과 같이 정의된다.\n\\[\n\\pmb Z = \\pmb A  \\pmb X\n\\tag{6.8}\\]\n이제 주성분을 만드는 절차는 다음과 같다.\n1.첫번쨰 주성분 \\(Z_1 = \\pmb a_1^t \\pmb X\\) 를 다음과 같은 조건이 만족하게 찾는다.\n\nMaximize \\(Var(Z_1)= \\pmb a_1^t \\pmb \\Sigma \\pmb a_1\\)\nsubject to \\(\\pmb a_1^t \\pmb a_1=1\\), i,e,\n\n\\[\na_{11}^2 + a_{12}^2 + a_{13}^2 + \\cdots + a_{1p}^2=1  \n\\]\n\n두번쨰 주성분 \\(Z_2 = \\pmb a_2^t \\pmb X\\) 다음과 같은 조건이 만족하게 찾는다.\n\n\nMaximize \\(Var(Z_2)= \\pmb a_2^t \\pmb \\Sigma \\pmb a_2\\)\nsubject to \\(a_{21}^2 + a_{22}^2 + \\dots + a_{2p}^2=1\\)\n\\(Z_2\\) is uncorrelated with \\(Z_1\\), i.e.\n\n\\[\nCov(Z_1,Z_2)= \\pmb a_1^t \\pmb \\Sigma \\pmb a_2=0\n\\]\n….\n\n\\(k\\) 번째 주성분 \\(Z_k = \\pmb a_k^t  \\pmb X\\) 다음과 같은 조건이 만족하게 찾는다.\n\n\nMaximize \\(Var(Z_k)= \\pmb a_k^t \\pmb \\Sigma \\pmb a_k\\)\nsubject to \\(a_{k1}^2 + a_{k2}^2 + \\dots + a_{kp}^2=1\\)\n\\(Z_k\\) is uncorrelated with \\(Z_1,Z_2,\\dots,Z_{k-1}\\),\n\n\\[\nCov(Z_k,Z_i)= \\pmb a_k^t \\pmb \\Sigma \\pmb a_i=0 \\quad \\text{ for } \\quad i=1,2,\\dots,k-1\n\\] 주성분의 계수를 찾을 때 실제로는 공분산행렬 \\(\\pmb \\Sigma\\)을 모르기 때문에 표본으로부터 얻어진 표본 공분산행렬 \\(\\hat {\\pmb \\Sigma} \\equiv \\pmb S\\)를 이용한다. 또는 표본 상관계수행렬 \\(\\hat {\\pmb C}\\)를 이용한다.\n\n\n6.2.2 양정치 행렬의 스펙트럼분해\n주성분분석에서 선형결합에 이용되는 계수 \\(a_{ij}\\)는 공분산행렬의 스펙트럼 분해(Spectral Decomposition)을 통하여 찾을 수 있다 (식 B.4 참조).\n공분산행렬은 대칭인 양정치행렬(positive definite matrix)이기 때문에 양의 실수를 가지는 고유값(eigenvalues)을 가진다. \\(p \\times p\\) 공분산 행렬을 \\(\\pmb \\Sigma\\)라하면 다음과 같은 스펙트럼 분해가 가능하다.\n\\[\n\\pmb \\Sigma = \\pmb P \\pmb \\Lambda \\pmb P^t\n\\] 여기서 \\(\\pmb \\Lambda=diag(\\lambda_1,\\lambda_2,\\dots,\\lambda_q)\\)는 대각원소가 공분산행렬의 고유값인 대각행렬이며 고유값은 다음과 같이 내림차순으로 정렬되어 있다고 하자.\n\\[\n\\lambda_1 \\ge\\lambda_2 \\ge \\dots \\ge \\lambda_p\n\\]\n행렬 \\(\\pmb P\\)는 \\(p \\times p\\) 직교행렬(orthonomal matrix)로서 \\(k\\)번째 열벡터 \\(\\pmb a_k\\)는 \\(k\\) 번째 고유값에 해당하는 고유벡터이다.\n\\[\n\\pmb \\Sigma \\pmb a_k = \\lambda_k \\pmb a_k \\quad \\text{and} \\quad  a_{k1}^2 + a_{k2}^2 + \\dots + a_{kp}^2=1, \\quad k=1,2,\\dots,p\n\\]\n\\[\n\\pmb P = [ \\pmb a_1~~ \\pmb a_2~~ \\dots~~ \\pmb a_p ]\n\\]\n여기서 행렬 \\(\\pmb P\\)는 직교행렬이므로 각 고유벡터들은 다음과 같은 관계가 있다.\n\\[\n\\begin{cases}\n\\pmb a_i^t \\pmb a_j = 0 & \\text{if }~~ i \\ne j \\\\\n\\pmb a_i^t \\pmb a_i = 1 & \\text{if }~~ i = j\n\\end{cases}\n\\]\n\n\n6.2.3 이차형식의 최대값\n주어진 대칭인 양정치행렬 \\(\\pmb \\Sigma\\)에 대하여 다음과 같이 2차형식(quadratic form) \\(\\pmb x^t \\Sigma \\pmb x\\)의 값을 최대로 하는 \\(p\\)-차원의 벡터 \\(\\pmb x\\)를 찾는 문제를 풀어보자.\n\\[\n\\max_{\\pmb x} \\frac{\\pmb x^t \\pmb \\Sigma \\pmb x}{\\pmb x^t \\pmb x} = \\max_{\\pmb x; | \\pmb x|=1} \\pmb x^t \\pmb \\Sigma \\pmb x\n\\] 위의 문제에서 2차형식 \\(\\pmb x^t \\pmb \\Sigma \\pmb x\\)는 벡터 \\(\\pmb x\\)의 길이를 증가시키면 비례하여 증가하므로 이차형식을 벡터의 길이의 제곱 \\(\\pmb x^t \\pmb x\\)로 나누어 표준화시킨 다음 그 양을 최대화하는 벡터를 찾는 문제로 생각하는 것이다.\n2차형식(quadratic form) \\(\\pmb x^t \\pmb \\Sigma \\pmb x\\)은 다음과 같은 스펙트럼분해가 가능하다. \\[\n\\pmb x^t \\pmb \\Sigma \\pmb x= \\pmb x^t \\pmb P \\pmb \\Lambda \\pmb P^t x = \\pmb x^t [ \\pmb a_1~~ \\pmb a_2~~ \\dots ~~\\pmb  a_p ]\n\\begin{bmatrix}\n\\lambda_1 & 0 & \\dots & 0 \\\\\n0 & \\lambda_2 & \\dots & 0 \\\\\n\\dots & \\dots & \\dots & \\dots \\\\\n0 & 0 & \\dots & \\lambda_p \\\\\n\\end{bmatrix}\n[ \\pmb a_1~~ \\pmb a_2~~ \\dots ~~\\pmb a_p ]^t \\pmb x\n\\]\n새로운 변수 \\(\\pmb z= \\pmb P^t \\pmb x\\)를 정의하면 2차형식은 다음과 같이 표시된다.\n\\[\n\\pmb x^t \\pmb \\Sigma \\pmb x = \\pmb z^t  \\pmb \\Lambda \\pmb z = \\lambda_1  z_1^2 + \\lambda_2 z_2^2 + \\dots \\lambda_p z_p^2\n\\] 고유값은 다음과 같은 관계를 가지므로\n\\[\n\\lambda_1 \\ge\\lambda_2 \\ge \\dots \\ge \\lambda_p\n\\] 다음과 같은 부등식을 얻을 수 있으며\n\\[  \n\\lambda_p \\sum_{i=1}^p z_i^2 \\le  \\sum_{i=1}^p  \\lambda_i z_i^2 \\le \\lambda_1 \\sum_{i=1}^p z_i^2\n\\] 다시 위의 부등식에서 모든 항을 \\(\\pmb z^t \\pmb z = \\sum_{i=1}^p \\pmb z_i^2\\)으로 나누면 다음의 부등식을 얻는다.\n\\[\n\\lambda_p  \\le  \\frac{\\sum_{i=1}^p  \\lambda_i z_i^2}{\\sum_{i=1}^p z_i^2} \\le \\lambda_1\n\\]\n또한 \\(\\pmb z^t \\pmb z = \\pmb x^t \\pmb P \\pmb P^t \\pmb x^t=\\pmb x^t \\pmb x\\)으므로 마지막으로 아래의 부등식을 얻는다.\n\\[\n\\lambda_p  \\le  \\frac{\\pmb x^t \\pmb \\Sigma \\pmb x}{\\pmb x^t \\pmb x} \\le \\lambda_1\n\\] 이 부등식의 의미는 표준화된 2차형식의 최대값은 가장 큰 고유값이며 최소값은 가장 작은 고유값이다.\n\n\n6.2.4 주성분의 계수\n\\(p\\)-차원의 벡터 \\(\\pmb a\\)의 선형결합으로 새로운 변수 \\(Z_1= \\pmb a^t \\pmb X\\)를 고려할때 \\(Z_1\\)의 분산은 다음과 같다.\n\\[\nVar(Z_1) = Var( \\pmb a^t \\pmb X) = \\pmb a^t Var(\\pmb X) \\pmb a = \\pmb a^t \\pmb \\Sigma \\pmb a\n\\] 위에서 얻은 이차형식의 최대값을 구하는 과정을 보면 선형결합의 계수 \\(\\pmb a\\)를 공분산행렬에 대한 가장 큰 고유값 \\(\\lambda_1\\)에 해당하는 표준고유벡터 \\(\\pmb a_1\\) 를 사용하면 \\(Z_1\\)의 분산을 최대화 할 수 있다.\n\\[\nVar(Z_1) = Var( \\pmb a_1^t \\pmb X) = \\pmb a_1^t \\pmb \\Sigma \\pmb a_1 = \\pmb a_1^t (\\lambda_1 \\pmb a_1) = \\lambda_1 \\]\n이렇게 첫 번째 주성분 \\(Z_1 = \\pmb a_1^t \\pmb X\\) 을 찾을 수 있으며 두 번째 주성분, 세 번째 주성분들도 유사한 방법을 통해 찾아나간다. 두번째 주성분은 공분산행렬에 대한 두 번째로 큰 고유값 \\(\\lambda_2\\)에 해당하는 표준 고유벡터 \\(\\pmb a_2\\)로 사용하면 그 분산은 \\(\\lambda_2\\)이고 첫 번째 주성분과의 상관관계는 0이다.\n\\[\nVar(Z_2) = Var(\\pmb a_2^t \\pmb X) = \\pmb a_2^t Var(\\pmb X) \\pmb a_2 = \\lambda_2\n\\] \\[\nCov(Z_1,Z_2) = Cov(\\pmb a_1^t \\pmb X, \\pmb a_2^t \\pmb X) = \\pmb a_1^t Cov(\\pmb X) \\pmb a_2 =\n\\pmb a_1^t \\pmb P \\pmb \\Lambda \\pmb P^t \\pmb a_2 =0\n\\]\n따라서 주성분분석에서 \\(i\\)번째 주성분의 계수는 공분산행렬 \\(\\pmb \\Sigma\\)의 고유값 중 \\(i\\) 번째 큰 값인 \\(\\lambda_i\\)에 해당하는 고유벡터 \\(\\pmb a_i\\)로 놓는다.\n\\[\nZ_i = \\pmb a_i^t \\pmb X ,\\quad i=1,2,\\dots,p\n\\]\n\n\n6.2.5 공분산행렬과 상관계수행렬\n앞절에서 설명한 바와 같이 주성분을 반드는 경우 공분산행렬(Covariance matrix)를 사용하면 원래 변수의 단위가 바뀌면 주성분의 계수도 영향을 받는다. 변수들 중 분산이 큰 변수가 있으면 그 변수에 해당하는 주성분의 계수가 커지는 현상이 발생한다.\n이러한 단점을 보완하기 위하여 보통의 경우 주성분은 공분산행렬이 아닌 상관계수행렬 \\(\\pmb C\\) (Correlation matrix)의 고유값과 고유벡터를 이용하여 주성분을 만든다.\n앞에서 전개한 주성분분석의 이론은 공분산행렬 \\(\\pmb \\Sigma\\)를 상관계수행렬 \\(\\pmb C\\) 로 바꾸어 전개하면 된다. 주의할 점은 상관계수행렬을 이용하는 경우 모든 대각원소가 1으므로 총변동은 변수의 개수 \\(p\\)가 된다.\n\\[\n\\sum_{i=1}^p V(X_i) = \\sum_{i=1}^p V(Z_i) = trace( \\pmb C) = p  \n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>주성분 분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-pca.html#주성분-분석",
    "href": "qmd/multivar-pca.html#주성분-분석",
    "title": "6  주성분 분석",
    "section": "6.3 주성분 분석",
    "text": "6.3 주성분 분석\n\n6.3.1 주성분 개수의 선택\n주성분은 변수들의 모든 선형결합중에서 서로 상관되지 않고 원래 자료의 변동을 가능한 한 많이 설명한다는 의미에서 성분의 중요성이 감소하는 순서되로 유도된다. 따라서 주성분 분석의 목적에 따라 원래의 변수 개수인 \\(p\\)보다 적은 개수의 주성분들을 선택하여 원래 변수가 가지고 있는 변동의 많은 부분을 설명하려고 한다.\n문제는 몇 개의 주성분을 선택해야 하는 문제이다.\n각 주성분의 분산은 공분산 \\(\\pmb  \\Sigma\\)의 고유값이고 그 고유값들은 다음과 같은 순서로 정렬되어 있다\n\\[\n\\lambda_1 \\ge\\lambda_2 \\ge \\dots \\ge \\lambda_p\n\\]\n따라서 원래 변수 \\(X_1,X_2,\\dots,X_p\\)들의 분산의 합은 모든 고유값의 합과 같다.\n\\[\n\\sum_{i=1}^p Var(X_i) =\\sum_{i=1}^p \\sigma_{ii} = \\sum_{i=1}^p \\lambda_i = \\sum_{i=1}^p Var(Z_i)\n\\]\n이는 다음과 같은 식에서 유도 된다.\n\\[\ntrace(\\pmb  \\Sigma) = trace(\\pmb  P \\pmb  \\Lambda \\pmb  P^t) =trace( \\pmb  \\Lambda \\pmb  P^t \\pmb  P)= trace( \\pmb  \\Lambda)\n\\]\n따라서 주성분 \\(Z_i\\)는 자료의 총변동 \\(\\sum_{i=1}^p \\lambda_i =trace(\\pmb  \\Sigma)\\)를 다음과 같은 비율만큼 설명한다.\n\\[\nP_i = \\frac{\\lambda_i}{\\sum_{j=1}^p \\lambda_j}\n\\tag{6.9}\\]\n그리고 만약 \\(m&lt;p\\)개의 주성분 \\(Z_1,Z_2,\\dots,Z_m\\) 을 선택했다면 자료의 총변동을 다음과 같은 비율만큼 설명한다.\n\\[\nP_1+P_2+\\dots+P_m = \\frac{\\lambda_1+\\lambda_2+\\dots + \\lambda_m}{\\sum_{j=1}^p \\lambda_j}\n\\tag{6.10}\\]\n주성분의 개수를 선택하는 방법은 매우 다양하며 다음과 같은 몇 개의 절차가 일번적인 방법이다.\n\n총변동의 70%에서 90%를 설명할 수 있게 주성분의 개수를 선택\n\n\\[ 0.7 \\le P_1+P_2+\\dots+P_m \\le 0.9 \\]\n\n평균고유값 \\(\\sum_{i=1}^p \\lambda_i /p\\)보다 작은 분산을 가지는 주성분을 제외한다.\n주성분이 상관행렬로부터 추출되는 경우 고유값이 1보다 작은 주성분을 제외한다.\nScree diagram을 이용: Scree diagram은 고유값을 큰 순서대로 그림으로 그리고 선을 연결항 그래프이다. 고유값의 감소가 느리게 진행되는 때까지 큰 고유치를 선택하면 된다.\n\n\n\n6.3.2 주성분의 척도변경\n원래의 변수벡터 \\(\\pmb  X\\)와 주성분 \\(Z_j\\)의 공분산은 다음과 같다.\n\\[ Cov(\\pmb  X, Z_j) =Cov(\\pmb  X, \\pmb  a_j^t \\pmb  X) = Cov(\\pmb  X, \\pmb  X^t )\\pmb  a_j = \\pmb  \\Sigma \\pmb  a_j \\]\n고유벡터 \\(\\pmb  a_j\\)의 성질은 $ a_j =_j a_j$이므로 다음과 같은 결과를 얻고\n\\[\nCov(\\pmb  X , Z_j) = \\pmb  \\Sigma \\pmb  a_j =\\lambda_j \\pmb  a_j\n\\]\n따라서 원래의 변수 \\(X_i\\)와 주성분 \\(Z_j\\)의 공분산은 다음과 같다.\n\\[\nCov(X_i, Z_j) = \\lambda_j  a_{ji}\n\\] 더 나아가 원래의 변수 \\(X_i\\)와 주성분 \\(Z_j\\)의 상관계수는 다음과 같다.\n\\[ Corr(X_i, Z_j) = \\frac{Cov(X_i, Z_j)} {\\sqrt{Var(X_i) Var(Z_j)}}\n=  \\frac{\\lambda_j a_{ji}} {\\sqrt{\\sigma_{ii} \\lambda_j}}\n=\\frac{\\sqrt{\\lambda_j} a_{ji}} {\\sqrt{\\sigma_{ii} }}\n\\]\n만약 주성분이 상관계수 행렬에서 얻어졌다면 \\(\\sigma_{ii}=1\\)이므로\n\\[\nCorr(X_i, Z_j) = \\sqrt{\\lambda_j} a_{ji}\n\\]\n따라서 주성분을 만드는 경우 주성분의 표준 편차 \\(\\sqrt{\\lambda_j}\\) 으로 나누어 주면 원래 자료와의 공분산이 주성분변환의 계수로 나오게 된다.\n\\[ Cov \\left ( X_i, \\tfrac{Z_j}{\\sqrt{\\lambda_j}} \\right ) =  a_{ji} \\]\n\n\n6.3.3 주성분 점수\n자료의 변동을 설명하기 위하여 \\(m\\)개의 주성분이 필요하다면 각 표본의 개체에 대한 새로운 주성분의 값을 구할 수 있다. 이를 주성분 점수(Principal Component score)라고 한다.\n\\(p\\)개의 변수를 가지는 확률벡터를 독립적으로 \\(n\\)개 표본추출했다고 하자. \\(X_{ij}\\)는 \\(i\\)번째 표본의 \\(j\\)번째 변수이라고 하면 \\(i\\)번째 개체의 관측벡터는 \\(\\pmb  X_i\\)는 다음과 같다.\n\\[ \\pmb  X_i=(X_{i1},X_{i2},\\dots,X_{ip})^t \\] \\(j\\)번째 변수의 표본 평균을 \\(\\bar X_j = \\sum_{i=1}^n X_{ij}/n\\) 으로 정의하고 추출된 표본의 평균벡터를 \\(\\bar {\\pmb  X}\\)를 다음과 같다고 하자.\n\\[\n\\bar {\\pmb  X}  = (\\bar X_1,\\bar X_2,\\dots,\\bar X_p)^t\n\\]\n이제 각 관측값을 해당하는 변수의 평균으로 빼준 벡터 \\(\\pmb  Y_i\\) 를 고려하자.\n\\[\n\\pmb  Y_i = \\pmb  X_i - \\bar {\\pmb  X} =(X_{i1}- \\bar X_1 ,X_{i2}-\\bar X_2,\\dots,X_{ip}-\\bar X_p)^t\n\\]\n이제 \\(i\\)번째 개체의 \\(j\\) 번째 주성분 점수는 다음과 같이 정의된다.\n\\[  \nZ_{ij} =  \\pmb  a_j^t \\pmb  Y_i = \\sum_{k=1}^p a_{jk} Y_{ik} = \\sum_{k=1}^p a_{jk} (X_{ik} - \\bar X_k)\n\\tag{6.11}\\]\n주성분 점수는 각각 관측된 개체가 원래 \\(p\\)개의 변수를 가지고 있다면 주성분분석을 통해서 더 적은 수, \\(m&lt;p\\)개의 새로운 변수로서 관측 변수의 개수를 축소했다고 생각하면 된다.\n\\[\n\\pmb  Z_i=(Z_{i1},Z_{i2},\\dots,Z_{im})^t\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>주성분 분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-pca.html#예제-올림픽-7종-경기-자료",
    "href": "qmd/multivar-pca.html#예제-올림픽-7종-경기-자료",
    "title": "6  주성분 분석",
    "section": "6.4 예제: 올림픽 7종 경기 자료",
    "text": "6.4 예제: 올림픽 7종 경기 자료\n올림픽 7종 경기 자료에 주성분분석을 적용해보자. 7종 경기의 종목은 100m 허들, 높이뛰기, 투포환, 200m 달리기, 멀리뛰기, 창던지기, 800m달리기이다. 순위는 혼성 경기 득점표에 따라 각 종목의 종합 득점으로 정한다.\n올림픽 7종 경기 자료는 다음과 같이 불러올 수 있다. 25명의 선수들에 대한 7개의 종목의 기록과 총점(score)으로 구성된 자료이다. 마지막 변수 score 는 올림픽 위원회가 7개의 종목의 기록을 결합해서 만든 점수로서 메달의 순위를 결정한다.\n\nlibrary(HSAUR2)\n\ndf_1 &lt;- heptathlon\ndim(df_1)\n\n[1] 25  8\n\ncolnames(df_1)\n\n[1] \"hurdles\"  \"highjump\" \"shot\"     \"run200m\"  \"longjump\" \"javelin\"  \"run800m\" \n[8] \"score\"   \n\nhead(df_1)\n\n                    hurdles highjump  shot run200m longjump javelin run800m\nJoyner-Kersee (USA)   12.69     1.86 15.80   22.56     7.27   45.66  128.51\nJohn (GDR)            12.85     1.80 16.23   23.65     6.71   42.56  126.12\nBehmer (GDR)          13.20     1.83 14.20   23.10     6.68   44.54  124.20\nSablovskaite (URS)    13.61     1.80 15.23   23.92     6.25   42.78  132.24\nChoubenkova (URS)     13.51     1.74 14.76   23.93     6.32   47.46  127.90\nSchulz (GDR)          13.75     1.83 13.50   24.65     6.33   42.82  125.79\n                    score\nJoyner-Kersee (USA)  7291\nJohn (GDR)           6897\nBehmer (GDR)         6858\nSablovskaite (URS)   6540\nChoubenkova (URS)    6540\nSchulz (GDR)         6411\n\n\n먼저 7개의 자료의 값이 크기가 크면 좋은 기록이 나타내도록 변환을 한다. 따라서 육상경기 변수는 각 관측값을 그 변수의 최대값에서 빼준다. 또한 파푸아뉴기니아 선수가 너무 실력이 떨어져서 이상치로 분류하여 분석에서 제외한다.\n\ndf_1$hurdles &lt;- with(df_1, max(hurdles)-hurdles)\ndf_1$run200m &lt;- with(df_1, max(run200m)-run200m)\ndf_1$run800m &lt;- with(df_1, max(run800m)-run800m)\n\ndf_1 &lt;- df_1 %&gt;% \n            filter(!str_detect(rownames(.), \"PNG\"))\ndim(df_1)\n\n[1] 24  8\n\nhead(df_1)\n\n                    hurdles highjump  shot run200m longjump javelin run800m\nJoyner-Kersee (USA)    3.73     1.86 15.80    4.05     7.27   45.66   34.92\nJohn (GDR)             3.57     1.80 16.23    2.96     6.71   42.56   37.31\nBehmer (GDR)           3.22     1.83 14.20    3.51     6.68   44.54   39.23\nSablovskaite (URS)     2.81     1.80 15.23    2.69     6.25   42.78   31.19\nChoubenkova (URS)      2.91     1.74 14.76    2.68     6.32   47.46   35.53\nSchulz (GDR)           2.67     1.83 13.50    1.96     6.33   42.82   37.64\n                    score\nJoyner-Kersee (USA)  7291\nJohn (GDR)           6897\nBehmer (GDR)         6858\nSablovskaite (URS)   6540\nChoubenkova (URS)    6540\nSchulz (GDR)         6411\n\n\n먼저 7개 변수의 상관관계를 보자\n\ndf_1_t &lt;- df_1 %&gt;% \n          select(-score)\n\ncor(df_1_t)\n\n           hurdles  highjump      shot   run200m  longjump   javelin   run800m\nhurdles  1.0000000 0.5817409 0.7666860 0.8300371 0.8893472 0.3324779 0.5587794\nhighjump 0.5817409 1.0000000 0.4646854 0.3909024 0.6626910 0.3480793 0.1523350\nshot     0.7666860 0.4646854 1.0000000 0.6694330 0.7840380 0.3430333 0.4082925\nrun200m  0.8300371 0.3909024 0.6694330 1.0000000 0.8106176 0.4707969 0.5731902\nlongjump 0.8893472 0.6626910 0.7840380 0.8106176 1.0000000 0.2870826 0.5233809\njavelin  0.3324779 0.3480793 0.3430333 0.4707969 0.2870826 1.0000000 0.2559348\nrun800m  0.5587794 0.1523350 0.4082925 0.5731902 0.5233809 0.2559348 1.0000000\n\n\n주성분 분석은 함수 princomp 를 사용한다. 맨 마지막 8번째 변수 score 를 제외한 7개의 변수로 주성분분석을 실시하자. 함수 princomp 에서 선택문 cor = TRUE 는 상관계수행렬로 주성분분석을 실행하라는 선택이며 함수 summary 에서 loadings = TRUE 는 주성분의 계수를 보여주라는 것이다.\n\npca1 &lt;- princomp(df_1_t, cor = TRUE)\n\nsummary(pca1,loadings = TRUE)\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3     Comp.4     Comp.5\nStandard deviation     2.0793370 0.9481532 0.9109016 0.68319667 0.54618878\nProportion of Variance 0.6176632 0.1284278 0.1185345 0.06667967 0.04261745\nCumulative Proportion  0.6176632 0.7460909 0.8646255 0.93130515 0.97392260\n                           Comp.6      Comp.7\nStandard deviation     0.33745486 0.262042024\nProportion of Variance 0.01626797 0.009809432\nCumulative Proportion  0.99019057 1.000000000\n\nLoadings:\n         Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7\nhurdles   0.450         0.174         0.199  0.847       \nhighjump  0.315 -0.651  0.209 -0.557               -0.332\nshot      0.402         0.153  0.548 -0.672        -0.229\nrun200m   0.427  0.185 -0.130  0.231  0.618 -0.333 -0.470\nlongjump  0.451         0.270         0.122 -0.383  0.749\njavelin   0.242 -0.326 -0.881                       0.211\nrun800m   0.303  0.657 -0.193 -0.574 -0.319              \n\n\n주성분 분석의 결과를 다음과 같이 해석할 수 있다.\n\n주성분 2개를 선택하면 총변동의 76 % 를 설명하고 3개를 선택하면 86 % 를 설명한다.\n\nProportion of Variance 은 식 6.9 에서 정의된 \\(P_i\\)의 값을 나타낸다.\nCumulative Proportion 식 6.10 의 \\(P_i\\) 이 누적합을 나타낸다.\n\n먼저, 계수의 값이 없는 이유는 상대적으로 작은 값은 제시하지 않아서 이다\n첫번째 주성분(Comp.1)의 계수를 보면 각 종목 점수의 가중평균과 유사하다.\n두 번째 주성분(Comp.2)은 800달리기와 높이뛰기의 차이로 해석된다(각각의 계수가 다른 변수에 비해 절상대적으로 크고 부호가 반대이다).\n3번째 주성분(Comp.3)은 창던지기(javelin)의 능력을 나타내는것으로 해석된다.\n\n주성분 개수의 선택에 이용되는 scree plot은 다음과 같다. 막대의 높이는 주성분의 분산, 즉 고유치와 비례한다\n\nplot(pca1, main = \"scree plot\")\n\n\n\n\n\n\n\n\n25명의 선수에 대한 식 6.11 에 의한 주성분 점수를 구할 수 있고 첫번째와 두번째 주성분 점수를 산점도로 그려보았다.\n\nhead(pca1$scores)\n\n                       Comp.1     Comp.2       Comp.3     Comp.4     Comp.5\nJoyner-Kersee (USA) 4.8598544 -0.1428696  0.006170444  0.2997271  0.3696153\nJohn (GDR)          3.2156489  0.9689924  0.249166030  0.5609829 -0.7698539\nBehmer (GDR)        2.9891207  0.7102977 -0.635677626 -0.5666763  0.1944444\nSablovskaite (URS)  1.3158405  0.1828572 -0.256022940  0.6508784 -0.6166041\nChoubenkova (URS)   1.5357870  0.9824589 -1.818885157  0.8008982 -0.6023826\nSchulz (GDR)        0.9790817  0.3587703 -0.421970960 -1.1374969 -0.7302135\n                         Comp.6      Comp.7\nJoyner-Kersee (USA) -0.27632076  0.48611032\nJohn (GDR)           0.38582368  0.05283965\nBehmer (GDR)        -0.26334765 -0.11292729\nSablovskaite (URS)  -0.22039763 -0.54216684\nChoubenkova (URS)    0.08186703  0.30728837\nSchulz (GDR)        -0.25984050 -0.03921361\n\ndf_pca &lt;- as.data.frame(pca1$scores)\n\ndf_pca &lt;- df_pca %&gt;% \n  rownames_to_column(\"label\") %&gt;%\n  left_join(df_1 %&gt;% rownames_to_column(\"label\") %&gt;% select(label, score), by=\"label\") %&gt;%\n  mutate(rank = as.integer(rank(-score))) %&gt;%\n  mutate(label = paste0(label, \"(\", rank, \")\"))\n\n\nhead(df_pca)\n\n                   label    Comp.1     Comp.2       Comp.3     Comp.4\n1 Joyner-Kersee (USA)(1) 4.8598544 -0.1428696  0.006170444  0.2997271\n2          John (GDR)(2) 3.2156489  0.9689924  0.249166030  0.5609829\n3        Behmer (GDR)(3) 2.9891207  0.7102977 -0.635677626 -0.5666763\n4  Sablovskaite (URS)(4) 1.3158405  0.1828572 -0.256022940  0.6508784\n5   Choubenkova (URS)(4) 1.5357870  0.9824589 -1.818885157  0.8008982\n6        Schulz (GDR)(6) 0.9790817  0.3587703 -0.421970960 -1.1374969\n      Comp.5      Comp.6      Comp.7 score rank\n1  0.3696153 -0.27632076  0.48611032  7291    1\n2 -0.7698539  0.38582368  0.05283965  6897    2\n3  0.1944444 -0.26334765 -0.11292729  6858    3\n4 -0.6166041 -0.22039763 -0.54216684  6540    4\n5 -0.6023826  0.08186703  0.30728837  6540    4\n6 -0.7302135 -0.25984050 -0.03921361  6411    6\n\ndf_pca%&gt;%\n  ggplot(aes(x=Comp.1, y=Comp.2 , label=label, size=score)) + \n  geom_point(size= 2, alpha = 0.7) +\n  geom_text(size= 3 ,vjust = -0.7, color = \"blue\", alpha=0.9) +\n  geom_hline(yintercept=0) +\n  geom_vline(xintercept=0) +\n  theme_bw() + \n  labs(x=\"PCA 1\", y=\"PCA 2\", title = \"주성분 점수 산점도\")\n\n\n\n\n\n\n\n\nBiplot 은 25개의 개체에 대하여 2개의 주성분 점수와 그 방향을 이차원 공간에 표시한 그림으로 주성분의 구조를 이해하는데 큰 도움이 된다. Biplot 은 패키지 ggfortify 의 autoplot 함수를 이용하면 다음과 같이 그릴 수 있다.\n\nautoplot(pca1, label = TRUE, loadings = TRUE, loadings.label = TRUE)\n\n\n\n\n\n\n\n\n화살표로 표시된 각 변수에 대한 방향 벡터는 2개의 주성분 계수들 중 각 변수에 해당하는 두 계수(Comp.1, Comp.2) 벡터의 방향이다. 위의 Biplot에서 다음과 같은 사실을 알 수 있다.\n\n각 변수에 대한 벡터의 방향은 두 주성분의 축에서 더 평행한 방향으로 주성분 점수에 기여도가 높다.\n각 변수에 대한 벡터의 각도는 상관성을 나타내며 그 각이 작을수록 상관관계가 크다.\n첫번째 주성분은 hurdle, longjump, 200m가 변동을 주로 설명한다.\n\n선수 Joyner가 오른쪽 끝에 위치하는 것은 첫번째 주성분 점수가 가장 크고 hurdle, longjump, 200m 에서 좋은 점수를 낸것을 알려준다.\n\n두번째 주성분은 800m와 highjump가 변동을 주로 설명하며 두 변수의 상관관계가 상대적으로 작으므로 방향이 반대인 것을 알 수 있다.\n\n800m와 highjump 벡터 방향이 서로 반대이며 각 벡터 방향에 가까운 선수들이 해당 종목의 점수가 높음을 알 수 있다.\n예를 들어 선수 Yuping 은 highjump 점수가 높으므로 highjump 방향인 아래쪽에 위치하고 있다.\n\n\n마지막으로 첫번째 주성분 점수와 올림픽 위원회가 구한 총점과의 관계를 보자. 두 값이 매우 강한 상관관계를 나타낸다.\n\ncor(df_1$score, df_pca$`Comp.1`)\n\n[1] 0.9931168\n\nplot(df_1$score, df_pca$`Comp.1`, xlab=\"Olympic Score\", ylab=\"PCA1\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>주성분 분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-pca.html#예제-포도주-자료",
    "href": "qmd/multivar-pca.html#예제-포도주-자료",
    "title": "6  주성분 분석",
    "section": "6.5 예제: 포도주 자료",
    "text": "6.5 예제: 포도주 자료\n포도주 자료는 먼저 1978년에 이탈리아 피에몬테(Piedmont) 지역에서 재배된 세 가지 포도 품종(cultivar) 와인의 화학 성분을 측정한 자료이다. 전체 178 걔의 와인이 3개의 그룹으로 나누어저 있다. 변수 Class 는 포도의 품종을 나타내는 범주형 변수이다.\n\n\n\nClass\n포도 품종(지역)\n\n\n\n\n1\nBarolo (바롤로)\n\n\n2\nGrignolino (그리뇰리노)\n\n\n3\nBarbera (바르베라)\n\n\n\n포도주 자료는 사용와인의 화학 조성을 기반으로 주요 성분 간 상관관계를 분석할 수 있으며 포도 품종 간 차이 비교하는데 사용된다. 차원 축소(PCA) 및 군집 분석에 자주 사용되는 자료이다.\n13개의 변수에 대한 설명은 다음과 같다.\n\n\n\n\n\n\n\n변수명\n설명\n\n\n\n\nClass\n포도 품종 (1, 2, 3 세 가지 품종 구분)\n\n\nAlcohol\n알코올 함량 (%)\n\n\nMalic_Acid\n사과산 함량 – 와인의 신맛을 결정하는 유기산\n\n\nAsh\n회분 – 무기질 함량 지표\n\n\nAlcalinity_of_Ash\n회분의 알칼리도 – 산-알칼리 균형 지표\n\n\nMagnesium\n마그네슘 함량 (mg/L)\n\n\nTotal_Phenols\n총 페놀 성분 – 향, 맛, 항산화 효과에 기여\n\n\nFlavanoids\n플라보노이드 성분 – 떫은맛, 색, 향과 관련\n\n\nNonflavanoid_Phenols\n비플라보노이드 페놀 성분\n\n\nProanthocyanins\n프로안토시아닌 – 탄닌 전구체, 색과 떫은맛 영향\n\n\nColor_Intensity\n색 농도 – 와인의 색상이 얼마나 짙은지\n\n\nHue\n색조 – 적색 대비 황색 비율\n\n\nOD280_OD315\n280nm/315nm 흡광도 비율 – 페놀 함량 및 품질 지표\n\n\nProline\n프롤린 함량 – 아미노산, 와인 향과 숙성에 중요한 역할\n\n\n\n\n6.5.1 자료 불러오기\n\nurl &lt;- \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\nwine &lt;- read.csv(url, header = FALSE)\n\n# Add column names from UCI description\ncolnames(wine) &lt;- c(\"Class\",\"Alcohol\",\"Malic_Acid\",\"Ash\",\"Alcalinity_Ash\",\"Magnesium\",\n                    \"Total_Phenols\",\"Flavanoids\",\"Nonflav_Phenols\",\"Proanthocyanins\",\n                    \"Color_Intensity\",\"Hue\",\"OD280_OD315\",\"Proline\")\nhead(wine)\n\n  Class Alcohol Malic_Acid  Ash Alcalinity_Ash Magnesium Total_Phenols\n1     1   14.23       1.71 2.43           15.6       127          2.80\n2     1   13.20       1.78 2.14           11.2       100          2.65\n3     1   13.16       2.36 2.67           18.6       101          2.80\n4     1   14.37       1.95 2.50           16.8       113          3.85\n5     1   13.24       2.59 2.87           21.0       118          2.80\n6     1   14.20       1.76 2.45           15.2       112          3.27\n  Flavanoids Nonflav_Phenols Proanthocyanins Color_Intensity  Hue OD280_OD315\n1       3.06            0.28            2.29            5.64 1.04        3.92\n2       2.76            0.26            1.28            4.38 1.05        3.40\n3       3.24            0.30            2.81            5.68 1.03        3.17\n4       3.49            0.24            2.18            7.80 0.86        3.45\n5       2.69            0.39            1.82            4.32 1.04        2.93\n6       3.39            0.34            1.97            6.75 1.05        2.85\n  Proline\n1    1065\n2    1050\n3    1185\n4    1480\n5     735\n6    1450\n\n\n먼저 품종(Class)을 제외한 13개의 변수의 상관계수 행렬을 시각화 해보자.\n\n## 품종 제외한 변수 선택\ndf_2 &lt;- wine %&gt;% select(-Class)\n\n## 상관계수 행렬 계산\ncor_mat &lt;- cor(df_2, use=\"pairwise.complete.obs\") \n\n# 히트맵 그리기\npheatmap(cor_mat,  display_numbers = TRUE, number_format = \"%.2f\")\n\n\n\n\n\n\n\n\n상관계수 행렬을 보먄 변수들이 3개의 그룹으로 나누어져 있다것을 알 수 있다.\n\n\n6.5.2 주성분 분석\n이제 wine 자료에 대하여 주성분 분석을 적용해 보자.\n\npca_wine &lt;- princomp(df_2, cor = TRUE)\nsummary(pca_wine, loadings = TRUE)\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3    Comp.4     Comp.5\nStandard deviation     2.1692972 1.5801816 1.2025273 0.9586313 0.92370351\nProportion of Variance 0.3619885 0.1920749 0.1112363 0.0706903 0.06563294\nCumulative Proportion  0.3619885 0.5540634 0.6652997 0.7359900 0.80162293\n                           Comp.6     Comp.7     Comp.8     Comp.9    Comp.10\nStandard deviation     0.80103498 0.74231281 0.59033665 0.53747553 0.50090167\nProportion of Variance 0.04935823 0.04238679 0.02680749 0.02222153 0.01930019\nCumulative Proportion  0.85098116 0.89336795 0.92017544 0.94239698 0.96169717\n                          Comp.11    Comp.12     Comp.13\nStandard deviation     0.47517222 0.41081655 0.321524394\nProportion of Variance 0.01736836 0.01298233 0.007952149\nCumulative Proportion  0.97906553 0.99204785 1.000000000\n\nLoadings:\n                Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9\nAlcohol          0.144  0.484  0.207         0.266  0.214         0.396  0.509\nMalic_Acid      -0.245  0.225        -0.537         0.537 -0.421              \nAsh                     0.316 -0.626  0.214  0.143  0.154  0.149 -0.170 -0.308\nAlcalinity_Ash  -0.239        -0.612               -0.101  0.287  0.428  0.200\nMagnesium        0.142  0.300 -0.131  0.352 -0.727        -0.323 -0.156  0.271\nTotal_Phenols    0.395        -0.146 -0.198  0.149               -0.406  0.286\nFlavanoids       0.423        -0.151 -0.152  0.109               -0.187       \nNonflav_Phenols -0.299        -0.170  0.203  0.501 -0.259 -0.595 -0.233  0.196\nProanthocyanins  0.313        -0.149 -0.399 -0.137 -0.534 -0.372  0.368 -0.209\nColor_Intensity         0.530  0.137               -0.419  0.228              \nHue              0.297 -0.279         0.428  0.174  0.106 -0.232  0.437       \nOD280_OD315      0.376 -0.164 -0.166 -0.184  0.101  0.266                0.137\nProline          0.287  0.365  0.127  0.232  0.158  0.120         0.120 -0.576\n                Comp.10 Comp.11 Comp.12 Comp.13\nAlcohol          0.212   0.226   0.266         \nMalic_Acid      -0.309          -0.122         \nAsh                      0.499          -0.141 \nAlcalinity_Ash          -0.479                 \nMagnesium                                      \nTotal_Phenols   -0.320  -0.304   0.304  -0.464 \nFlavanoids      -0.163                   0.832 \nNonflav_Phenols  0.216  -0.117           0.114 \nProanthocyanins  0.134   0.237          -0.117 \nColor_Intensity -0.291          -0.604         \nHue             -0.522          -0.259         \nOD280_OD315      0.524          -0.601  -0.157 \nProline          0.162  -0.539                 \n\n\n\n첫 번째 주성분의 계수를 보면 Flavanoids , Total_Phenols, OD280_OD315, Proanthocyanins 의 계수값이 양수로 다른 변수들에 비하여 상재적으로 크게 나타난 것을 알 수 있다. 또한 앞의 상관관계 분석에서도 4개 변수들의 상관관계가 높다는 것을 알 수 있다. 4개의 변수들은 맛의 풍부함 또는 맛의 강도와 관련된 변수들이다.\n두번째 주성분의 계수를 보면 Color_Intensity , Alcohol, Ash, Hue 의 계수값이 양수로서 다른 변수들에 비하여 상대적으로 크게 나타난 것을 알 수 있다. 이러한 변수들는 서로 상관관계가 높다는 것을 알 수 있다. 이 변수들 중 Color_Intensity 와 Hue 는 색상에 관련된 특성을 가지고 있다.\n\n자료의 변동을 설명하는 비율을 보기 위해서 scree plot 을 만들어 보자. 앞의 결과에서 보았듯이 두 개의 주성분이 55%의 변동을 설명한다\n\nplot(pca_wine, main = \"scree plot\")\n\n\n\n\n\n\n\n\nBiplot 은 178개의 포도주 샘플에 대하여 2개의 주성분 점수와 그 방향을 이차원 공간에 표시한 그림으로 주성분의 구조를 이해하는데 큰 도움이 된다. 아래 그림에서 보듯이 첫 번째 주성분과 두 번쨰 주성분을 구성하는 변수들의 역할을 시각화하여 볼 수 있다.\n\nautoplot(pca_wine, label = FALSE, loadings = TRUE, loadings.label = TRUE)\n\n\n\n\n\n\n\n\n이제 마지막으로 주성분 분석을 통해 얻은 2개의 주성분 점수를 이용하여 포도 품종 간의 차이를 비교해 보자. 14개의 변수가 아닌 2개의 주성분으로 차원을 축소했음에도 불구하고 3개의 품종이 잘 구분되는 것을 알 수 있다.\n\n#- 주성분 점수 데이터프레임 생성\ndf_pca &lt;- as.data.frame(pca_wine$scores)\n\n#- 품종 정보를 데이터프레임에 추가\ndf_pca$Class &lt;- factor(wine$Class)\n\n#- 주성분 점수 산점도 그리기\nggplot(df_pca, aes(Comp.1, Comp.2, color = Class)) +\n  geom_point(size = 2) +\n  labs(title = \"PCA on Wine dataset\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>주성분 분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-cancor.html",
    "href": "qmd/multivar-cancor.html",
    "title": "7  정준상관분석",
    "section": "",
    "text": "7.1 상관계수\n두 개의 확률변수의 상관계수(Correlation Cefficient)는 두 변수의 선형 관계의 정도를 나타내는 측도이다. 두 개의 확률변수 \\(X_1\\)과 \\(X_2\\) 의 상관계수 \\(\\rho\\) 는 다음과 같이 정의된다.\n\\[\n\\rho =\\rho(X_1, X_2) = cor(X_1, X_2)  =  \\frac{ Cov(X_1,X_2)} { \\sqrt{ Var(X_1) Var(X_2)}}\n\\tag{7.1}\\]\n상관계수 \\(\\rho\\)는 -1과 1 사이의 값을 가지며 상관계수가 0 이면 두 확률변수의 선형관계는 존재하지 않는다. 상관계수가 1 에 가까울수록 두 변수는 양의 선형관계가 강해지며 반대로 -1 에 가까울수록 두 변수는 음의 선형관계가 강해진다. 두 변수의 상관계수가 0이라고 해서 관계(relation)가 없다고 단정할 수 없다. 왜냐하면 상관계수는 두 변수의 선형관계(linear relationship)만을 나타내는 측도이고 비선형 관계 등 다른 특별한 관계를 반영하지는 못한다.\n두 개의 변수에 대한 \\(n\\)개의 독립표본 \\((X_{11},X_{12}),(X_{21},X_{22}), \\dots, (X_{n1},X_{n2})\\)이 주어지면 표본으로 부터 모집단의 상관계수를 추정할 수 있는 표본 상관계수 \\(\\hat \\rho\\) 을 다음과 같이 계산할 수 있다.\n\\[\n\\hat \\rho = \\frac{ \\sum_{i=1}^n (X_{i1} -\\bar X_1)(X_{i2}-\\bar X_2)} { \\sqrt{ \\sum_{i=1}^n (X_{i1} - \\bar X_1)^2 (X_{i2} - \\bar X_2)^2}}\n\\tag{7.2}\\]\n여기서 \\(\\bar X_1\\)과 \\(\\bar X_2\\)는 각각 첫 번째 변수의 표본 \\(X_{11},X_{21},\\dots X_{n1}\\)과 두 번째 변수의 표본 \\(X_{12},X_{22},\\dots X_{n2}\\)의 평균이다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>정준상관분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-cancor.html#상관계수",
    "href": "qmd/multivar-cancor.html#상관계수",
    "title": "7  정준상관분석",
    "section": "",
    "text": "보기 7.1 (스위스의 47개 주 자료) R 패키지에 내장된 swiss 자료는 1988년 스위스의 47개 주에 대한 출산율과 사회경제변수를 모아놓은 자료이다(\\(n=47\\)). 6개의 변수에 대한 설명과 자료의 일부는 다음과 같다.\n\nFertility: \\(I_g\\), common standardized fertility measure\nAgriculture: % of males involved in agriculture as occupation\nExamination: % draftees receiving highest mark on army examination\nEducation: % education beyond primary school for draftees.\nCatholic: % catholic (as opposed to protestant).\nInfant.Mortality: live births who live less than 1 year.\n\n\nhead(swiss)\n\n             Fertility Agriculture Examination Education Catholic\nCourtelary        80.2        17.0          15        12     9.96\nDelemont          83.1        45.1           6         9    84.84\nFranches-Mnt      92.5        39.7           5         5    93.40\nMoutier           85.8        36.5          12         7    33.77\nNeuveville        76.9        43.5          17        15     5.16\nPorrentruy        76.1        35.3           9         7    90.57\n             Infant.Mortality\nCourtelary               22.2\nDelemont                 22.2\nFranches-Mnt             20.2\nMoutier                  20.3\nNeuveville               20.6\nPorrentruy               26.6\n\n\n자료에서 두개의 변수에 대한 상관계수는 cor 함수로 다음과 같이 계산할 수 있고 더 나아가 데이터프레임 swiss 에 있는 모든 변수들에 대한 상관계수행렬도 동시에 계산할 수 있다.\n\ncor(swiss)\n\n                  Fertility Agriculture Examination   Education   Catholic\nFertility         1.0000000  0.35307918  -0.6458827 -0.66378886  0.4636847\nAgriculture       0.3530792  1.00000000  -0.6865422 -0.63952252  0.4010951\nExamination      -0.6458827 -0.68654221   1.0000000  0.69841530 -0.5727418\nEducation        -0.6637889 -0.63952252   0.6984153  1.00000000 -0.1538589\nCatholic          0.4636847  0.40109505  -0.5727418 -0.15385892  1.0000000\nInfant.Mortality  0.4165560 -0.06085861  -0.1140216 -0.09932185  0.1754959\n                 Infant.Mortality\nFertility              0.41655603\nAgriculture           -0.06085861\nExamination           -0.11402160\nEducation             -0.09932185\nCatholic               0.17549591\nInfant.Mortality       1.00000000",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>정준상관분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-cancor.html#다중상관계수",
    "href": "qmd/multivar-cancor.html#다중상관계수",
    "title": "7  정준상관분석",
    "section": "7.2 다중상관계수",
    "text": "7.2 다중상관계수\n상관계수는 두 개의 확률변수에 대한 선형관계를 나타내는 측도이다. 두 개의 변수에 대한 측도인 상관계수를 를 두 개의 확률벡터에 대한 관계를 나타내는 측도로 확장할 수 있다.\n이렇게 두 개의 확률벡터에 대한 상관관계를 나타내는 측도를 정준상관계수(Canonical Correlation Coeffcient)라고 한다. 이 절에서는 두 개의 확률벡터에 대한 관계를 측정하는 정준상관계수를 정의하기 전에 하나의 확률변수와 여러 개의 변수를 포함하는 확률벡터의 관계를 나타내는 다중상관계수(Multiple Correlation Coefficient)을 먼저 정의하고자 한다.\n확률벡터 \\(\\pmb X\\)를 \\(p\\)개의 확률변수로 이루어졌다고 하고 하나의 확률변수 \\(X_1\\) (편의상 첫 번째 확률변수를 선택하였다)와 나머지 \\(p-1\\)개의 변수로 구성된 확률벡터를 \\(\\pmb X_*\\)라고 하자.\n\\[\n\\pmb X =\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3 \\\\\n\\vdots \\\\\nX_p\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nX_1 \\\\\n\\pmb X_*\n\\end{bmatrix},\n\\quad\nCov(\\pmb X) =\n\\begin{bmatrix}\n\\sigma_{11} & \\pmb \\sigma_{12}^t \\\\\n\\pmb \\sigma_{12} & \\pmb \\Sigma_{22}\n\\end{bmatrix}\n\\] 위의 식에서 \\(\\sigma_{11}\\)은 \\(X_1\\)의 분산, \\(\\pmb \\sigma_{12}\\)은 \\(X_1\\)과 \\(\\pmb X_*\\)의 (\\(p-1\\))-차원 공분산 벡터, \\(\\pmb \\Sigma_{22}\\)는 \\(\\pmb X_*\\)의 \\((p-1)\\times (p-1)\\)-차원 공분산행렬이다.\n하나의 확률변수와 여러 개의 확률변수로 구성된 확률벡터간의 선형관계는 \\(X_1\\)과 \\(\\pmb X_*\\)에 포함된 각각의 확률변수에 대하여 각각 \\(p-1\\)개의 상관계수를 구하여 따로 따로 파악할 수 있다.\n\\[\n\\rho_{12}=cor(X_1,X_2), ~ \\rho_{13}=cor(X_1,X_3),  \\dots, ~ \\rho_{1p}=cor(X_1,X_p)\n\\]\n하지만 이러한 관계는 각각 두 개의 변수들에 대한 관계로서 하나의 확률변수와 확률벡터의 관계를 하나의 측도로서 종합적으로 반영하는 것은 아니다. 이렇게 확률변수와 확률벡터의 관계를 하나의 측도로 나타내기 위하여 주성분분석이나 회귀분석과 유사하게 확률벡터 \\(\\pmb X_*\\)에 포함된 확률변수들의 선형조합을 생각하고 확률변수 \\(X_1\\)과 선형조합으로 만들어진 새로운 확률변수 \\(\\pmb a^t \\pmb X_*\\)의 상관관계를 생각해 보자.\n\\[\ncor(X_1,  a_2 X_2 + a_3 X_3 +\\dots + a_p X_p) = cor(X_1,\\pmb a^t \\pmb X_*)\n\\]\n위의 식에서 선형조합으로 만들어진 새로운 확률변수와의 상관계수는 계수벡터 \\(\\pmb a\\)의 계수값에 따라 달라진다. 이렇게 무수히 많은 값이 가능한 경우 최대의 상관계수를 가지는 계수를 고려하는 것이 다중상관계수의 정의이다.\n\\[\n\\rho(X_1, \\pmb X_*) = \\max_{\\pmb a} cor(X_1, \\pmb a^t \\pmb X_*)\n\\tag{7.3}\\]\n최대의 상관계수를 가지는 계수를 구하기 위하여 먼저 임의의 벡터 \\(\\pmb a\\)에 대하여 \\(X_1\\)과 \\(\\pmb a^t \\pmb X_*\\)의 상관계수를 유도해보자. 먼저\n\\[\n\\begin{aligned}\nCov(X_1,\\pmb a^t \\pmb X_*) & =\n  E [(X_1 -E(X_1)][\\pmb a^t \\pmb X_*-E(\\pmb a^t \\pmb X_*) ] \\\\\n  &= \\pmb a^t E [(X_1 -E(X_1)] [ \\pmb X_*-E(\\pmb X_*) ] \\\\\n  &= \\pmb a^t \\pmb \\sigma_{12} \\\\\n  Var(\\pmb a^t \\pmb X_*) &= \\pmb a^t Var( \\pmb X_*) \\pmb a \\\\\n   &= \\pmb a^t \\pmb \\Sigma_{22} \\pmb a\n\\end{aligned}\n\\]\n따라서\n\\[\ncor(X_1,\\pmb a^t \\pmb X_*) =  \\frac{ Cov(X_1,\\pmb a^t \\pmb X_*)} { \\sqrt{ Var(X_1) Var(\\pmb a^t \\pmb X_*)}} = \\frac{\\pmb a^t \\pmb \\sigma_{12}  } { \\sqrt{\\sigma_{11} (\\pmb a^t \\pmb \\Sigma_{22} \\pmb a ) }}\n\\] 여기서 한 가지 유의해야할 점은 계수벡터 \\(\\pmb a\\)의 계수들의 부호만을 바꾸면 다중상관계수의 부호가 바뀐다는 점이다.\n\\[\ncor(X_1,-\\pmb a^t \\pmb X_*) = \\frac{-\\pmb a^t \\pmb \\sigma_{12}  } { \\sqrt{\\sigma_{11} [(-\\pmb a^t) \\pmb \\Sigma_{22} (-\\pmb a )] }}  =  - cor(X_1,\\pmb a^t \\pmb X_*)\n\\] 따라서 다중상관계수를 구하는 경우에는 그 부호에 관계없이 절대값이 가장 큰 경우를 고려해야 한다. 이러한 점을 고려하여 다중상관계수를 다음과 같이 자신의 제곱값을 통하여 유도할 수 있다. 이제 다중상관계수의 계수벡터를 구하기 위한 다중상관계수의 제곱에 대한 최대값을 구해보자.\n\\[\n\\begin{aligned}\ncor^2(X_1,\\pmb a^t \\pmb X_*) &=\n\\frac{ (\\pmb a^t \\pmb \\sigma_{12})^2  } { \\sigma_{11} (\\pmb a^t \\pmb \\Sigma_{22} \\pmb a ) } \\\\\n&= \\frac{(\\pmb a^t \\pmb \\Sigma_{22}^{1/2} \\pmb \\Sigma_{22}^{-1/2} \\pmb \\sigma_{12})^2  } { \\sigma_{11} (\\pmb a^t \\pmb \\Sigma_{22} \\pmb a ) }  \\\\\n& \\le \\frac{ (\\pmb a^t \\pmb \\Sigma_{22}^{1/2}\\pmb \\Sigma_{22}^{1/2} \\pmb a)(\\pmb \\sigma_{12}^t \\pmb \\Sigma_{22}^{-1/2} \\pmb \\Sigma_{22}^{-1/2} \\pmb \\sigma_{12})  } { \\sigma_{11} (\\pmb a^t \\pmb \\Sigma_{22} \\pmb a ) }  \\\\\n&= \\frac{ (\\pmb a^t \\pmb \\Sigma_{22} \\pmb a)(\\pmb \\sigma_{12}^t \\pmb \\Sigma_{22}^{-1}  \\pmb \\sigma_{12})  } { \\sigma_{11} (\\pmb a^t \\pmb \\Sigma_{22} \\pmb a ) }  \\\\\n&= \\frac{ \\pmb \\sigma_{12}^t \\pmb \\Sigma_{22}^{-1}  \\pmb \\sigma_{12}  } { \\sigma_{11}  }  \\\\\n\\end{aligned}\n\\]\n위의 유도식에서 공분산 행렬 \\(\\pmb \\Sigma\\) 의 제곱근 행렬 \\({\\pmb \\Sigma}^{1/2}\\) 는 양정치 행렬의 성질 식 C.17 을 이용하였다.\n또한 위의 유도식에서 사용한 부등식은 코쉬-쉬바르쯔(Cauchy-Schwarz) 부등식을 적용한 결과이다 (식 5.18 참조)\n\\[\n(\\pmb \\alpha^t \\pmb \\beta)^2  \\le (\\pmb \\alpha^t \\pmb \\alpha)(\\pmb \\beta^t \\pmb \\beta)\n\\] 또한 위의 유도식에서 부등식의 등식이 성립하는 경우는 임의의 상수 \\(\\lambda\\)에 대하여 다음과 같은 관계가 성립하는 경우이다.\n\\[\n\\Sigma_{22}^{1/2} \\pmb a = \\lambda \\pmb \\Sigma_{22}^{-1/2}  \\pmb \\sigma_{12} \\quad \\rightarrow \\quad\n\\pmb a = \\lambda \\pmb \\Sigma_{22}^{-1}  \\pmb \\sigma_{12}\n\\tag{7.4}\\]\n이제 \\(X_1\\)과 \\(\\pmb X_*\\) 의 다중상관계수는 다음과 같이 구해진다.\n\\[\n\\rho(X_1, \\pmb a^t \\pmb X_*) = \\frac{ (\\pmb \\sigma_{12}^t \\pmb \\Sigma_{22}^{-1}  \\pmb \\sigma_{12})^{1/2}  } { (\\sigma_{11})^{1/2}  }\n\\tag{7.5}\\]\n여기서 \\(\\pmb a = \\lambda \\pmb \\Sigma_{22}^{-1}  \\pmb \\sigma_{12}\\) , \\(0 \\le \\rho(X_1, \\pmb a^t \\pmb X_*) \\le 1\\)\n다중상관계수의 계산은 다음과 같이 상관계수들로 구할 수 있다.\n\\[\n\\rho(X_1, \\pmb a^t \\pmb X_*) = (\\pmb \\rho_{12}^t \\pmb R_{22}^{-1}  \\pmb \\rho_{12})^{1/2}\n\\] 여기서 \\(\\pmb \\rho_{12}\\)는 \\(X_1\\)과 \\(\\pmb X_*\\)의 상관계수 벡터이며 \\(\\pmb R_{22}\\)는 \\(\\pmb X_*\\)의 상관계수행렬이다. 이러한 계산식의 유도는 아래와 같은 공분산 행렬과 상관계수 행렬의 관계(2차원 확률벡터의 예)로부터 유도할 수 있다.\n\\[  \n\\pmb R =\n\\begin{bmatrix}\n1 & \\rho_{12} \\\\\n\\rho_{12} & 1 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1/\\sqrt{\\sigma_{11}} & 0 \\\\\n0 & 1/\\sqrt{\\sigma_{22}} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} \\\\\n\\sigma_{12} & \\sigma_{22} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1/\\sqrt{\\sigma_{11}} & 0 \\\\\n0 & 1/\\sqrt{\\sigma_{22}} \\\\\n\\end{bmatrix}\n\\]\n표본으로서 자료가 주어진 경우에는 식 7.5 에서 각각의 모수에 대하여 그 추정량을 구하면 표본 다중상관계수를 구할 수 있다.\n\\[\n\\hat \\rho(X_1, \\pmb a^t \\pmb X_*) = \\frac{ ( \\hat {\\pmb \\sigma}_{12}^t \\hat {\\pmb \\Sigma}_{22}^{-1}  \\hat {\\pmb \\sigma}_{12})^{1/2}  } { (\\hat \\sigma_{11})^{1/2}  } = ( \\hat {\\pmb \\rho}_{12}^t \\hat {\\pmb R}_{22}^{-1}  \\hat {\\pmb \\rho}_{12} )^{1/2}\n\\]\nswiss 자료에서 출산율(\\(X_1\\))과 나머지 5개의 사회경제변수(\\(\\pmb X_*\\))의 표본 다중상관계수는 다음과 같이 구할 수 있다.\n\nn&lt;-dim(swiss)[1] ; p &lt;- dim(swiss)[2]\ndim(swiss)\n\n[1] 47  6\n\nR &lt;- cor(swiss)\nR12 &lt;- matrix(R[2:p,1],p-1,1)\nR12\n\n           [,1]\n[1,]  0.3530792\n[2,] -0.6458827\n[3,] -0.6637889\n[4,]  0.4636847\n[5,]  0.4165560\n\nR22 &lt;- matrix(R[2:p,2:p],p-1,p-1)\nR22\n\n            [,1]       [,2]        [,3]       [,4]        [,5]\n[1,]  1.00000000 -0.6865422 -0.63952252  0.4010951 -0.06085861\n[2,] -0.68654221  1.0000000  0.69841530 -0.5727418 -0.11402160\n[3,] -0.63952252  0.6984153  1.00000000 -0.1538589 -0.09932185\n[4,]  0.40109505 -0.5727418 -0.15385892  1.0000000  0.17549591\n[5,] -0.06085861 -0.1140216 -0.09932185  0.1754959  1.00000000\n\nmulcor &lt;- sqrt(t(R12) %*% solve(R22) %*% R12)\nmulcor\n\n          [,1]\n[1,] 0.8406753\n\n\n참고로 표본 다중상관계수의 제곱값은 는 \\(X_1\\)을 종속변수, \\(\\pmb X_*\\)을 독립변수로 선형회귀직선(linear regression)을 적합하였을 경우 결정계수(coefficient of determination) \\(R^2\\) 와 같다.\n\\[\n\\hat \\rho^2 (X_1, \\pmb a^t \\pmb X_*) = R^2\n\\] 아래 R 프로그램의 결과에서 확인할 수 있다.\n\nres &lt;- lm(Fertility~ Agriculture + Examination + Education + Catholic + Infant.Mortality,data=swiss)\nsummary(res)\n\n\nCall:\nlm(formula = Fertility ~ Agriculture + Examination + Education + \n    Catholic + Infant.Mortality, data = swiss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2743  -5.2617   0.5032   4.1198  15.3213 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      66.91518   10.70604   6.250 1.91e-07 ***\nAgriculture      -0.17211    0.07030  -2.448  0.01873 *  \nExamination      -0.25801    0.25388  -1.016  0.31546    \nEducation        -0.87094    0.18303  -4.758 2.43e-05 ***\nCatholic          0.10412    0.03526   2.953  0.00519 ** \nInfant.Mortality  1.07705    0.38172   2.822  0.00734 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.165 on 41 degrees of freedom\nMultiple R-squared:  0.7067,    Adjusted R-squared:  0.671 \nF-statistic: 19.76 on 5 and 41 DF,  p-value: 5.594e-10\n\nmulcor^2\n\n         [,1]\n[1,] 0.706735",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>정준상관분석</span>"
    ]
  },
  {
    "objectID": "qmd/multivar-cancor.html#정준상관계수",
    "href": "qmd/multivar-cancor.html#정준상관계수",
    "title": "7  정준상관분석",
    "section": "7.3 정준상관계수",
    "text": "7.3 정준상관계수\n\n7.3.1 정준상관계수의 정의\n이제 두 개의 확률벡터에 대한 상관관계를 나타내는 측도인 정준상관계수(Canonical Correlation Coeffcient)에 대하여 알아보자. 확률벡터 \\(\\pmb X\\)가 두 개의 확률벡터 \\(\\pmb X_1\\)과 \\(\\pmb X_2\\)로 나누어져 있다고 가정하자. 두 확률 벡터의 차원은 각각 \\(p\\)와 \\(q\\) 라고 하자 (편의상 \\(p \\le q\\)라고 가정한다.)\n\\[\n\\pmb X =\n\\begin{bmatrix}\n\\pmb X_1 \\\\\n\\pmb X_2 \\\\\n\\end{bmatrix}\n\\quad\nCov(\\pmb X) =\n\\begin{bmatrix}\n\\pmb \\Sigma_{11} & \\pmb \\Sigma_{12} \\\\\n\\pmb \\Sigma_{12}^t & \\pmb \\Sigma_{22}\n\\end{bmatrix}\n\\]\n앞 절에서와 유사한 방법으로 각 확률벡터의 선형조합으로 만들어진 두 개의 새로운 확률변수를 이용하여 정준상관계수를 다음과 같이 정의한다.\n\\[\n\\rho(\\pmb X_1, \\pmb X_2) = \\max_{\\pmb a, \\pmb b}~~ cor(\\pmb a^t \\pmb X_1, \\pmb b^t \\pmb X_2)\n\\tag{7.6}\\]\n앞절에서 공분산행렬을 구할 때 사용한 같은 방법을 적용하면\n\\[\n\\begin{aligned}\nCov(\\pmb a^t \\pmb X_1,\\pmb b^t \\pmb X_2) &=\nE [(\\pmb a^t \\pmb X_1 -E(\\pmb a^t \\pmb X_1)][\\pmb b^t \\pmb X_2-E(\\pmb b^t \\pmb X_2) ] \\\\\n  &= \\pmb a^t E [(\\pmb X_1 -E(\\pmb X_1)] [ \\pmb X_2-E(\\pmb X_2) ]^t \\pmb b \\\\\n  &= \\pmb a^t \\pmb \\Sigma_{12} \\pmb b \\\\\n\\end{aligned}\n\\]\n따라서\n\\[\ncor(\\pmb a^t \\pmb X_1,\\pmb b^t \\pmb X_2) = \\frac{\\pmb a^t \\pmb \\Sigma_{12}  \\pmb b } { \\sqrt{(\\pmb a^t \\pmb \\Sigma_{11} \\pmb a ) (\\pmb b^t \\pmb \\Sigma_{22} \\pmb b ) }} = (\\pmb a^t \\pmb \\Sigma_{11} \\pmb a )^{-1/2} (\\pmb a^t \\pmb \\Sigma_{12} \\pmb b)(\\pmb b^t \\pmb \\Sigma_{22} \\pmb b )^{-1/2}\n\\]\n이제 새로운 두 변수의 상관계수를 최대로 하는 벡터 \\(\\pmb a\\)와 \\(\\pmb b\\)를 찾으면 정준상관계수가 구해진다. 여기서 정준상관계수행렬(canonical correlation matrix) \\(\\pmb C\\)을 다음과 같이 정의하자.\n\\[\n\\pmb C = \\pmb \\Sigma_{11}^{-1/2} \\pmb \\Sigma_{12} \\pmb \\Sigma_{22}^{-1/2}\n\\tag{7.7}\\]\n또한 정준상관계수행렬 \\(\\pmb C\\) 의 Sigular Value Decomposition(SVD)를 다음과 같이 고려하자 (SVD 에 대한 자세한 내용은 섹션 C.4 를 참조)\n\\[\n\\pmb C = \\pmb U \\pmb S \\pmb V^t\n\\tag{7.8}\\]\n여기서 \\(\\pmb U\\)와 \\(\\pmb V\\) 는 각각 차원이 \\(p \\times  p\\), \\(q \\times q\\)인 정규직교행렬(orthonormal matrix)이고 \\(\\pmb S\\)는 \\(p \\times q\\) 행렬로 대각원소는 \\(\\pmb C \\pmb C^t\\)의 고유값 \\(\\lambda_i\\)의 제곱근을 대각원소로 하고 비대각원소는 0이다.\n\\[\n\\pmb U^t \\pmb U = \\pmb I_p ,\\quad \\pmb V^t \\pmb V = \\pmb I_q\n\\] 만약 \\(\\pmb u_1, \\pmb u_2, \\dots ,\\pmb u_p\\)와 \\(\\pmb v_1, \\pmb v_2, \\dots ,\\pmb v_q\\)를 각각 \\(\\pmb U\\)와 \\(\\pmb V\\)의 정규직교벡터라고 놓으면 다음과 같이 표시할 수 있다.\n\\[\n\\pmb C = \\pmb U \\pmb S \\pmb V^t  = [ \\pmb u_1~ \\pmb u_2~ \\dots ~\\pmb u_p]\n\\begin{bmatrix}\n\\sqrt{\\lambda_1}   & 0 & \\dots & 0 & 0 & \\dots & 0\\\\\n0 & \\sqrt{\\lambda_2}  & \\dots & 0 & 0 & \\dots & 0\\\\\n0 & 0  & \\dots & 0 & 0 & \\dots & 0\\\\\n0 & 0  & \\dots & \\sqrt{\\lambda_p} & 0 & \\dots & 0\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\pmb v_1^t \\\\\n\\pmb v_2^t \\\\\n\\vdots \\\\\n\\pmb v_q^t \\\\\n\\end{bmatrix}\n\\]\n여기서 참고로 \\(\\pmb u_1, \\pmb u_2, \\dots ,\\pmb u_p\\)는 \\(\\pmb C \\pmb C^t\\)의 고유벡터이고 \\(\\pmb v_1, \\pmb v_2, \\dots ,\\pmb v_q\\)는 \\(\\pmb C^t \\pmb C\\)의 고유벡터이다. 또한 두 행렬 \\(\\pmb C \\pmb C^t\\)와 \\(\\pmb C^t \\pmb C\\)는 0이 아닌 고유값이 \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p\\)로 같다.\n\\[\n\\pmb C \\pmb C^t =  \\pmb \\Sigma_{11}^{-1/2} \\pmb \\Sigma_{12} \\pmb \\Sigma_{22}^{-1} \\pmb \\Sigma_{12}^t \\pmb \\Sigma_{11}^{-1/2} \\quad  \n\\pmb C^t \\pmb C =  \\pmb \\Sigma_{22}^{-1/2} \\pmb \\Sigma_{12}^t \\pmb \\Sigma_{11}^{-1} \\pmb \\Sigma_{12} \\pmb \\Sigma_{22}^{-1/2}\n\\] SVD 분해에 대한 자세한 내용은 섹션 C.4 를 참조하자.\n\n\n7.3.2 정준상관계수의 유도\n다시 정준상관계수의 정의에 따른 벡터 \\(\\pmb a\\)와 \\(\\pmb b\\)를 찾는 문제로 돌아가서 두 벡터 \\(\\pmb X_1\\)과 \\(\\pmb X_2\\)을 표준화한 후에 선형조합을 고려한다.\n\\[\nZ = \\pmb a^t \\pmb \\Sigma_{11}^{-1/2} (\\pmb X_1 - \\pmb \\mu_1), \\quad\nW = \\pmb b^t \\pmb \\Sigma_{22}^{-1/2}(\\pmb X_2-\\pmb \\mu_2)\n\\]\n여기서 상관계수는 상수를 더하거나 빼도 변함이 없으므로 평균벡터를 빼는것은 영향이 없다. 또한 두 벡터 \\(\\pmb a\\)와 \\(\\pmb b\\) 대신에 \\(\\pmb \\Sigma_{11}^{-1/2} \\pmb a\\)와 \\(\\pmb \\Sigma_{22}^{-1/2} \\pmb b\\)를 고려해도 어차피 두 개의 임의의 벡터이므로 정준상관계수의 정의에는 영향을 미치지 않는다.\n\\[\n\\begin{aligned}\n\\rho(\\pmb X_1, \\pmb X_2) &=\n\\max_{\\pmb a, \\pmb b}~~ cor[\\pmb a^t \\pmb \\Sigma_{11}^{-1/2} (\\pmb X_1 - \\pmb \\mu_1),  \\pmb b^t \\pmb \\Sigma_{22}^{-1/2}(\\pmb X_2-\\pmb \\mu_2) ]\\\\\n&= \\max_{\\pmb a_*, \\pmb b_*}~~ cor(\\pmb a_*^t \\pmb X_1, \\pmb b_*^t \\pmb X_2)\n\\end{aligned}\n\\]\n두 확률변수 \\(Z\\)와 \\(W\\)의 공분산과 분산은 다음과 같이 주어지고\n\\[\n\\begin{aligned}\nCov(Z,W) &= Cov(\\pmb a^t \\pmb \\Sigma_{11}^{-1/2} (\\pmb X_1 - \\pmb \\mu_1),  \\pmb b^t \\pmb \\Sigma_{22}^{-1/2}(\\pmb X_2-\\pmb \\mu_2) ) \\\\\n  &= \\pmb a^t \\pmb \\Sigma_{11}^{-1/2} \\pmb \\Sigma_{12} \\pmb \\Sigma_{22}^{-1/2} \\pmb b \\\\\n  &=   \\pmb a^t \\pmb C \\pmb b \\\\\nVar(Z) &=   \\pmb a^t \\pmb a \\\\\nVar(W) &=   \\pmb b^t \\pmb b\n\\end{aligned}\n\\]\n따라서\n\\[\ncor(Z,W) = \\frac{ \\pmb a^t \\pmb C \\pmb b } { \\sqrt{(\\pmb a^t \\pmb a)(\\pmb b^t \\pmb b)}}\n= \\frac{\\pmb a^t} {\\sqrt{(\\pmb a^t \\pmb a)}} \\pmb C  \\frac{\\pmb b} {\\sqrt{(\\pmb b^t \\pmb b)}}\n\\]\n위의 식에서 확률변수 \\(Z\\)와 \\(W\\)의 상관계수는 두 벡터 \\(\\pmb a/\\sqrt{\\pmb a^t \\pmb a}\\)와 \\(\\pmb b/\\sqrt{\\pmb b^t \\pmb b}\\)의 길이가 1이므로 두 벡터 \\(\\pmb a\\)와 \\(\\pmb b\\)의 길이를 1로 가정해도 무방하다 (\\(\\pmb a^t \\pmb a=\\pmb b^t \\pmb b=1\\)). 따라서 다음과 같이 표시할 수 있다.\n\\[\ncor(Z,W) =  \\pmb a^t \\pmb C \\pmb b, \\quad  \\pmb a^t \\pmb a=\\pmb b^t \\pmb b=1\n\\tag{7.9}\\]\n여기서 벡터공간의 알려진 사실을 이용한다. 두 벡터 \\(\\pmb a\\)와 \\(\\pmb b\\)는 각각 \\(p\\)와 \\(q\\)-차원 벡터이므로 다음과 같은 두 벡터 $$와 \\(\\pmb \\beta\\)가 존재하고 다음과 같이 표시할 수 있다.\n\\[\n\\begin{aligned}\n\\pmb a &= \\pmb U \\pmb \\alpha = \\alpha_1 \\pmb u_1 + \\alpha_2 \\pmb u_2 + \\dots + \\alpha_p \\pmb u_p \\\\\n\\pmb b &= \\pmb V \\pmb \\beta = \\beta_1 \\pmb v_1 + \\beta_2 \\pmb v_2 + \\dots + \\beta_q \\pmb v_q  \n\\end{aligned}\n\\tag{7.10}\\]\n여기서 \\(\\pmb \\alpha^t \\pmb \\alpha = \\pmb \\beta^t \\pmb \\beta =1\\).\n이제 식 7.9 에 식 7.10 를 대입하고 식 식 7.8 의 SVD를 이용하면\n\\[\n\\begin{aligned}\ncor(Z,W)  &= \\pmb a^t \\pmb C \\pmb b \\\\\n   &= \\pmb \\alpha^t \\pmb U^t \\pmb U \\pmb S \\pmb V^t \\pmb V \\pmb \\beta \\\\\n   &=  \\pmb \\alpha^t  \\pmb S \\pmb \\beta \\\\\n   &= \\sum_{i=1}^ p \\alpha_i \\beta_i \\sqrt{\\lambda_i} \\\\\n   &\\le (\\max_i \\sqrt{\\lambda_i} ) \\sum_{i=1}^ p \\alpha_i \\beta_i \\\\\n   & =  \\sqrt{\\lambda_1} \\sum_{i=1}^ p \\alpha_i \\beta_i\n\\end{aligned}\n\\]\n위의 식에서 \\(cor(Z,W)\\)가 상한값(upper bound)와 같아지려면 \\(\\pmb \\alpha\\)와 \\(\\pmb \\beta\\)의 계수는 다음과 같은 조건일 때이다.\n\\[\n\\alpha_1=1,~ \\alpha_2=\\dots=\\alpha_p=0, \\quad \\beta_1=1,~ \\beta_2=\\dots=\\beta_p=0\n\\]\n따라서 표준화된 두 벡터의 정준상관계수는 \\(\\pmb C \\pmb C^t\\)의 최대고유값의 제곱근이 되고 선형조합을 만드는 두 벡터 \\(\\pmb a\\) 와 \\(\\pmb b\\)는 각각 \\(\\pmb C \\pmb C^t\\) 와 \\(\\pmb C^t \\pmb C\\)의 첫번쨰 고유벡터가 된다.\n\\[\n\\begin{aligned}\n\\rho(\\pmb X_1,\\pmb X_2) &= Cov(Z,W) \\\\\n&= \\max_{\\pmb a, \\pmb b} ~~cor(\\pmb a^t \\pmb \\Sigma_{11}^{-1/2} (\\pmb X_1 - \\pmb \\mu_1)\n, \\pmb b^t \\pmb \\Sigma_{22}^{-1/2}(\\pmb X_2-\\pmb \\mu_2)) \\\\\n&= cor(\\pmb u_1^t \\pmb \\Sigma_{11}^{-1/2} (\\pmb X_1 - \\pmb \\mu_1)\n, \\pmb v_1^t \\pmb \\Sigma_{22}^{-1/2}(\\pmb X_2-\\pmb \\mu_2)) \\\\\n&= \\sqrt{\\lambda_1}\n\\end{aligned}\n\\]\n여기서 \\(\\pmb a =\\pmb u_1, ~ \\pmb b= \\pmb v_1\\)이다.\n정준상관계수를 정의하고 유도하는 과정을 보면 주성분분석과 유사하게 서로 공분산이 0인 확률변수들를 만들고 대응하는 두 변수들간의 상관계수를 큰 순서대로 만들 수 있다. 예를 들어 \\(Z_2= \\pmb u_2^t \\pmb \\Sigma_{11}^{-1/2} (\\pmb X_1 - \\pmb \\mu_1)\\)는 \\(Z_1=\\pmb u_1^t \\pmb \\Sigma_{11}^{-1/2} (\\pmb X_1 - \\pmb \\mu_1)\\)과 공분산이 0이고 또한 \\(W_2=\\pmb v_2^t \\pmb \\Sigma_{22}^{-1/2} (\\pmb X_2 - \\pmb \\mu_2)\\)는 \\(W_1=\\pmb v_1^t \\pmb \\Sigma_{22}^{-1/2} (\\pmb X_2 - \\pmb \\mu_2)\\)과 공분산이 0 이다 (정규분포인경우 독립이다). 더 나아가 \\(Z_2\\)와 \\(W_2\\)의 상관계수는 각각 \\(Z_1\\)과 \\(W_1\\)의 공분산이 0인 변수들중에서 최대의 상관계수 \\(\\sqrt{\\lambda_2}\\) 를 가지게 된다. 유사한 방법으로 \\(Z_i\\)와 \\(W_i\\)들을 정의할 수 있다.\n\n\n7.3.3 표본 정준상관계수\n이제 모집단이 아닌 표본이 주어졌을 경우 표본 정준상관계수를 구하는 방법은 다음과 같다. 이번에도 \\(p \\le q\\)를 가정하자.\n확률벡터 \\(\\pmb X\\)의 표본 상관계수행렬을 두 벡터 \\(\\pmb X_1\\), \\(\\pmb X_2\\)의 차원에 맞게 다음과 같이 표시하고 구한다.\n\\[\n\\pmb R =\n\\begin{bmatrix}\n\\pmb R_{11} & \\pmb R_{12} \\\\\n\\pmb R_{12}^t & \\pmb R_{22}\n\\end{bmatrix}\n\\]\n다음과 같은 두 행렬을 구한다.\n\\[\n\\pmb E_1 = \\pmb R_{11}^{-1} \\pmb R_{12}  \\pmb R_{22}^{-1} \\pmb R_{12}^t\n\\]\n\\[\n\\pmb E_2 = \\pmb R_{22}^{-1} \\pmb R_{12}^t  \\pmb R_{11}^{-1} \\pmb R_{12}\n\\]\n두 벡터 \\(\\pmb X_1\\), \\(\\pmb X_2\\)의 표본 정준상관계수는 \\(\\pmb E_1\\) 또는 \\(\\pmb E_2\\)의 최대 고유치의 제곱근이다. 참고로 \\(\\pmb E_1\\) 와 \\(\\pmb E_2\\)은 이 아닌 고유값이 같다.\nswiss 자료에서 출산율(Fertility)과 영아사망율(Infant.Mortality)을 하나의 벡터로 나머지 4개의 사회경제변수를 다른 벡터로 하여 표본 정준상관계수는 다음과 같이 구할 수 있다.\n\nn &lt;-dim(swiss)[1] \np &lt;- 2\nq &lt;- 4\nswiss0 &lt;- swiss[,c(1,6,2,3,4,5)] # 순서를 바꾸는 작업\nR &lt;- cor(swiss0)\nR11 &lt;- matrix(R[1:p,1:p],p,p)\nR22 &lt;- matrix(R[(p+1):(p+q),(p+1):(p+q)],q,q)\nR12 &lt;- matrix(R[1:p,(p+1):(p+q)],p,q)\nR11\n\n         [,1]     [,2]\n[1,] 1.000000 0.416556\n[2,] 0.416556 1.000000\n\nR22\n\n           [,1]       [,2]       [,3]       [,4]\n[1,]  1.0000000 -0.6865422 -0.6395225  0.4010951\n[2,] -0.6865422  1.0000000  0.6984153 -0.5727418\n[3,] -0.6395225  0.6984153  1.0000000 -0.1538589\n[4,]  0.4010951 -0.5727418 -0.1538589  1.0000000\n\nR12\n\n            [,1]       [,2]        [,3]      [,4]\n[1,]  0.35307918 -0.6458827 -0.66378886 0.4636847\n[2,] -0.06085861 -0.1140216 -0.09932185 0.1754959\n\nE1 &lt;-  solve(R11) %*% R12 %*% solve(R22) %*% t(R12)\nE2 &lt;-  solve(R22) %*% t(R12) %*% solve(R11) %*% R12\nE1.eigen &lt;- eigen(E1)\nE2.eigen &lt;- eigen(E2)\nrho &lt;- sqrt(E1.eigen$value[1])\nrho # sample CCA\n\n[1] 0.8142291\n\nsqrt(E2.eigen$value[1])\n\n[1] 0.8142291+0i\n\n\nR 패키지 CCA의 함수 cc 를 이용하면 표본 정준상관계수와 선형변환을 위한 벡터를 구할 수 있다.\n\nlibrary(CCA)\n\nX1 &lt;- swiss[,c(1,6)]\nX2 &lt;- swiss[,c(2,3,4,5)]\nres1 &lt;- cc(X1,X2)\nres1$cor # sample CCA\n\n[1] 0.8142291 0.2222637\n\nres1$xcoef # vectors for linear transformation for X1\n\n                        [,1]        [,2]\nFertility        -0.08456464 -0.02455185\nInfant.Mortality  0.05534823  0.37357101\n\nres1$ycoef # vectors for linear transformation for X2\n\n                   [,1]        [,2]\nAgriculture  0.01985292 -0.05136137\nExamination  0.02690124  0.02476760\nEducation    0.09414900 -0.03527373\nCatholic    -0.01164052  0.01793981",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>정준상관분석</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html",
    "href": "qmd/math_mat_basic.html",
    "title": "부록 A — 행렬의 기초",
    "section": "",
    "text": "A.1 벡터와 행렬\n이 장에서는 회귀분석의 이론 전개에 필요한 행렬 이론과 선형 대수의 기초에 대하여 알아볼 것이다.\n다음 \\(p\\)-차원 벡터(vector) 또는 열벡터(column vector) \\(\\pmb a\\) 는 \\(p\\)개의 원소 \\(a_1, a_2, \\dots, a_p\\) 를 하나의 열(column)에 배치한 형태를 가진 개체이다.\n\\[\n\\pmb a =\n\\begin{bmatrix}\na_1 \\\\\na_2 \\\\\n\\vdots \\\\\na_p\n\\end{bmatrix}\n\\tag{A.1}\\]\n차원이 \\(n \\times p\\) 인 행렬 \\(\\pmb A\\) 는 다음과 같이 \\(n\\)개의 행과 \\(p\\) 개의 열에 원소 \\(a_{ij}\\)를 다음과 같이 배치한 형태를 가진다.\n\\[\n\\pmb A =\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1p} \\\\\na_{21} & a_{22} & \\dots & a_{2p} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{n1} & a_{n2} & \\dots & a_{np}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#두-행렬의-덧셈",
    "href": "qmd/math_mat_basic.html#두-행렬의-덧셈",
    "title": "부록 A — 행렬의 기초",
    "section": "A.2 두 행렬의 덧셈",
    "text": "A.2 두 행렬의 덧셈\n두 행렬 \\(\\pmb A\\) 와 \\(\\pmb B\\) 를 더하는 규칙은 다음과 같다.\n\n두 행렬 \\(\\pmb A\\) 와 \\(\\pmb B\\) 는 행과 열의 갯수가 같아야 한다.\n\\(\\pmb A + \\pmb B = \\pmb C\\) 라고 하면, 덧셈의 결과로 만들어진 행렬 \\(\\pmb C\\)는 두 행렬과 같은 수의 행과 열을 가지면 각 원소는 다음과 같다.\n\n\\[ \\pmb A + \\pmb B = \\pmb C \\quad \\rightarrow \\quad c_{ij} = a_{ij} + b_{ij} \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#스칼라곱",
    "href": "qmd/math_mat_basic.html#스칼라곱",
    "title": "부록 A — 행렬의 기초",
    "section": "A.3 스칼라곱",
    "text": "A.3 스칼라곱\n임의의 실수 \\(\\lambda\\) (스칼라)가 주어졌을 때, \\(\\lambda\\) 와 행렬 \\(\\pmb A\\)의 스칼라곱(scalar product) 는 행렬의 모든 원소에 \\(\\lambda\\) 를 곱해준 행렬로 정의된다.\n예를 들어 \\(\\lambda=2\\), \\(\\pmb A \\in \\RR^{2\\times 3}\\) 인 경우\n\\[\n\\lambda \\pmb A =\n2\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n-1 & 0 & 2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 & 4 & 6 \\\\\n-2 & 0 & 4\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#벡터와-행렬의-곱셈",
    "href": "qmd/math_mat_basic.html#벡터와-행렬의-곱셈",
    "title": "부록 A — 행렬의 기초",
    "section": "A.4 벡터와 행렬의 곱셈",
    "text": "A.4 벡터와 행렬의 곱셈\n\\(n \\times p\\) 인 행렬 \\(\\pmb A\\) 와 \\(p\\)-차원 벡터(vector) \\(\\pmb b\\)는 다음과 같이 두 개의 서로 다른 형태로 나타낼 수 있다.\n\nA.4.1 행과 열의 내적\n먼저 행렬과 벡터의 곱셈은 행렬 \\(\\pmb A\\) 의 행벡터와 벡터 \\(\\pmb b\\) 의 내적(inner product)로 나타낼 수 있다.\n\\[\n\\begin{aligned}\n{\\pmb A} {\\pmb b} & =\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1p} \\\\\na_{21} & a_{22} & \\dots & a_{2p} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{n1} & a_{n2} & \\dots & a_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n{\\pmb r}^t_1 \\\\\n{\\pmb r}^t_2 \\\\\n\\vdots \\\\\n{\\pmb r}^t_n\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix}\n\\quad\n\\text{ where }\n{\\pmb r}^t_i =\n\\begin{bmatrix}\na_{i1} & a_{i2} & \\dots & a_{ip}\n\\end{bmatrix}  \\\\\n& =\n\\begin{bmatrix}\n{\\pmb r}^t_1 {\\pmb b} \\\\\n{\\pmb r}^t_2 {\\pmb b} \\\\\n\\vdots \\\\\n{\\pmb r}^t_n {\\pmb b}\n\\end{bmatrix}  \n=\n\\begin{bmatrix}\n\\sum_{j=1}^p a_{1j} b_j \\\\\n\\sum_{j=1}^p a_{2j} b_j \\\\\n\\vdots \\\\\n\\sum_{j=1}^p a_{nj} b_j\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n&lt;\\pmb r_1, \\pmb b&gt;  \\\\\n&lt;\\pmb r_2, \\pmb b&gt; \\\\\n\\vdots \\\\\n&lt;\\pmb r_n, \\pmb b&gt;\n\\end{bmatrix}\n\\end{aligned}\n\\]\n위에서 \\(&lt; \\pmb a, \\pmb b&gt;\\) 는 다음과 같은 두 벡터의 내적(inner product)을 의미한다.\n\\[ &lt; \\pmb a, \\pmb b&gt; = {\\pmb a}^t {\\pmb b} = \\sum_{i=1}^p a_i b_i \\]\n\n\nA.4.2 열벡터의 선형조합\n이제 행렬과 벡터의 곱셈을 행렬을 구성하는 열벡터들의 선형조합(linear combination)으로 나타낼 수 있다.\n\\[\n\\begin{aligned}\n{\\pmb A} {\\pmb b} & =\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1p} \\\\\na_{21} & a_{22} & \\dots & a_{2p} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{n1} & a_{n2} & \\dots & a_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n{\\pmb c}_1 & {\\pmb c}_2 & \\dots & {\\pmb c}_p\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_p\n\\end{bmatrix}\n\\quad\n\\text{ where }\n{\\pmb c}_j =\n\\begin{bmatrix}\na_{1j} \\\\\na_{2j} \\\\\n\\vdots \\\\\na_{nj}\n\\end{bmatrix} \\\\\n& =\nb_1\n\\begin{bmatrix}\na_{11} \\\\\na_{21} \\\\\n\\vdots \\\\\na_{n1}\n\\end{bmatrix}\n+\nb_2\n\\begin{bmatrix}\na_{12} \\\\\na_{22} \\\\\n\\vdots \\\\\na_{n2}\n\\end{bmatrix}\n+ \\cdots +\nb_p\n\\begin{bmatrix}\na_{1p} \\\\\na_{2p} \\\\\n\\vdots \\\\\na_{np}\n\\end{bmatrix}  \\\\\n& =\nb_1 {\\pmb c}_1 + b_2 {\\pmb c}_2 + \\cdots + b_p {\\pmb c}_p \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#행렬의-전치",
    "href": "qmd/math_mat_basic.html#행렬의-전치",
    "title": "부록 A — 행렬의 기초",
    "section": "A.5 행렬의 전치",
    "text": "A.5 행렬의 전치\n\\(\\pmb A^t\\)는 행렬의 전치(transpose)를 나타낸다. 행렬의 전치는 원소의 행과 열을 바꾸어 만든 행렬이다.\n\\[\n{\\pmb A}  =\n\\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1p} \\\\\na_{21} & a_{22} & \\dots & a_{2p} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{n1} & a_{n2} & \\dots & a_{np}\n\\end{bmatrix}\n= \\{ a_{ij} \\}_{ n \\times p}\n\\quad\n\\rightarrow\n\\quad\n{\\pmb A}^t  =\n\\begin{bmatrix}\na_{11} & a_{21} & \\dots & a_{n1} \\\\\na_{12} & a_{22} & \\dots & a_{n2} \\\\\n\\vdots & \\vdots &    & \\dots \\\\\na_{1p} & a_{2p} & \\dots & a_{np}\n\\end{bmatrix}\n= \\{ a_{ji} \\}_{ p \\times n}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#행렬의-곱셈",
    "href": "qmd/math_mat_basic.html#행렬의-곱셈",
    "title": "부록 A — 행렬의 기초",
    "section": "A.6 행렬의 곱셈",
    "text": "A.6 행렬의 곱셈\n먼저 두 행렬 \\(\\pmb A\\) 와 \\(\\pmb B\\) 의 곱셈\n\\[ \\pmb A \\times \\pmb B \\equiv \\pmb A \\pmb B \\]\n을 정의하려면 다음과 같은 조건이 만족되어야 한다.\n\n행렬 \\(\\pmb A\\) 의 열의 갯수와 행렬 \\(\\pmb B\\) 의 행의 갯수가 같아야 한다\n\n따라서 두 행렬의 곱셈은 순서를 바꾸면 정의 자체가 안될 수 있다.\n이제 두 행렬 \\(\\pmb A \\in \\RR^{m \\times n}\\) 와 \\(\\pmb B \\in \\RR^{n \\times k}\\)의 곱셈은 다음과 같이 정의된다.\n\\[ \\pmb A \\pmb B =  \\pmb C\\]\n행렬 \\(\\pmb C\\) 는 \\(m\\) 개의 행과 \\(k\\)개의 열로 구성된 행렬이며(\\(\\pmb C \\in \\RR^{m \\times k}\\)) 각 원소 \\(c_{ij}\\)는 다음과 같이 정의된다.\n\\[  c_{ij} = \\sum_{l=1}^n a_{il} b_{lk}, \\quad i=1,2,\\dots,m; j=1,2,\\dots,k \\]\n먼저 간단한 예제로 다음과 같은 두 개의 행렬의 곱을 생각해 보자.\n\\[\n\\pmb A \\pmb B =\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 1 \\\\\n-1 & 2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(1)(0) + (2)(-1) & (1)(1) + (2)(2) \\\\\n(3)(0) + (4)(-1) & (3)(1) + (4)(2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-2 & 5 \\\\\n-4 & 11\n\\end{bmatrix}\n\\]\n곱하는 순서를 바꾸어 계산해 보자.\n\\[\n\\pmb B \\pmb A =\n\\begin{bmatrix}\n0 & 1 \\\\\n-1 & 2\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(0)(1) + (1)(3) & (0)(2) + (1)(4) \\\\\n(-1)(1) + (2)(3) & (-1)(2) + (2)(4)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n3 & 4 \\\\\n5 & 6\n\\end{bmatrix}\n\\]\n위 두 결과를 보면 행렬의 곱셈에서는 교환법칙이 성립하지 않음을 알 수 있다.\n이제 차원이 다른 두 행렬의 곱셈을 살펴보자.\n\\[\n\\pmb A =\n\\begin{bmatrix}\n1 & 2 & 3\\\\\n3 & 2 & 1\n\\end{bmatrix},\n\\quad\n\\pmb B =\n\\begin{bmatrix}\n0 & 2 \\\\\n1 & -1 \\\\\n0 & 1\n\\end{bmatrix}\n\\]\n두 행렬의 곱셈은 다음과 같이 계산할 수 있다.\n\\[\n\\pmb A \\pmb B =\n\\begin{bmatrix}\n1 & 2 & 3\\\\\n3 & 2 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 2 \\\\\n1 & -1 \\\\\n0 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 & 3 \\\\\n2 & 5\n\\end{bmatrix}\n\\]\n두 행렬의 곱하는 순서를 바꾸면 차원이 전혀 다른 행렬이 얻어진다.\n\\[\n\\pmb B \\pmb A =\n\\begin{bmatrix}\n0 & 2 \\\\\n1 & -1 \\\\\n0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 & 3\\\\\n3 & 2 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n6 & 4 & 2 \\\\\n-2 & 0 & 2 \\\\\n3 & 2 & 1\n\\end{bmatrix}\n\\]\n행렬의 곱셈은 교환법칙이 성립하지 않는다.\n\\[  \\pmb A \\pmb B \\ne  \\pmb B \\pmb A \\tag{A.2}\\]\n\n\n\n\n\n\n주의\n\n\n\n교환법칙이 성립하지 않는다는 의미는 식 A.2 이 언제나 성립한다는 의미는 아니다. 아래와 같이 특별한 경우 교환법칙이 성립하는 경우도 있다.\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n1 & 3\n\\end{bmatrix}\n\\]\n\n\n\n행렬의 곱셈은 결합법칙과 배분법칙은 성립한다.\n\n\\[ (\\pmb A \\pmb B) \\pmb C = \\pmb A (\\pmb B \\pmb C) \\]\n\\[ (\\pmb A + \\pmb B) \\pmb C = \\pmb A \\pmb C +  \\pmb B \\pmb C \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#단위벡터와-항등행렬",
    "href": "qmd/math_mat_basic.html#단위벡터와-항등행렬",
    "title": "부록 A — 행렬의 기초",
    "section": "A.7 단위벡터와 항등행렬",
    "text": "A.7 단위벡터와 항등행렬\n\\(i\\)번째 단위벡터 \\(\\pmb e_i\\)를 정의하자. 단위벡터 \\(\\pmb e_i\\)는 \\(n\\)- 차원 벡터로서 \\(i\\)번째 원소만 1이고 나머지는 0인 벡터이다.\n\\[ \\pmb e_i =\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\  \n1 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{bmatrix}\n\\]\n즉 \\(n\\)-차원 항등행렬 \\(\\pmb I\\)는 n개의 단위벡터들을 모아놓은 것이다. 단위행렬은 대각원소가 1이고 나머지는 0인 정방행렬이다.\n\\[  \\pmb I = [ \\pmb e_1 ~~ \\pmb e_2 ~~ \\dots ~~ \\pmb e_n ] \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#대각합",
    "href": "qmd/math_mat_basic.html#대각합",
    "title": "부록 A — 행렬의 기초",
    "section": "A.8 대각합",
    "text": "A.8 대각합\n\\(\\pmb A = \\{ a_{ij} \\}\\)를 \\(n \\times n\\) 정방행렬(square matrix)인 경우, 행렬의 대각 원소(diagonal element)들의 합(trace)을 \\(tr(\\pmb A)\\)로 표시한다.\n\\[ tr(\\pmb A) = \\sum_{i=1}^n a_{ii} \\]\n두 행렬의 덧셈(뺄셈)에 대한 대각합에 대한 성질들은 다음과 같다.\n\\[ tr( {\\pmb A} \\pm {\\pmb B}) = tr({\\pmb A}) \\pm tr({\\pmb B}) \\]\n\n\n\n\n\n\n주의\n\n\n\n행렬의 곱셈은 일반적으로 교환법칙이 성립하지 않지만 대각합의 연산은 교환법칙이 성립한다.\n\\[  tr(\\pmb A \\pmb B)  = tr( \\pmb B \\pmb A) \\]\n\n\n대각합은 교환법칙이 성립히기 떄문에 다음과 같은 성질이 성립한다.\n\\[\n\\operatorname{tr}(\\pmb {A} \\pmb {K} \\pmb {L})=\\operatorname{tr}(\\pmb {K} \\pmb {L} \\pmb {A})\n\\]\n벡터의 연산에서도 대각합의 교환법칙이 성립되어 다음과 같은 유용한 식이 성립한다.\n\\[\n\\operatorname{tr}\\left(\\pmb {x} \\pmb {y}^t \\right)=\\operatorname{tr}\\left(\\pmb {y}^t  \\pmb {x}\\right)=\\pmb {y}^t  \\pmb {x} \\in \\mathbb{R} .\n\\]\n대각합의 교환법칙때문에 어떤 행렬의 앞에 특정 행렬을 곱하고, 뒤에 역행렬을 곱해도 대각합은 변하지 않는다.\n\\[\n\\operatorname{tr}\\left(\\pmb {S}^{-1} \\pmb {A} \\pmb {S}\\right) = \\operatorname{tr}\\left(\\pmb {A} \\pmb {S} \\pmb {S}^{-1}\\right)=\\operatorname{tr}(\\pmb {A})\n\\]\n대각합에 대한 그 밖의 성질들은 다음과 같다.\n\n\\(\\operatorname{tr}(\\alpha \\pmb {A})=\\alpha \\operatorname{tr}(\\pmb {A}), \\alpha \\in \\mathbb{R}\\) for \\(\\pmb {A} \\in \\mathbb{R}^{n \\times n}\\)\n\\(\\operatorname{tr}\\left(\\pmb {I}_n\\right)=n\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#sec-mat-determ",
    "href": "qmd/math_mat_basic.html#sec-mat-determ",
    "title": "부록 A — 행렬의 기초",
    "section": "A.9 행렬식",
    "text": "A.9 행렬식\n\\(\\pmb A\\)의 행렬식(determinant)을 \\(det(\\pmb A)=|\\pmb A|\\)로 표기한다.\n이차원 행렬 \\(\\pmb A\\) 의 행렬식은 다음과 같이 계산한다.\n\\[\n\\operatorname{det}( \\pmb {A})=\\left|\\begin{array}{ll}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{array}\\right|=a_{11} a_{22}-a_{12} a_{21} .\n\\]\n만약 행렬 \\(\\pmb A\\)가 대각행렬(diagonal matrix)이면 \\(|\\pmb A|\\)는 행렬의 대각원소의 곱이다 (\\(| \\pmb A| =\\prod a_{ii}\\)).\n두 행렬의 곱의 행렬식은 각 행렬의 행렬식의 곱이다.\n\\[ |\\pmb A \\pmb B | = | \\pmb A| |\\pmb B| \\]\n행렬식에 대한 유용한 공식들은 다음과 같다.\n\n\\(|{\\pmb A}^t| = |{\\pmb A}|\\)\n\\(|c {\\pmb A}| = c^n |{\\pmb A}|\\)\n\n만약 행렬 \\(\\pmb A\\)가 다음과 같은 분할행렬(partitioned matrix) 의 형태를 가지면\n\\[\n\\pmb A =\n\\begin{bmatrix}\n{\\pmb A}_{11} & {\\pmb A}_{12} \\\\\n{\\pmb 0} & {\\pmb A}_{22}\n\\end{bmatrix}\n\\]\n행렬 \\(\\pmb A\\)의 행렬식은 다음과 같이 주어진다.\n\\[ |{\\pmb A}| = |{\\pmb A}_{11}| |{\\pmb A}_{22} | \\]\n다음과 같은 행렬식에 대한 공식도 유용하다. p-차원 행렬 \\(\\pmb A\\)가 역행렬이 존재하고 벡터 \\(\\pmb u\\) 와 \\(\\pmb v\\) 에 대하여\n\\[ | \\pmb A + \\pmb u \\pmb v^t | = | \\pmb A | (1 + \\pmb v^t \\pmb A^{-1} \\pmb u)  \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#역행렬",
    "href": "qmd/math_mat_basic.html#역행렬",
    "title": "부록 A — 행렬의 기초",
    "section": "A.10 역행렬",
    "text": "A.10 역행렬\n만약 정방행렬 \\(\\pmb A\\)가 다음과 같은 조건을 만족하면 정칙행렬(invertible matrix 또는 nonsingular matrix)이라고 부른다.\n\\[  \\pmb A \\pmb A^{-1} = \\pmb A^{-1} \\pmb A = \\pmb I \\] 이경우 \\(\\pmb A^{-1}\\) 를 행렬 \\(\\pmb A\\)의 역행렬(inverse matrix) 이라고 정의한다.\n역행렬이 존재할 조건은 행렬 \\(\\pmb A\\)의 행렬식이 0이 아니어야 한다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#직교행렬",
    "href": "qmd/math_mat_basic.html#직교행렬",
    "title": "부록 A — 행렬의 기초",
    "section": "A.11 직교행렬",
    "text": "A.11 직교행렬\n만약 정방행렬 \\(\\pmb P\\)가 다음과 같은 조건을 만족하면 직교행렬(orthogonal matrix)라고 부른다.\n\\[  \\pmb P \\pmb P^t = \\pmb P^t \\pmb P = \\pmb I \\]\n직교행렬의 정의에서 주의할 점은 $P P^t = I $ 와 \\(\\pmb P^t \\pmb P = \\pmb I\\) 이 모두 성립하해야 한다는 점이다.\n행렬 \\(\\pmb P\\) 의 역행렬은 \\(\\pmb P^t\\) 이다.\n\\[ \\pmb P^{-1} = \\pmb P^t\\]\n만약 \\(\\pmb P\\)가 직교행렬이면 다음과 같은 성질을 가진다.\n\n\\(| \\pmb P | = \\pm 1\\) , 왜냐하면 \\[  | \\pmb P \\pmb P^t | = | \\pmb P | |\\pmb P^t |  = | \\pmb P|^2 = |\\pmb I| =1 \\]\n임의의 정방행렬 \\(\\pmb A\\)에 대하여 다음이 성립한다. \\[ tr(\\pmb P \\pmb A \\pmb P^t) = tr(\\pmb A \\pmb P^t \\pmb P) = tr(\\pmb A) \\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#벡터의-선형독립",
    "href": "qmd/math_mat_basic.html#벡터의-선형독립",
    "title": "부록 A — 행렬의 기초",
    "section": "A.12 벡터의 선형독립",
    "text": "A.12 벡터의 선형독립\nn 개의 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 의 선형결합(또는 선형결합, linear combination)이란 각 벡터에 스칼라를 곱하여 더한 것들이다.\n만약 \\(r_1,r_2,\\dots, r_n\\) 가 임의의 실수일 때, 다음과 같은 형태의 선형식을 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\)의 선형결합(linear combination)이라고 한다:\n\\[ r_1 \\pmb v_1 + r_2 \\pmb v_2 + \\cdots + r_n \\pmb v_n   \\tag{A.3}\\]\n\n정의 A.1 (벡터의 선형독립과 선형종속) n 개의 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 가 있다고 하자. 만약 다음 식이 만약 모두 \\(0\\)인 \\(n\\)개의 스칼라 \\(r_1,r_2,\\dots,r_n\\)에 대해서만 성립하면 \\(n\\)개 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 들은 선형독립(linearly independent)라고 한다.\n\\[\nr_1 \\pmb v_1 + r_2 \\pmb v_2 + \\dots + r_n \\pmb v_n = \\pmb 0 \\quad \\Longleftrightarrow\nr_1 = r_2 = \\dots = r_n =0\n\\tag{A.4}\\]\n또한 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 가 선형독립이 아니면 선형종속(linear dependent)라고 한다. 벡터 \\(\\pmb v_1, ~~ \\pmb v_2, ~~\\dots ~~, \\pmb v_n\\) 가 선형종속이면 모두 0이 아닌 \\(r_1,r_2,\\dots,r_n\\) 이 존재하여 다음이 성립한다는 것이다.\n\\[\n\\exists~ r_1,r_2,\\dots,r_n \\in R \\quad \\text{ s.t. } (r_1,r_2,\\dots,r_n) \\ne \\pmb 0, \\\\ r_1\\pmb v_1 + r_2 \\pmb v_2 + \\dots + r_n \\pmb v_n = \\pmb 0\n\\tag{A.5}\\]\n\\(\\blacksquare\\)\n\n예를 들어 다음과 같이 주어진 3개의 3-차원 벡터들은 선형종속이다.\n\\[\n\\pmb v_1 =\n\\begin{bmatrix}\n1\\\\\n2\\\\\n3\n\\end{bmatrix},\n\\quad\n\\pmb v_2 =\n\\begin{bmatrix}\n1\\\\\n0\\\\\n1\n\\end{bmatrix},\n\\quad\n\\pmb v_3 =\n\\begin{bmatrix}\n3\\\\\n2\\\\\n5\n\\end{bmatrix}\n\\tag{A.6}\\]\n왜냐하면 다음과 같이 모두 0이 아닌 스칼라에 의해서 다음 식이 성립하기 떄문이다. 즉 벡터 \\(\\pmb v_3\\)는 \\(\\pmb v_2\\) 에 2를 곱하여 \\(\\pmb v_1\\)에 더한 값과 같다.\n\\[\n\\pmb v_3 = \\pmb v_1 + 2 \\pmb v_2 \\quad \\Longleftrightarrow \\quad    \\pmb v_1 + 2 \\pmb v_2 -\\pmb v_3 = 0\n\\]\n이제 다음과 같이 주어진 3개의 3-차원 벡터들은 선형독립이다. 즉 3개 벡터의 선형 조합이 0이 될 수 있도록 만드는 스칼라는 모두 0인 경우 밖에 없다.\n\\[\n\\pmb v_1 =\n\\begin{bmatrix}\n1\\\\\n2\\\\\n3\n\\end{bmatrix},\n\\quad\n\\pmb v_2 =\n\\begin{bmatrix}\n1\\\\\n0\\\\\n1\n\\end{bmatrix},\n\\quad\n\\pmb v_3 =\n\\begin{bmatrix}\n3\\\\\n2\\\\\n4\n\\end{bmatrix}\n\\tag{A.7}\\]\n이제 다음과 같이 주어진 4개의 3-차원 벡터들은 선형종속이다.\n\\[\n\\pmb v_1 =\n\\begin{bmatrix}\n1\\\\\n2\\\\\n3\n\\end{bmatrix},\n\\quad\n\\pmb v_2 =\n\\begin{bmatrix}\n1\\\\\n0\\\\\n1\n\\end{bmatrix},\n\\quad\n\\pmb v_3 =\n\\begin{bmatrix}\n3\\\\\n2\\\\\n4\n\\end{bmatrix}\n\\quad\n\\pmb v_4 =\n\\begin{bmatrix}\n0\\\\\n0\\\\\n1\n\\end{bmatrix}\n\\tag{A.8}\\]\n\\(\\pmb v_3\\) 가 다음과 같이 다른 벡터의 선형결합으로 나타난는 것을 보여준다.\n\\[ \\pmb v_3 = \\begin{bmatrix}\n3\\\\\n2\\\\\n4\n\\end{bmatrix}\n= (1)\\pmb v_1 +  (2)\\pmb v_2 +  (-1)\\pmb v_4\n= (1)\n\\begin{bmatrix}\n1\\\\\n2\\\\\n3\n\\end{bmatrix}\n+\n(2)\n\\begin{bmatrix}\n1\\\\\n0\\\\\n1\n\\end{bmatrix}\n+\n(-1)\\begin{bmatrix}\n0\\\\\n0\\\\\n1\n\\end{bmatrix}\n\\]\n식 A.8 와 같이 3차원 벡터가 4개인 경우 벡터의 값에 관계없이 선형종속으로 나타난다. n-차원 실수공간 \\(R^n\\)에서 임의의 \\(n+1\\) 개의 벡터는 항상 선형종속이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_mat_basic.html#우드베리-공식",
    "href": "qmd/math_mat_basic.html#우드베리-공식",
    "title": "부록 A — 행렬의 기초",
    "section": "A.13 우드베리 공식",
    "text": "A.13 우드베리 공식\n다음은 우드베리공식(Woodbury formula) 과 파생된 유용한 공식들이다.\n\\[\n(\\pmb A+\\pmb U\\pmb C\\pmb V)^{-1} = \\pmb A^{-1}-\\pmb A^{-1} \\pmb U (\\pmb C^{-1} + \\pmb V \\pmb A^{-1}\\pmb U)^{-1} \\pmb V \\pmb A^{-1}\n\\]\n\\[\n(\\pmb I+\\pmb U \\pmb C\\pmb V)^{-1} = \\pmb I - \\pmb U (\\pmb C^{-1} + \\pmb V \\pmb U)^{-1} \\pmb V\n\\]\n\\[ (\\pmb A+\\pmb u\\pmb v^t)^{-1} = \\pmb A^{-1} - \\frac{ \\pmb A^{-1} \\pmb u\\pmb v^t \\pmb A^{-1}}{1+\\pmb v^t \\pmb A^{-1}\\pmb u}  \\tag{A.9}\\]\n\\[\n(a \\pmb I_n + b \\pmb 1_n \\pmb 1_n^t)^{-1} = \\frac{1}{a} \\left [ \\pmb I_n - \\frac{b}{a+nb} \\pmb 1 \\pmb 1^t \\right ]\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>행렬의 기초</span>"
    ]
  },
  {
    "objectID": "qmd/math_eigen_value.html",
    "href": "qmd/math_eigen_value.html",
    "title": "부록 B — 고유값과 고유벡터",
    "section": "",
    "text": "B.1 특성다항식\n특성다항식(Characteristic polynomial)은 다음과 같이 정의된다\n실수 \\(\\lambda \\in \\mathbb{R}\\) 와 정방행렬(square matrix) \\(\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}\\) 에 대하여\n\\[\n\\begin{aligned}\np_{\\boldsymbol{A}}(\\lambda) & :=\\operatorname{det}(\\boldsymbol{A}-\\lambda \\boldsymbol{I}) \\\\\n& =c_0+c_1 \\lambda+c_2 \\lambda^2+\\cdots+c_{n-1} \\lambda^{n-1}+(-1)^n \\lambda^n,\n\\end{aligned}\n\\tag{B.1}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>고유값과 고유벡터</span>"
    ]
  },
  {
    "objectID": "qmd/math_eigen_value.html#고유값과-고유벡터",
    "href": "qmd/math_eigen_value.html#고유값과-고유벡터",
    "title": "부록 B — 고유값과 고유벡터",
    "section": "B.2 고유값과 고유벡터",
    "text": "B.2 고유값과 고유벡터\n\nB.2.1 정의\n\\(n\\)-차원 정방행렬 \\(\\pmb A\\) 이 있을 때, 다음 식을 만족하는 \\(\\lambda\\) 와 벡터 \\(\\pmb x\\)가 존재하면 \\(\\lambda\\) 를 행렬 \\(\\pmb A\\) 의 고유값(eigenvalue), \\(\\pmb x\\) 를 행렬 \\(\\pmb A\\) 의 고유벡터(eigenvector)라고 한다 (부교재 definition 4.6)\n\\[ \\pmb A \\pmb x = \\lambda \\pmb x  \\tag{B.2}\\]\n\n고유벡터는 유일하지 않다. 즉, 벡터 \\(\\pmb x\\) 가 고유벡터이면 \\(c \\pmb x\\) 도 고유벡터이다.\n\n\\[ \\pmb A (c \\pmb x) = c \\pmb A \\pmb x = c \\lambda \\pmb x = \\lambda (c \\pmb x) \\]\n\n\nB.2.2 계산\n다음 4개의 문장은 동치이다\n\n\\(\\lambda\\) 는 행렬 \\(\\pmb A\\) 의 고유값이다.\n방정식 \\((\\pmb A - \\lambda \\pmb I)\\pmb x = \\pmb 0\\) 은 영벡터이외의 해 \\(\\pmb x\\) 를 가진다(nontrivial solution)\n행렬 \\(\\pmb A - \\lambda \\pmb I\\) 의 행렬식이 0 이다.\n\n\\[ \\operatorname{det}(\\pmb A - \\lambda \\pmb I) = 0  \\tag{B.3}\\]\n\n행렬 \\(\\pmb A - \\lambda \\pmb I\\) 의 rank가 \\(n\\) 보다 작다.\n\n위에서 행렬식이 0 인 방정식 식 B.3 을 푸는 것은 특성방정식 식 B.1 이 0 인 방정식을 을 푸는 것과 동일하다.\n\n예제 B.1 다음과 같은 \\(2 \\times 2\\) 행렬의 고유값과 고유행렬을 구해보자.\n\\[\n\\pmb{A}=\n\\begin{bmatrix}\n4 & 2 \\\\\n1 & 3\n\\end{bmatrix}\n\\] 다음과 같이 행렬 \\(\\pmb A\\) 에 대한 특성다항식을 이용하여 고유값을 구할 수 있다.\n\\[\n\\begin{aligned}\np_{\\pmb{A}}(\\lambda) & =\\operatorname{det}(\\pmb{A}-\\lambda \\pmb{I}) \\\\\n& =\\operatorname{det}\\left(\\left[\\begin{array}{ll}\n4 & 2 \\\\\n1 & 3\n\\end{array}\\right]-\\left[\\begin{array}{cc}\n\\lambda & 0 \\\\\n0 & \\lambda\n\\end{array}\\right]\\right)=\\left|\\begin{array}{cc}\n4-\\lambda & 2 \\\\\n1 & 3-\\lambda\n\\end{array}\\right| \\\\\n& =(4-\\lambda)(3-\\lambda)-(2)(1) \\\\\n& = (2-\\lambda)(5-\\lambda)\n\\end{aligned}\n\\]\n위의 방정식에서 \\(\\lambda\\)에 대한 다항식 \\(p_{\\pmb{A}}(\\lambda)=0\\) 의 근 을 구하면 고유값을 구할 수 있다. 따라서 행렬 \\(\\pmb A\\) 의 고유값은 \\(\\lambda_1=2\\) 와 \\(\\lambda_2=5\\) 이다.\n이제 각각의 고유값에 대한 고유행렬을 다음과 같이 고유벡터의 정의 식 B.2 에 의하여 구해보자.\n\\[\n\\begin{bmatrix}\n4 & 2 \\\\\n1 & 3\n\\end{bmatrix}\n\\pmb{x} =\n\\lambda \\pmb{x}\n\\quad \\rightarrow\n\\quad\n\\begin{bmatrix}\n4-\\lambda & 2 \\\\\n1 & 3-\\lambda\n\\end{bmatrix}\n\\pmb{x}=\\pmb{0}\n\\] 먼저, 고유값 \\(\\lambda_2 = 5\\) 고유행렬은 다음과 같이 정의된다.\n\\[\n\\begin{bmatrix}\n4-5 & 2 \\\\\n1 & 3-5\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-1 & 2 \\\\\n1 & -2\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}\n=\\pmb{0}\n\\quad \\rightarrow\nx_1 - 2x_2 = 0\n\\] 이제 위의 방정식을 만족하는 고유벡터는 다음과 같이 구할 수 있다.\n\\[\n\\pmb x_2 =\n\\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix}\n\\] 유의할 점은 고유벡터는 방정식을 만족하는 무수히 많은 벡터 중에 하나의 예일 뿐이다. 예를 들어 길이가 1 인 단위벡터(unit vector)인 고유벡터를 구하고 싶다면 위의 벡터를 길이가 1인 단위벡터로 바꾸면 된다.\n\\[\n\\pmb x_2 =\n\\begin{bmatrix}\n2/\\sqrt{5} \\\\\n1/\\sqrt{5}\n\\end{bmatrix}\n\\]\n또한 \\(\\lambda_1=2\\) 에 대한 고유벡터는 다음과 같이 단위벡터로 구할 수 있다.\n\\[\n\\left[\\begin{array}{cc}\n4-2 & 2 \\\\\n1 & 3-2\n\\end{array}\\right] \\pmb{x}=\\left[\\begin{array}{ll}\n2 & 2 \\\\\n1 & 1\n\\end{array}\\right] \\pmb{x}=\\pmb{0}\n\\quad \\rightarrow\nx_1 + x_2 = 0\n\\] \\[\n\\pmb x_1 =\n\\begin{bmatrix}\n1/\\sqrt{2} \\\\\n-1/\\sqrt{2}\n\\end{bmatrix}\n\\] \\(\\blacksquare\\)\n\n\n\nB.2.3 중복도와 고유공간\n\n대수적 중복도(algebraic multiplicity) 는 특성다항식 식 B.1 이 0인 방정식을 푸는 경우 다항식에서 고유값이 중근(multiple root)의 해로 나타나는 차수를 의미한다.\n기하적 중복도(geometric multiplicity) 는 고유값에 대응하는 고유벡터들 중 선형독립인 고유벡터들의 최대 개수를 의미한다.\n고유 공간(eigenspace)은 고유값에 대응하는 고유벡터들이 생성하는 벡터공간을 의미한다.\n\n\n\n예제 B.2 3차원 행렬 \\(\\pmb A\\) 가 다음과 같을 때\n\\[\\pmb A=\\left[\\begin{array}{ccc}0 & 0 & -2 \\\\ 1 & 2 & 1 \\\\ 1 & 0 & 3\\end{array}\\right]\\]\n행렬 \\(\\pmb A\\)의 특성다항식은 다음과 같다.\n\\[\n\\operatorname{det}(\\lambda \\pmb I -\\pmb A)=\n\\left|\\begin{array}{ccc}\n\\lambda & 0 & 2 \\\\\n-1 & \\lambda-2 & -1 \\\\\n-1 & 0 & \\lambda-3\n\\end{array}\\right|=(\\lambda-1)(\\lambda-2)^2\n\\] 참고로 특성방정식을 푸는 경우, 방정식 \\(\\operatorname{det}(\\pmb A - \\lambda \\pmb I)=0\\) 이나 \\(\\operatorname{det}(\\lambda \\pmb I -\\pmb A)= 0\\) 중 어느 것을 사용해도 상관없다.\n위의 식에서 3차원 행렬의 행렬식은 다음과 같이 구할 수 있다. 첫 번째 행을 기준으로 전개하면\n\\[\n\\begin{aligned}\n\\operatorname{det}(\\lambda \\pmb I -\\pmb A) & =  \n-\\lambda \\cdot\n\\begin{vmatrix}\n2 - \\lambda & 1 \\\\\n0 & 3 - \\lambda\n\\end{vmatrix}\n+ 0 \\cdot\n\\begin{vmatrix}\n1 & 1 \\\\\n1 & 3 - \\lambda\n\\end{vmatrix}\n+ (-2) \\cdot\n\\begin{vmatrix}\n1 & 2 - \\lambda \\\\\n1 & 0\n\\end{vmatrix}  \\\\\n& = -\\lambda[(2 - \\lambda)(3 - \\lambda) - 0]\n-2[(1)(0) - (1)(2 - \\lambda)] \\\\\n& = (\\lambda-1)(\\lambda-2)^2\n\\end{aligned}\n\\]\n첫번째 고유값은 \\(\\lambda_1=1\\) 이다. 고유벡터를 구하기 위해서는 다음과 같은 방정식을 풀면 된다.\n\\[ (\\lambda_1 \\pmb I -\\pmb A )\\pmb x = \\pmb 0  \\]\n위의 방정식을 풀면\n\\[\n(\\lambda_1 \\pmb I -\\pmb A )\\pmb x= (\\pmb I -\\pmb A )\\pmb x\n=\n\\begin{bmatrix}\n1 & 0 & 2 \\\\\n-1 & -1 & -1 \\\\\n-1 & 0 & -2\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\n아래와 같이 간단히 할 수 있으며\n\\[ x_1 = -2x_3, \\quad x_2 = x_3 \\] 다음과 같은 고유값과 고유벡터를 얻을 수 있다.\n\\[ \\lambda_1=1 \\quad \\rightarrow \\quad  {\\pmb x}_1=\\begin{bmatrix}-2 \\\\ 1 \\\\ 1\\end{bmatrix} \\]\n첫번째 고유값은 \\(\\lambda_1=1\\) 이며 대수적 중복도는 1이고 기하적 중복도도 1이다. 기하적 중복도도 1이란 의미는 고유벡터가 선형독립인 1개의 벡터로 이루어져 있다는 의미이다. 이 경우 고유공간 \\(E_1\\) 은 한 개의 고유벡터 \\(\\pmb x_1\\) 이 생성하는 부분공간을 의미한다.\n\\[\nE_1 = \\text{span}\\left\\{\\begin{bmatrix}-2 \\\\ 1 \\\\ 1\\end{bmatrix} \\right\\}\n\\]\n다음으로 두번째 고유값에 대한 방정식 \\((\\lambda_2 \\pmb I -\\pmb A )\\pmb x = \\pmb 0\\) 을 풀면 다음과 같다.\n\\[\n(\\lambda_2 \\pmb I -\\pmb A )\\pmb x= (2\\pmb I -\\pmb A )\\pmb x =\n\\begin{bmatrix}\n2 & 0 & 2 \\\\\n-1 & 0 & -1 \\\\\n-1 & 0 & -1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\n이 방정식은 아래와 같이 간단히 할 수 있으며\n\\[ x_1 = -x_3 \\] 주어진 방정식이 하나이기 떄문에 다음과 같이 서로 선형독립인 두 개의 고유벡터를 얻을 수 있다.\n\\[\n\\lambda_2=2\\quad \\rightarrow \\quad  {\\pmb x}_2=\\begin{bmatrix}-1 \\\\ 0 \\\\ 1\\end{bmatrix}\n\\quad {\\pmb x}_3=\\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix}\n\\]\n위에서 두번째 고유값은 \\(\\lambda_2=2\\) 이며 대수적 중복도는 2이다. 또한 선형독립인 2개의 고유벡터를 구할 수 있으므로 기하적 중복도는 2이다.\n이 경우 \\(E_2\\) 는 두 개의 고유벡터 \\(\\pmb x_2, \\pmb x_3\\) 가 생성하는 부분공간을 의미한다.\n\\[\nE_2 = \\text{span}\\left\\{\\begin{bmatrix}-1 \\\\ 0 \\\\ 1\\end{bmatrix}, \\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix}\\right\\}\n\\]\n\\(\\blacksquare\\)\n\n\n이제 대수적 중복도와 기하적 중복도가 다른 경우에 대한 예제를 들어보자.\n\n\n예제 B.3 3차원 행렬 \\(\\pmb A\\) 가 다음과 같을 때\n\\[\\pmb A=\\left[\\begin{array}{ccc}1 & 0 & 2 \\\\ -1 & 1 & 3 \\\\ 0 & 0 & 2\\end{array}\\right]\\]\n행렬 \\(\\pmb A\\)의 특성다항식은 다음과 같다.\n\\[\n\\operatorname{det}(\\lambda \\pmb I -\\pmb A)=\n\\left|\\begin{array}{ccc}\n\\lambda-1 & 0 & -2 \\\\\n1 & \\lambda-1 & -3  \\\\\n0 & 0 & \\lambda-2\n\\end{array}\\right|=(\\lambda-1)^2(\\lambda-2)\n\\] 첫번째 고유값은 \\(\\lambda_1=1\\) 이다. 고유벡터를 구하기 위해서는 다음과 같은 방정식을 풀면 된다.\n\\[ (\\lambda_1 \\pmb I -\\pmb A )\\pmb x = \\pmb 0  \\]\n위의 방정식을 풀면\n\\[\n(\\lambda_1 \\pmb I -\\pmb A )\\pmb x= (\\pmb I -\\pmb A )\\pmb x =\n\\begin{bmatrix}\n0 & 0 & -2 \\\\\n1 & 0 & -3 \\\\\n0 & 0 & -1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\n아래와 같이 간단히 할 수 있으며\n\\[ \\quad x_1 = x_3 =0 \\] 다음과 같은 하나의 고유벡터를 얻을 수 있다.\n\\[ \\lambda_1=1 \\quad \\rightarrow \\quad  x_1=\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\]\n첫번째 고유값은 \\(\\lambda_1=1\\) 이며 대수적 중복도는 2이지만 기하적 중복도는 1이다. 이 경우 고유공간 \\(E_1\\) 은 한 개의 고유벡터 \\(\\pmb x_1\\) 이 생성하는 부분공간을 의미한다.\n\\[\nE_1 = \\text{span}\\left\\{\\begin{bmatrix}0 \\\\ 1 \\\\ 0\\end{bmatrix} \\right\\}\n\\]\n다음으로 두번째 고유값에 대한 방정식 \\((\\lambda_2 \\pmb I -\\pmb A )\\pmb x = \\pmb 0\\) 을 풀면 다음과 같다.\n\\[\n(\\lambda_2 \\pmb I -\\pmb A )\\pmb x= (2\\pmb I -\\pmb A )\\pmb x =\n\\begin{bmatrix}\n1 & 0 & 2 \\\\\n1 & 1 & -3 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}\n\\]\n이 방정식은 아래와 같이 간단히 할 수 있으며\n\\[ x_1 = -2x_3, \\quad x_2=5x_3  \\] 다음과 같은 한 개의 고유벡터를 얻을 수 있다.\n\\[\n\\lambda_2=2\\quad \\rightarrow \\quad  x_2=\\begin{bmatrix}-2 \\\\ 5 \\\\ 1\\end{bmatrix}\n\\]\n위에서 두번째 고유값은 \\(\\lambda_2=2\\) 이며 대수적 중복도는 1이다. 또한 선형독립인 1개의 고유벡터를 구할 수 있으므로 기하적 중복도는 1이다.\n이 경우 \\(E_2\\) 는 한 개의 고유벡터 \\(\\pmb x_2\\) 가 생성하는 부분공간을 의미한다.\n\\[\nE_2 = \\text{span}\\left\\{\\begin{bmatrix}-2\\\\ 5 \\\\ 1\\end{bmatrix}\\right\\}\n\\]\n\\(\\blacksquare\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>고유값과 고유벡터</span>"
    ]
  },
  {
    "objectID": "qmd/math_eigen_value.html#대칭행렬의-대각화",
    "href": "qmd/math_eigen_value.html#대칭행렬의-대각화",
    "title": "부록 B — 고유값과 고유벡터",
    "section": "B.3 대칭행렬의 대각화",
    "text": "B.3 대칭행렬의 대각화\n\n예제 B.4 이제 대칭행렬에 대한 고유값과 고유행렬을 구해보자. 대칭행렬은 고유값이 실수이고 서로 다르며, 서로 직교하는 고유벡터를 가진다.\n\\[\n\\pmb{A}=\n\\tfrac{1}{2}\n\\begin{bmatrix}\n5 & -2 \\\\\n-2 & 5\n\\end{bmatrix}\n\\]\n행렬 \\(\\pmb{A}\\) 의 특성다항식은 다음과 같이 구할 수 있다.\n\\[\n\\begin{aligned}\n& \\operatorname{det}(\\pmb{A}-\\lambda \\pmb{I})=\\operatorname{det}\\left(\\left[\\begin{array}{cc}\n\\frac{5}{2}-\\lambda & -1 \\\\\n-1 & \\frac{5}{2}-\\lambda\n\\end{array}\\right]\\right) \\\\\n& =\\left(\\frac{5}{2}-\\lambda\\right)^2-1=\\lambda^2-5 \\lambda+\\frac{21}{4}=\\left(\\lambda-\\frac{7}{2}\\right)\\left(\\lambda-\\frac{3}{2}\\right)\n\\end{aligned}\n\\]\n따라서 행렬 \\(\\pmb{A}\\) 의 고유값은 각각 \\(\\lambda_1=\\frac{7}{2}\\) 과 \\(\\lambda_2=\\frac{3}{2}\\) 이며 대응하는 고유벡터 \\(\\pmb p_1\\) 과 \\(\\pmb p_2\\) 는 다음과 같이 구할 수 있다.\n\\[\n\\pmb{A} \\pmb{p}_1=\\frac{7}{2} \\pmb{p}_1, \\quad \\pmb{A} \\pmb{p}_2=\\frac{3}{2} \\pmb{p}_2\n\\] 위의 고유빅터에 대한 방정식을 풀어서 길이가 1인 고유벡터를 구하면 다음과 같다.\n\\[\n\\pmb{p}_1=\\frac{1}{\\sqrt{2}}\n\\begin{bmatrix}\n1 \\\\\n-1\n\\end{bmatrix},\n\\quad\n\\pmb{p}_2=\\frac{1}{\\sqrt{2}}\n\\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix}\n\\]\n이제 고유벡터는 서로 직교하는 단위벡터임을 알 수 있다.\n\\[ \\pmb{p}_1^t  \\pmb{p}_2 = 0 \\]\n이제 고유값과 고유벡터를 구했으니 대칭행렬 \\(\\pmb{A}\\) 를 대각화 해보자. 대칭행렬 \\(\\pmb{A}\\) 는 다음과 같이 대각화 할 수 있다. 먼저 두 고유벡터를 열벡터로 하는 행렬 \\(\\pmb{P}\\) 를 정의하자. 행렬 \\(\\pmb{P}\\) 는 서로 직교하는 벡터로 구성되었으므로 직교행렬이다.\n\\[\n\\pmb{P}=\\left[\\pmb{p}_1, \\pmb{p}_2\\right]=\\frac{1}{\\sqrt{2}}\n\\begin{bmatrix}\n1 & 1 \\\\\n-1 & 1\n\\end{bmatrix}\n\\quad \\rightarrow \\quad \\pmb{P} \\pmb{P}^t = \\pmb{P}^t \\pmb{P} = \\pmb{I}\n\\]\n이제 \\(\\pmb{P}\\) 의 역행렬은 \\(\\pmb{P}^t\\) 이므로 다음과 같이 대칭행렬 \\(\\pmb A\\)의 대각화를 유도할 수 있다. 이러한 대각화를 행렬의 스펙트럼분해(Spectral Decomposition)이라고 부른다.\n\\[\n\\pmb{P}^{-t} \\pmb{A} \\pmb{P}=\n\\begin{bmatrix}\n\\frac{7}{2} & 0 \\\\\n0 & \\frac{3}{2}\n\\end{bmatrix}\n=\\pmb{D} .\n\\tag{B.4}\\]\n위의 식은 다음과 같이 쓸수 있다.\n\\[\n\\underbrace{\\frac{1}{2}\n\\begin{bmatrix}\n5 & -2 \\\\\n-2 & 5\n\\end{bmatrix}}_{\\pmb{A}}\n=\\underbrace{\\frac{1}{\\sqrt{2}}\n\\begin{bmatrix}\n1 & 1 \\\\\n-1 & 1\n\\end{bmatrix}}_{\\pmb P}\n\\underbrace{\n\\begin{bmatrix}\n\\frac{7}{2} & 0 \\\\\n0 & \\frac{3}{2}\n\\end{bmatrix}}_{\\pmb{D}}\n\\underbrace{\\frac{1}{\\sqrt{2}}\n\\begin{bmatrix}\n1 & -1 \\\\\n1 & 1\n\\end{bmatrix}}_{\\pmb P^{t}}\n\\]\n또한 위의 식은 다음과 같이 나타낼 수 있다.\n\\[\n\\begin{aligned}\n{\\pmb A} & =\n\\frac{1}{2}\n\\begin{bmatrix}\n5 & -2 \\\\\n-2 & 5\n\\end{bmatrix} \\\\\n& = \\lambda_1 {\\pmb p_1} {\\pmb p_1^t} + \\lambda_2 {\\pmb p_2} {\\pmb p_2^t} \\\\\n& =\n\\tfrac{7}{4}\n\\begin{bmatrix}\n1 \\\\\n-1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & -1 \\\\\n\\end{bmatrix}\n+\n\\frac{3}{4}\n\\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 1 \\\\\n\\end{bmatrix} \\\\\n& =\n\\tfrac{7}{4}\n\\begin{bmatrix}\n1  & -1\\\\\n-1 & 1\n\\end{bmatrix}\n+\n\\frac{3}{4}\n\\begin{bmatrix}\n1  & 1\\\\\n1  & 1\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\\(\\blacksquare\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>고유값과 고유벡터</span>"
    ]
  },
  {
    "objectID": "qmd/math_matrix_decomp.html",
    "href": "qmd/math_matrix_decomp.html",
    "title": "부록 C — 행렬의 분해",
    "section": "",
    "text": "C.1 Gram–Schmidt 방법\n서로 독립인 \\(n\\)차원의 벡터들이 \\(p\\)개 있을떄 \\[ \\pmb  a_1, \\pmb  a_2, \\dots, \\pmb  a_p \\] 이들이 만드는 열공간을 \\(C\\) 라고 하자.\n\\[\n\\begin{aligned}\nC &= span \\{ \\pmb  a_1, \\pmb  a_2, \\dots, \\pmb  a_p \\} \\notag \\\\\n& = \\{~c_1 \\pmb  a_1 +c_2 \\pmb  a_2+\\dots+c_p \\pmb  a_p ~|~ \\text{ all possible  real values of } c_1,c_2, \\dots ,c_p ~\\}\n\\end{aligned}\n\\tag{C.1}\\]\n이제 우리는 위와 동일한 열공간 \\(C\\) 만드는 정규직교 벡터들을 찾는 방법을 알아보고자 한다.\n\\[ \\pmb  q_1, \\pmb  q_2, \\dots, \\pmb  q_p \\quad \\text{ where } \\pmb  q_i^t \\pmb  q_j = 0,~~  \\pmb  q_i^t \\pmb  q_i = 1 \\]\n그리고\n\\[\nC = span \\{ \\pmb  q_1, \\pmb  q_2, \\dots, \\pmb  q_p \\}  = span \\{ \\pmb  a_1, \\pmb  a_2, \\dots, \\pmb  a_p \\}\n\\tag{C.2}\\]\n이제 앞 절의 벡터의 사영에 대한 결과를 사용하여 다음과 같은 직교하는 \\(p\\) 개의 벡터들을 축차적으로 만들어 보자.\n\\[\n\\begin{aligned}\n\\tilde {\\pmb  q}_1 & = \\pmb  a_1 \\notag \\\\\n\\tilde {\\pmb  q}_2 & = \\pmb  a_2 - proj_{\\tilde {\\pmb  q}_1} (\\pmb  a_2) \\notag \\\\\n\\tilde {\\pmb  q}_3 & = \\pmb  a_3 - proj_{\\tilde {\\pmb  q}_1} (\\pmb  a_3)  -proj_{\\tilde {\\pmb  q}_2} (\\pmb  a_3) \\notag \\\\\n\\tilde {\\pmb  q}_4 & = \\pmb  a_4 - proj_{\\tilde {\\pmb  q}_1} (\\pmb  a_4)  -proj_{\\tilde {\\pmb  q}_2} (\\pmb  a_4) -proj_{\\tilde {\\pmb  q}_3} (\\pmb  a_4) \\notag \\\\\n& \\dots  \\notag \\\\\n\\tilde {\\pmb  q}_p &= \\pmb  a_p - \\sum_{k=1}^p proj_{\\tilde {\\pmb  q}_k} (\\pmb  a_p)\n\\end{aligned}\n\\tag{C.3}\\]\n축차적으로 만든 벡터들을 정규벡터로 만들면 원래의 벡터들 \\(\\pmb  a_1, \\pmb  a_2, \\dots, \\pmb  a_p\\)이 생성하는 동일한 열공간을 만드는 정규직교 벡터 \\(\\pmb  q_1, \\pmb  q_2, \\dots, \\pmb  q_p\\)를 만들 수 있다.\n\\[\n\\pmb  q_i = \\tilde {\\pmb  q}_i / \\norm{\\tilde {\\pmb  q}_i}, \\quad i=1,2,\\dots,p\n\\tag{C.4}\\]\nGram–Schmidt 방법으로 만든 벡터들이 직교하는 것은 다음과 같이 증명할 수 있다. 먼저 ?eq-proofortho 에 의하여 \\(\\tilde {\\pmb  q}_1\\) 과 \\(\\tilde {\\pmb  q}_2\\)는 직교한다. 이제 임의의 \\(i\\)에 대하여 \\(\\tilde {\\pmb  q}_1, \\tilde {\\pmb  q}_2, \\cdots, \\tilde {\\pmb  q}_{i-1}\\) 벡터들이 직교한다고 가정하자. 모든 \\(1 \\le j \\le i-1\\) 에 대하여\n\\[\n\\begin{aligned}\n\\tilde {\\pmb  q}^t_{j} \\tilde {\\pmb  q}_{i}  & =\n  \\tilde {\\pmb  q}^t_{j} \\left [ \\pmb  a_i - \\sum_{k=1}^{i-1} proj_{\\tilde {\\pmb  q}_k} (\\pmb  a_i)  \\right ]\\\\\n   & =  \\tilde {\\pmb  q}^t_{j} \\left [ \\pmb  a_i -proj_{\\tilde {\\pmb  q}_j} (\\pmb  a_i) \\right ]\n   -  \\left [ \\sum_{\\substack{1\\le k \\le i-1 \\\\ k \\ne j}} \\tilde {\\pmb  q}^t_{j} ~proj_{\\tilde {\\pmb  q}_k} (\\pmb  a_i)  \\right ]  \\\\\n   & = 0 + 0\n\\end{aligned}\n\\]\n위에서 마지막 단계의 직교성은 다음과 같은 사실로 부터 유도된다.\n\\[\n\\tilde {\\pmb  q}^t_{j}  proj_{\\tilde {\\pmb  q}_k} (\\pmb  a_i) =0 \\quad \\text{ for  } 1 \\le j,k \\le i-1 , k \\ne j\n\\]\n식 C.3 과 식 C.4 의 알고리즘을 Gram–Schmidt 방법이라고 부른다. 위의 두 식에 의한 알고리즘을 다음과 같은 사실을 이용하면 좀 더 간단한 방법의 알고리즘이 나온다.\n\\[ proj_{\\tilde {\\pmb  q}_k} (\\pmb  a_l) = \\frac{\\pmb  a_l^t \\tilde {\\pmb  q}_k} {\\tilde {\\pmb  q}_k^t \\tilde {\\pmb  q}_k} \\tilde {\\pmb  q}_k = \\frac{\\pmb  a_l^t \\tilde {\\pmb  q}_k} {\\norm{\\tilde {\\pmb  q}_k}^2} \\tilde {\\pmb  q}_k=  (\\pmb  a_l^t \\pmb  q_k) \\pmb  q_k \\]\n다음은 Gram–Schmidt 방법을 설명한 그림이다.\nGram–Schmidt 방법(출처:Introduction to Applied Linear Algebra by Boyd and Vandenberghe, 2019)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>행렬의 분해</span>"
    ]
  },
  {
    "objectID": "qmd/math_matrix_decomp.html#gramschmidt-방법",
    "href": "qmd/math_matrix_decomp.html#gramschmidt-방법",
    "title": "부록 C — 행렬의 분해",
    "section": "",
    "text": "\\(\\pmb  a_i -proj_{\\tilde {\\pmb  q}_j} (\\pmb  a_i)\\)는 \\(\\tilde {\\pmb  q}^t_{j}\\)와 직교한다.\n가정에 의하여 \\(\\tilde {\\pmb  q}_1, \\tilde {\\pmb  q}_2, \\cdots, \\tilde {\\pmb  q}_{i-1}\\) 는 직교하고 \\(proj_{\\tilde {\\pmb  q}_k }(\\pmb  a_i)\\) 는 \\(\\tilde {\\pmb  q}_{k}\\) 와 같은 방향을 가진다.\n\n\n\n\n\n\\(p\\)개의 벡터 \\(\\pmb  a_1, \\pmb  a_2, \\dots, \\pmb  a_p\\)에 대하여\nfor \\(i=1,2,\\dots,p\\)\n\n\\(\\tilde {\\pmb  q}_i = \\pmb  a_i - (\\pmb  q_1^t \\pmb  a_i) \\pmb  q_1 - \\dots - (\\pmb  q_{i-1}^t \\pmb  a_i) \\pmb  q_{i-1}\\) (직교화)\n\\(\\pmb  q_i = \\tilde {\\pmb  q}_i/ \\norm{\\pmb  q_i}\\) (정규화)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>행렬의 분해</span>"
    ]
  },
  {
    "objectID": "qmd/math_matrix_decomp.html#lu-분해",
    "href": "qmd/math_matrix_decomp.html#lu-분해",
    "title": "부록 C — 행렬의 분해",
    "section": "C.2 LU 분해",
    "text": "C.2 LU 분해\n정방행렬 \\(\\pmb  A\\)를 다음과 같이 하삼각행렬 \\(\\pmb  L\\)과 상삼각행렬 \\(\\pmb  U\\)의 곱으로 나타내는 것을 LU 분해라고 한다.\n\\[ \\pmb  A = \\pmb  L \\pmb  U \\]\n\n\n\n\n\nLU 분해\n\n\n\n\n이러한 LU 분해는 행렬 \\(\\pmb  A\\)에 행연산을 적용하여 쉽게 구할 수 있다. 예를 들어 위에서 고려한 \\(2 \\times 2\\) 행렬에 행연산을 적용하여 대각원소 아래를 0으로 만들면 LU 분해를 쉽게 유도할 수 있다.\n\\[\n\\begin{bmatrix}\n1 & 0\\\\\n-3 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4  \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 2\\\\\n0 & -2\n\\end{bmatrix}\n\\]\n따라서\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4  \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0\\\\\n3 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2\\\\\n0 & -2\n\\end{bmatrix}\n= \\pmb  L \\pmb  U\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>행렬의 분해</span>"
    ]
  },
  {
    "objectID": "qmd/math_matrix_decomp.html#qr-분해",
    "href": "qmd/math_matrix_decomp.html#qr-분해",
    "title": "부록 C — 행렬의 분해",
    "section": "C.3 QR 분해",
    "text": "C.3 QR 분해\n식 C.3 과 식 C.4 에 주어진 Gram–Schmidt 방법을 원래 벡터들 \\(\\pmb  a_1, \\pmb  a_2, \\dots, \\pmb  a_p\\)에 대하여 다시 다음과 같이 나타낼 수 있다.\n\\[\n\\begin{aligned}\n\\pmb  a_1 & = \\tilde {\\pmb  q}_1  \\\\\n   &= \\norm{\\tilde {\\pmb  q}_1} \\pmb  q_1 \\\\\n\\pmb  a_2 & = \\tilde {\\pmb  q}_2  + proj_{\\tilde {\\pmb  q}_1} (\\pmb  a_2) \\\\\n       & = \\tilde {\\pmb  q}_2 + \\frac{\\pmb  a^t_2 \\tilde {\\pmb  q}_1}{\\tilde {\\pmb  q}^t_1 \\tilde {\\pmb  q}_1} \\tilde {\\pmb  q}_1 \\\\\n       &=  (\\pmb  a^t_2 {\\pmb  q}_1) {\\pmb  q}_1 + \\norm{\\tilde {\\pmb  q}_2} {\\pmb  q}_2 \\\\\n\\pmb  a_3 & = \\tilde {\\pmb  q}_3  + proj_{\\tilde {\\pmb  q}_1} (\\pmb  a_3) + proj_{\\tilde {\\pmb  q}_2} (\\pmb  a_3) \\\\\n       & = \\tilde {\\pmb  q}_3 + \\frac{\\pmb  a^t_3 \\tilde {\\pmb  q}_1}{\\tilde {\\pmb  q}^t_1 \\tilde {\\pmb  q}_1} \\tilde {\\pmb  q}_1 +\\frac{\\pmb  a^t_3 \\tilde {\\pmb  q}_2}{\\tilde {\\pmb  q}^t_2 \\tilde {\\pmb  q}_2} \\tilde {\\pmb  q}_2 \\\\\n       &=  (\\pmb  a^t_3 {\\pmb  q}_1) {\\pmb  q}_1 + (\\pmb  a^t_3 {\\pmb  q}_2) {\\pmb  q}_2+ \\norm{\\tilde {\\pmb  q}_3} {\\pmb  q}_3 \\\\\n       & \\cdots \\\\\n\\pmb  a_p & = (\\pmb  a^t_p {\\pmb  q}_1) {\\pmb  q}_1 + (\\pmb  a^t_p {\\pmb  q}_2) {\\pmb  q}_2+ \\dots + (\\pmb  a^t_p {\\pmb  q}_{p-1}) {\\pmb  q}_{p-1} +\\norm{\\tilde {\\pmb  q}_p} {\\pmb  q}_p\n\\end{aligned}\n\\]\n즉 위의 축차식을 보면 원래 벡터 \\(\\pmb  a_i\\) 는 Gram–Schmidt 방법으로 구한 정규직교벡터 \\(\\pmb  q_1,\\pmb  q_2, \\dots, \\pmb  q_p\\) 의 선형 조합으로 나타낼 수 있다.\n이제 Gram–Schmidt 방법으로 구한 정규직교벡터들 \\(\\pmb  q_1, \\pmb  q_2, \\dots, \\pmb  q_p\\) 을 모아놓은 행렬을 \\(\\pmb  Q\\) 라고 하고 위에서 \\(\\pmb  a_i\\)들이 직교행렬의 선형조합으로 표시될때 계수들을 모아놓는 상삼각행렬을 \\(\\pmb  R\\) 이라고 하자. 그려면 다음과 같은 QR 분해가 주어진다.\n\\[\n\\pmb  A = \\pmb  Q \\pmb  R\n\\tag{C.5}\\]\n여기서\n\\[\n\\begin{aligned}\n\\pmb  Q & = [\\pmb  q_1~~ \\pmb  q_2 ~ \\dots ~\\pmb  q_p ], \\quad  \\pmb  Q^t \\pmb  Q  =\\pmb  I  \\\\\n& \\\\\n\\pmb  R & =\n\\begin{bmatrix}\n\\norm{\\tilde {\\pmb  q}_1} & \\pmb  a^t_2 {\\pmb  q}_1 & \\pmb  a^t_3 {\\pmb  q}_1 & \\dots & \\pmb  a^t_p {\\pmb  q}_1 \\\\\n0 & \\norm{\\tilde {\\pmb  q}_2} &  \\pmb  a^t_3 {\\pmb  q}_2 & \\dots & \\pmb  a^t_p {\\pmb  q}_2 \\\\\n0 & 0 & \\norm{\\tilde {\\pmb  q}_3}  & \\dots & \\pmb  a^t_p {\\pmb  q}_3 \\\\\n& & & \\dots & \\\\\n0 & 0 & 0 & \\dots & \\norm{\\tilde {\\pmb  q}_p}\n\\end{bmatrix}\n\\end{aligned}\n\\]\n이제 Gram–Schmidt 방법과 QR 분해를 실제 예제를 통하여 구해보자\n아래와 같이 4차원 벡터 3개가 있다.\n\\[\n\\pmb  a_1 =\n\\begin{bmatrix}\n-1 \\\\\n1 \\\\\n-1 \\\\\n1\n\\end{bmatrix}\n\\quad\n\\pmb  a_2 =\n\\begin{bmatrix}\n-1 \\\\\n3 \\\\\n-1 \\\\\n3\n\\end{bmatrix}\n\\quad\n\\pmb  a_3 =\n\\begin{bmatrix}\n1 \\\\\n3 \\\\\n5 \\\\\n7\n\\end{bmatrix}\n\\tag{C.6}\\]\n위의 벡터 \\(\\pmb  a_1 , \\pmb  a_2 , \\pmb  a_3\\)에 대하여 Gram–Schmidt 방법을 적용해보자.\n\n\\(i=1\\). 먼저 \\(\\norm{\\tilde {\\pmb  q}_1}= \\norm{\\pmb  a_1}=2\\)이므로 첫번째 벡터 \\(\\pmb  q_1\\)를 만든다.\n\n\\[\n\\pmb  q_1  = \\tilde {\\pmb  q}_1 / \\norm{\\tilde {\\pmb  q}_1} =\n\\begin{bmatrix}\n-1/2 \\\\\n1/2 \\\\\n-1/2 \\\\\n1/2\n\\end{bmatrix}\n\\]\n\n\\(i=2\\). 이제 두번째 직교벡터 \\(\\pmb  q_2\\)를 만들자. \\(\\pmb  q^t_1 \\pmb  a_2 =4\\)이므로\n\n\\[\n\\tilde {\\pmb  q_2} = \\pmb  a_2 -(\\pmb  q_1^t \\pmb  a_2) \\pmb  q_1 =\n\\begin{bmatrix}\n-1 \\\\\n3 \\\\\n-1 \\\\\n3\n\\end{bmatrix}\n-4\n\\begin{bmatrix}\n-1/2 \\\\\n1/2 \\\\\n-1/2 \\\\\n1/2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n1\n\\end{bmatrix}\n\\]\n그리고 \\(\\norm{\\tilde {\\pmb  q}_2} = 2\\)이므로\n\\[\n\\pmb  q_2  = \\tilde {\\pmb  q}_2/\\norm{\\tilde {\\pmb  q}_2}\n=\n\\begin{bmatrix}\n1/2 \\\\\n1/2 \\\\\n1/2 \\\\\n1/2\n\\end{bmatrix}\n\\]\n\n\\(i=3\\) 마지막으로 \\(\\pmb  q_1^t \\pmb  a_3 =2\\), \\(\\pmb  q_2^t \\pmb  a_3=8\\) 이므로\n\n\\[\n\\tilde {\\pmb  q_3} = \\pmb  a_3 -(\\pmb  q_1^t \\pmb  a_3) \\pmb  q_1 -(\\pmb  q_2^t \\pmb  a_3) \\pmb  q_2=\n\\begin{bmatrix}\n1 \\\\\n3 \\\\\n5 \\\\\n7\n\\end{bmatrix}\n-2\n\\begin{bmatrix}\n-1/2 \\\\\n1/2 \\\\\n-1/2 \\\\\n1/2\n\\end{bmatrix}\n-8\n\\begin{bmatrix}\n1/2 \\\\\n1/2 \\\\\n1/2 \\\\\n1/2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-2 \\\\\n-2\\\\\n2 \\\\\n2\n\\end{bmatrix}\n\\] 또한 \\(\\norm{\\tilde {\\pmb  q}_3} = 4\\)이므로\n\\[\n\\pmb  q_3  = \\tilde {\\pmb  q}_3/\\norm{\\tilde {\\pmb  q}_3}\n=\n\\begin{bmatrix}\n-1/2 \\\\\n-1/2 \\\\\n1/2 \\\\\n1/2\n\\end{bmatrix}\n\\]\n따라서 Gram–Schmidt 방법으로 만든 정규직교벡터는 다음과 같다.\n\\[\n\\pmb  q_1 =\n\\begin{bmatrix}\n-1/2 \\\\\n1/2 \\\\\n-1/2 \\\\\n1/2\n\\end{bmatrix}\n\\quad\n\\pmb  q_2 =\n\\begin{bmatrix}\n1/2 \\\\\n1/2 \\\\\n1/2 \\\\\n1/2\n\\end{bmatrix}\n\\quad\n\\pmb  q_3 =\n\\begin{bmatrix}\n-1/2 \\\\\n-1/2 \\\\\n1/2 \\\\\n1/2\n\\end{bmatrix}\n\\]\n이제 위에서 구한 Gram-Schmidt 방법으로 얻은 결과를 이용하여 QR 분해를 구해보자.\n식 C.6 에서 주어진 백터들을 열로 가진 행렬 \\(\\pmb  A\\)의 QR분해를 구해보자.\n\\[\n\\pmb  A =\n\\begin{bmatrix}\n-1 & -1 & 1 \\\\\n1 & 3 & 3 \\\\\n-1 & -1 & 5 \\\\\n1 & 3 & 7\n\\end{bmatrix}\n\\]\n앞의 예제에서 구한 직교벡터를 그대로 이용하면 \\(\\pmb  Q\\)는 쉽게 구해진다.\n\\[\n\\pmb  Q =\n\\begin{bmatrix}\n-1/2 & 1/2 & -1/2 \\\\\n1/2 & 1/2 & -1/2 \\\\\n-1/2 & 1/2 & 1/2 \\\\\n1/2 & 1/2 & 1/2\n\\end{bmatrix}\n\\]\n또한 식 C.5 에 주어진 공식을 이용하면 행렬 \\(\\pmb  R\\)은 다음과 같이 구할 수 있다.\n\\[\n\\pmb  R  =\n\\begin{bmatrix}\n\\norm{\\tilde {\\pmb  q}_1} & \\pmb  a^t_2 {\\pmb  q}_1 & \\pmb  a^t_3 {\\pmb  q}_1  \\\\\n0 & \\norm{\\tilde {\\pmb  q}_2} &  \\pmb  a^t_3 {\\pmb  q}_2  \\\\\n0 & 0 & \\norm{\\tilde {\\pmb  q}_3}  \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 & 4 & 2  \\\\\n0 & 2 &  8  \\\\\n0 & 0 & 4\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>행렬의 분해</span>"
    ]
  },
  {
    "objectID": "qmd/math_matrix_decomp.html#sec-matdecomp-svd",
    "href": "qmd/math_matrix_decomp.html#sec-matdecomp-svd",
    "title": "부록 C — 행렬의 분해",
    "section": "C.4 SVD 분해",
    "text": "C.4 SVD 분해\n\nC.4.1 특이값과 특이벡터\n고유값과 고유벡터는 정방행렬인 경우 정의되는 것으로서 행렬이 정방행렬이 아닌 경우에는 구할 수 없다. 이제 고유값과 유사한 성질을 가지는 특이값을 일반행렬에서 정의해보자.\n\\(\\pmb  A\\)가 \\(m \\times n\\) 일반행렬이라고 가정하고 그 계수 \\(r\\)이라고 하자 (\\(r(\\pmb  A)=r\\)). 이제 서로 직교하는 \\(n\\)-차원의 벡터들의 집합 \\(\\pmb  v_1, \\pmb  v_2, \\dots, \\pmb  v_n\\)과 다른 직교하는 \\(m\\)-차원의 벡터들의 집합 \\(\\pmb  u_1, \\pmb  u_2, \\dots, \\pmb  u_m\\)을 생각하자.\n행렬 \\(\\pmb  A\\)의 특이값(singular values) \\(\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r&gt;0\\)과 왼쪽 특이벡터(left singular vectors) \\(\\pmb  u_1, \\pmb  u_2, \\dots, \\pmb  u_m\\) 그리고 오른쪽 특이벡터(right singular vectors) \\(\\pmb  v_1, \\pmb  v_2, \\dots, \\pmb  v_n\\) 는 다음과 같은 성질을 만족한다.\n\\[\n\\pmb  A \\pmb  v_1 = \\sigma_1 \\pmb  u_1, \\quad \\pmb  A \\pmb  v_2 = \\sigma_2 \\pmb  u_2, \\quad \\dots \\quad\n\\pmb  A \\pmb  v_r = \\sigma_r \\pmb  u_r,  \\quad \\pmb  A \\pmb  v_{r+1} =  \\pmb  0 , \\quad \\dots,  \\quad\n\\pmb  A \\pmb  v_n = \\pmb  0\n\\tag{C.7}\\]\n\\(n \\times n\\) 정방행렬 \\(\\pmb  V\\)와 \\(m \\times m\\) 정방행렬 \\(\\pmb  U\\) 를 각각 서로 직교하는 정규벡터 \\(\\pmb  v_1, \\pmb  v_2, \\dots, \\pmb  v_n\\) 과 \\(\\pmb  u_1, \\pmb  u_2, \\dots, \\pmb  u_m\\) 으로 구성되는 직교행렬이라고 하자.\n\\[\n\\pmb  V = [\\pmb  v_1~ \\pmb  v_2~ \\dots~ \\pmb  v_n], \\quad\n\\pmb  U = [\\pmb  u_1 ~ \\pmb  u_2 ~ \\dots ~ \\pmb  u_m ]\n\\]\n식 C.7 에 나타난 관계를 행렬 \\(\\pmb  V\\)와 \\(\\pmb  U\\)로 나타내면 다음과 같이 표현할 수 있다.\n\\[\n\\pmb  A \\pmb  V =  \\pmb  U \\pmb  \\Sigma\n\\tag{C.8}\\]\n위에서 \\(m \\times n\\) 행렬 \\(\\pmb  \\Sigma\\)는 다음과 같은 형태를 가진다.\n\\[\n\\pmb  \\Sigma\n=\\begin{bmatrix}\n\\pmb  \\Sigma_r & \\pmb  0 \\\\\n\\pmb  0 & \\pmb  0\n\\end{bmatrix}, \\quad\n\\pmb  \\Sigma_r =\n\\begin{bmatrix}\n\\sigma_1 & & 0 & \\\\\n& \\sigma_2 & & \\\\\n& & \\ddots & \\\\\n& 0 & & \\sigma_r\n\\end{bmatrix}\n\\]\n\n\nC.4.2 SVD 분해\n이제 행렬 \\(\\pmb  V\\)가 직교행렬을 이용하면 다음과 같은 SVD 분해(singular value decomposition; 특이값 분해)을 정의할 수 있다.\n\\[\n\\underset{m \\times n}{ \\pmb  A}  = \\underset{m \\times m}{\\pmb  U} ~~\\underset{m \\times n}{\\pmb  \\Sigma}~~ \\underset{n \\times n}{\\pmb  V^t}\n\\tag{C.9}\\]\n위의 식 C.9 을 전개하면 다음과 같이 계수가 1인 행렬 \\(\\pmb  u_k \\pmb  v_k^t\\) 들의 선형조합으로 행렬 \\(\\pmb  A\\)를 나타낼 수 있다.\n\\[\n\\pmb  A = \\sigma_1 \\pmb  u_1 \\pmb  v_1^t + \\sigma_2 \\pmb  u_2 \\pmb  v_2^t + \\dots\n\\sigma_r \\pmb  u_r \\pmb  v_r^t\n\\tag{C.10}\\]\n또한 식 C.8 에서 \\(\\pmb  \\Sigma\\)에서 0이 되는 값을 제외하면 처음 \\(r\\)개의 요소들만 이루어진 부분으로만 축소된 SVD 분해를 구할 수 있다.\n\\[\n\\pmb  A \\pmb  V_r =  \\pmb  U_r \\pmb  \\Sigma_r,  \\quad\n\\pmb  A [ \\pmb  v_1~ \\pmb  v_2~ \\dots~ \\pmb  v_r] =\n[ \\pmb  u_1~ \\pmb  u_2~ \\dots~ \\pmb  u_r ]\n\\begin{bmatrix}\n\\sigma_1 & & 0 & \\\\\n& \\sigma_2 & & \\\\\n& & \\ddots & \\\\\n& 0 & & \\sigma_r\n\\end{bmatrix}\n\\tag{C.11}\\]\n위의 식 C.11 에서 주의할 점은 행렬 \\(\\pmb  V_r\\)과 \\(\\pmb  U_r\\)은 정방행렬이 아니고 직교행렬도 아니다. \\(\\pmb  V_r^t \\pmb  V_r =\\pmb  I\\) 와 \\(\\pmb  U^t_r \\pmb  U_r = \\pmb  I\\)이 성립하지만 일반적으로 \\(\\pmb  V_r \\pmb  V^t_r \\ne \\pmb  I\\), \\(\\pmb  U_r \\pmb  U^t_r \\ne \\pmb  I\\)이다.\n\n\nC.4.3 특이값과 특이벡터의 계산\n\\(m \\times n\\) 행렬 \\(\\pmb  A\\)의 SVD 분해 식 C.9 로 부터 행렬 \\(\\pmb  A^t \\pmb  A\\)와 \\(\\pmb  A \\pmb  A^t\\)를 다음과 같이 나타낼 수 있다.\n\\[\n\\begin{aligned}\n\\pmb  A^t \\pmb  A & = (\\pmb  V \\pmb  \\Sigma^t \\pmb  U^t)(\\pmb  U \\pmb  \\Sigma \\pmb  V^t)\n= \\pmb  V \\pmb  \\Sigma^t  \\pmb  \\Sigma \\pmb  V^t\n\\\\\n\\pmb  A \\pmb  A^t & = (\\pmb  U \\pmb  \\Sigma \\pmb  V^t) (\\pmb  V \\pmb  \\Sigma^t \\pmb  U^t)\n= \\pmb  U \\pmb  \\Sigma \\Sigma^t \\pmb  U^t  \n\\end{aligned}\n\\tag{C.12}\\]\n위에서 \\(\\pmb  A^t \\pmb  A\\)와 \\(\\pmb  A \\pmb  A^t\\)는 모두 대칭행렬이지만 서로 차원이 다르다. 또한 식 C.12 을 보면 두 행렬이 모두 \\(\\pmb  Q \\pmb  \\Lambda \\pmb  Q^t\\)의 형식으로 분해되는 것을 알 수 있다. 즉 다음과 같은 사실을 알 수 있다.\n\n\\(n \\times n\\) 비음정치행렬 \\(\\pmb  A^t \\pmb  A\\)의 고유벡터 행렬은 \\(\\pmb  V\\)이다.\n\\(m \\times m\\) 비음정치행렬 \\(\\pmb  A \\pmb  A^t\\)의 고유벡터 행렬은 \\(\\pmb  U\\)이다.\n행렬 \\(\\pmb  A^t \\pmb  A\\)와 \\(\\pmb  A \\pmb  A^t\\)의 0이 아닌 고유값은 \\(\\sigma_1^2, \\sigma_2^2,\\dots, \\sigma_r^2\\) 이다.\n\n따라서 다음과 같은 방법으로 특이값과 특이벡터를 계산할 수 있다. 위의 방법은 두 행렬 \\(\\pmb  A^t \\pmb  A\\)와 \\(\\pmb  A \\pmb  A^t\\)를 모두 구하지 않고 \\(\\pmb  A^t \\pmb  A\\)의 고유값과 고유벡터만으로 SVD 분해를 구하는 방법이다 (만약 행렬 \\(\\pmb  A\\)가 \\(100000 \\times 5\\)이라면 \\(\\pmb  A \\pmb  A^t\\)는 \\(100000 \\times 100000\\)이다!)\n먼저 \\(\\pmb  A^t \\pmb  A\\)의 고유벡터 \\(\\pmb  v_1, \\dots,\\pmb  v_r\\)을 다음과 같은 고유값과 고유벡터의 정의로 먼저 구한다.\n\\[\n\\pmb  A^t \\pmb  A \\pmb  v_k = \\lambda_k \\pmb  v_k= \\sigma^2_k \\pmb  v_k, \\quad k=1,2,\\dots,r\n\\tag{C.13}\\]\n다음으로 다음의 식으로 \\(\\pmb  u_1, \\dots,\\pmb  u_r\\) 를 구한다.\n\\[\n\\pmb  u_k = \\frac{\\pmb  A \\pmb  v_k}{\\sigma_k}, \\quad k=1,2,\\dots, r\n\\tag{C.14}\\]\n식 C.14 에서 다음과 같이 \\(\\pmb  u_k\\)가 행렬 \\(\\pmb  A \\pmb  A^t\\)의 고유벡터임을 확인할 수 있다.\n\\[\n\\pmb  A \\pmb  A^t \\pmb  u_k = \\pmb  A \\pmb  A^t \\left ( \\frac{\\pmb  A \\pmb  v_k}{\\sigma_k} \\right ) =\n\\pmb  A \\left ( \\frac{\\sigma^2_k \\pmb  v_k}{\\sigma_k} \\right) = \\sigma^2_k \\pmb  u_k\n\\]\n또한 식 C.13 에서 \\(\\pmb  v_k\\)는 정규직교벡터이므로 다음과 같이 \\(\\pmb  u_k\\)도 정규직교행려임을 보일 수 있다.\n\\[\n\\pmb  u^t_k \\pmb  u_l = \\left ( \\frac{\\pmb  A \\pmb  v_k}{\\sigma_k} \\right )^t\n\\left ( \\frac{\\pmb  A \\pmb  v_l}{\\sigma_l} \\right ) =\n\\frac{ \\pmb  v_k^t (\\pmb  A^t \\pmb  A \\pmb  v_l) }{\\sigma_k \\sigma_l} =\n\\frac{\\sigma_l}{\\sigma_k} \\pmb  v_k^t \\pmb  v_l  =\n\\begin{cases}\n1 & \\text{ if } k=l \\\\\n0 & \\text{ if } k \\ne l\n\\end{cases}\n\\]\n위에서 구한 \\(r\\)개의 \\(\\pmb  v_k\\)와 \\(\\pmb  u_k\\)외에 \\(n-r\\)과 \\(m-r\\) 개의 서로 직교하는 나머지 \\(\\pmb  v\\)와 \\(\\pmb  u\\)도 구할 수 있다.\n\n\nC.4.4 SVD 분해의 기하학적 의미\n다음은 SVD 분해의 기하학적 의미를 설명한 그림이다.\n\n\n\n\n\nSVD 분해의 기하학적 의미",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>행렬의 분해</span>"
    ]
  },
  {
    "objectID": "qmd/math_matrix_decomp.html#양정치행렬",
    "href": "qmd/math_matrix_decomp.html#양정치행렬",
    "title": "부록 C — 행렬의 분해",
    "section": "C.5 양정치행렬",
    "text": "C.5 양정치행렬\n\nC.5.1 이차형식\n\\(n\\)-차원 벡터 \\({\\pmb  x}^t=[x_1,x_2,\\dots,x_n]\\)과 대칭행렬 \\(\\pmb  A\\)에 대하여 이차형식(quadratic form)은 다음과 같이 정의된다.\n\\[\nQ_A(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x =\\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j\n\\tag{C.15}\\]\n이차형식의 정의에서 반드시 행렬 \\(\\pmb  A\\)를 대칭행렬로 정의하지 않아도 되지만 임의의 행렬에 대하여 이차형식의 값이 동일한 대칭행렬이 존재하기 때문에 정의에서 이차형식으로 국한하는 것이 일반적이다.\n\n정의 C.1 (양정치 행렬) 이차형식 \\(Q_A(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x\\)가 영벡터가 아닌 모든 벡터 \\(\\pmb  x\\)에 대하여 0 보다 크면, 즉\n\\[ \\pmb  x^t \\pmb  A \\pmb  x  &gt;0  \\quad \\text{ for all } \\quad \\pmb x \\in \\RR^n\\]\n\\(\\pmb  A\\)를 양정치(positive definite)라고 부른다.\n만약 이차형식 \\(Q_A(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x\\)가 영벡터가 아닌 모든 벡터 \\(\\pmb  x\\)에 대하여 0 보다 크거나 같다면\n\\[ \\pmb  x^t \\pmb  A \\pmb  x  \\ge 0 \\quad \\text{ for all } \\quad \\pmb x \\in \\RR^n\\]\n\\(\\pmb  A\\)를 양반정치(positive semi-definite)라고 부른다.\n\\(\\blacksquare\\)\n\n정칙행렬 \\(\\pmb  B\\)에 대하여 다음과 같은 선형변환을 고려하자.\n\\[   \\pmb  x = \\pmb  B \\pmb  y \\quad \\text{ or } \\quad \\pmb  y = \\pmb  {B}^{-1} \\pmb  x \\]\n벡터 \\(\\pmb  x\\)로 정의된 이차형식은 벡터 \\(\\pmb  y\\)의 형태로 다음과 같이 변환할 수 있다.\n\\[\nQ(\\pmb  x) = \\pmb  x^t \\pmb  A \\pmb  x = \\pmb  y^t \\pmb  B^t \\pmb  A \\pmb  B \\pmb  y =Q^*(\\pmb  y)\n\\]\n이차형식의 성질은 정칙 선형변환에서 유지된다. 즉 행렬 \\(\\pmb  A\\)가 양(반)정치 행렬이고 행렬 \\(\\pmb  B\\)가 정칙행렬이면 행렬 \\(\\pmb  B^t \\pmb  A \\pmb  B\\)도 양(반)정치 행렬이다.\n\n\nC.5.2 양정치행렬의 성질\n양정치행렬 \\(\\pmb  A\\) 은 모든 고유값은 양수이다. 따라서 고유값과 고유벡터의 성질에 위해 유도된 식 B.4 에 의하여 다음과 같은 분해가 가능하다.\n\\[\n\\begin{aligned}\n\\pmb{A} & = \\pmb{P} \\pmb{D} \\pmb{P}^{t} \\\\\n& = \\pmb{P} \\pmb{D}^{1/2} \\pmb{D}^{1/2} \\pmb{P}^{t} \\\\\n& = \\left [ \\pmb{P} \\pmb{D}^{1/2} \\pmb{P}^{t} \\right ] \\left [ \\pmb{P}\\pmb{D}^{1/2} \\pmb{P}^{t} \\right ] \\\\\n& = \\pmb{A}^{1/2} \\pmb{A}^{1/2}\n\\end{aligned}\n\\tag{C.16}\\]\n위의 식에서 \\(\\pmb{D}^{1/2}\\)는 \\(\\pmb{D}\\)의 대각원소의 제곱근을 원소로 하는 대각행렬이다. 이는 양정치 행렬의 고유값이 모두 양수이기 떄문에 가능하다.\n예를 들어 다변량 확률 벡터의 공분산 행렬 \\(\\pmb \\Sigma\\) 또는 상관행렬 \\(\\pmb R\\) 은 모두 양정치 행렬이다. 따라서 다음과 같은 분해가 가능하다.\n\\[\n\\pmb \\Sigma = \\pmb \\Sigma^{1/2} \\pmb \\Sigma^{1/2}, \\quad \\pmb R = \\pmb R^{1/2} \\pmb R^{1/2}\n\\tag{C.17}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>행렬의 분해</span>"
    ]
  },
  {
    "objectID": "qmd/app-likelihood-ratio.html",
    "href": "qmd/app-likelihood-ratio.html",
    "title": "부록 D — 가능도비 검정",
    "section": "",
    "text": "D.1 가능도비 검정의 기초\n가능도비 검정(likelihood ratio test) 은 제약 있는 모형과 제약 없는 모형의 최대가능도 함수(maximum likelihood function)의 비율를 이용하여 두 모형을 비교하는 검정이며 통계적 가설 검정에 널리 사용되고 있다.\n먼저 확률 변수 또는 확률 벡터 \\(\\pmb X\\) 가 확률 밀도 함수 \\(f(\\pmb X | \\pmb \\theta)\\) 를 따른다고 가정하자. 또한 다음과 같은 귀무 가설 검정을 고려하자. 또한 모수벡터 \\(\\pmb \\theta\\) 는 전체 모수 공간 \\(\\pmb \\Theta\\) 에 속한다고 가정한다.\n\\[\nH_0: \\pmb \\theta \\in {\\pmb \\Theta}_0 \\quad \\text{ vs } \\quad H_a: \\pmb \\theta \\in {\\pmb \\Theta} \\setminus {\\pmb \\Theta}_0\n\\tag{D.1}\\]\n위의 가설에서 귀무 가설 \\(H_0\\) 는 모수 공간 \\(\\pmb \\Theta\\) 의 부분 집합 \\(\\pmb \\Theta_0\\) 에 모수가 속한다는 것 (모수에 대한 제약조건)을 의미한다.\n이러한 가설 검정을 위하여 표본 벡터 \\(\\pmb X_1, \\ldots, \\pmb  X_n\\) 에 대한 가능도 함수 \\(L(\\pmb \\theta)\\) 와 로그 가능도 함수 \\(\\ell(\\pmb \\theta)\\)는 다음과 같이 정의된다. 참고로 가능도 함수는 주어진 표본의 값에 대하여 모수 \\(\\pmb \\theta\\) 의 함수로 생각할 수 있다.\n\\[\n\\begin{aligned}\nL(\\pmb \\theta) & = \\prod_{i=1}^n f(\\pmb X_i| \\pmb \\theta) \\\\\n\\ell(\\pmb \\theta) & = \\log \\prod_{i=1}^n f(\\pmb X_i| \\pmb \\theta) \\\\\n&= \\sum_{i=1}^n \\log f(\\pmb X_i| \\pmb \\theta)\n\\end{aligned}\n\\]\n이제 최대 가능도 추정을 다음 두 개의 경우에 대하여 고려해 보자.\n제약이 있는 경우에 대한 최대 가능도 추정량은 다음의 조건을 만족하는 경우\n\\[\n\\hat{\\pmb \\theta}_0 = \\arg\\max_{\\pmb \\theta \\in \\pmb \\Theta_0} L(\\pmb \\theta)\n\\]\n제약이 없는 경우에 대한 최대 가능도 추정량은 다음의 조건을 만족하는 경우니다.\n\\[\n\\hat{\\pmb \\theta} = \\arg\\max_{\\pmb \\theta \\in \\pmb \\Theta} L(\\pmb \\theta)\n\\]\n이제 두 경우에 대한 최대 가능도 추정량을 이용하여 가능도비 검정 통계량 \\(\\Lambda\\) 를 다음과 같이 정의한다.\n\\[\n\\begin{aligned}\n  \\Lambda & =\\frac{\\sup_{\\pmb \\theta \\in \\pmb \\Theta_0} L(\\pmb \\theta)}{\\sup_{\\pmb \\theta \\in \\pmb \\Theta} L(\\pmb \\theta)} \\\\\n  & =\\frac{L(\\hat{\\pmb \\theta}_0)}{L(\\hat{\\pmb \\theta})} \\in (0,1]\n\\end{aligned}\n\\]\n가능도비 \\(\\Lambda\\) 는 귀무 가설이 참일 때 1에 가까운 값을 가지며, 실제 모수가 귀무 가설의 제약조건에서 멀어지면 0에 가까운 값을 가진다. 따라서 귀무 가설을 기각하기 위한 기각역은 \\(\\Lambda\\) 가 작은 값이 되는 영역으로 설정한다. 또한 가설 검정의 편의성을 위하여 가능도비에 로그를 취하고 \\(-2\\) 를 취한 값을 검정에 이용한다.\n\\[\n\\lambda = -2 \\log \\Lambda = -2 \\left\\{ \\ell(\\hat{\\pmb \\theta}_0) - \\ell(\\hat{\\pmb \\theta}) \\right\\} \\in [0, \\infty)\n\\tag{D.2}\\]\n이제 위의 식에서 정의된 \\(\\lambda\\) 는 값이 크면 클수록 귀무가설에 반대되는 증거이다. 따라서 \\(\\lambda\\)의 값이 주어진 기각역 \\(c\\) 보다 크면 귀무 가설을 기각한다.\n\\[\n\\text{reject } H_0 \\quad \\text{ if } \\lambda &gt; c\n\\]\n기각역 \\(c\\) 는 \\(\\lambda\\) 에 비례하는 적절한 검정 통계량을 찾은 다음, 주어진 확률 분포와 표본의 갯수에 따라서 검정 통계량의 정확한 분포를 구하여 유도할 수 있다. 하지만 대부분의 경우에는 \\(\\lambda\\) 의 다음과 같은 점근적 성질(표본의 개수가 증가할 때 극한 분포를 이용)을 이용하여 기각역을 유도한다(Wilks’ theorem)\n\\[\n\\lambda = -2\\log\\Lambda \\rightarrow_{d} \\chi^2_{\\nu}\n\\] 위의 성질에서 \\(\\rightarrow_{d}\\) 는 분포의 수렴을 의미하며, \\(\\chi^2_\\nu\\) 는 자유도 \\(\\nu\\) 인 카이제곱 분포를 나타낸다. 자유도 \\(\\nu\\) 는 전체 모수 공간과 제약조건 공간 차원의 차이이며 다음과 같이 계산된다.\n\\[\n\\nu = \\dim(\\pmb \\Theta)-\\dim( \\pmb \\Theta_0)\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>가능도비 검정</span>"
    ]
  },
  {
    "objectID": "qmd/app-likelihood-ratio.html#가능도비-검정의-기초",
    "href": "qmd/app-likelihood-ratio.html#가능도비-검정의-기초",
    "title": "부록 D — 가능도비 검정",
    "section": "",
    "text": "제약이 있는 경우, 즉 귀무 가설이 참인 경우: \\(\\theta \\in \\pmb \\Theta_0\\)\n\n\n\n\n제약이 없는 경우, 즉 귀무 가설이 거짓인 경우",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>가능도비 검정</span>"
    ]
  },
  {
    "objectID": "qmd/app-likelihood-ratio.html#다변량-정규분포의-가능도비-검정",
    "href": "qmd/app-likelihood-ratio.html#다변량-정규분포의-가능도비-검정",
    "title": "부록 D — 가능도비 검정",
    "section": "D.2 다변량 정규분포의 가능도비 검정",
    "text": "D.2 다변량 정규분포의 가능도비 검정\n\nD.2.1 두 평균벡터의 비교\n확률 벡터 \\(\\pmb X\\) 과 \\(\\pmb Y\\) 가 평균이 각각 \\(\\pmb \\mu_1\\), \\(\\pmb \\mu_2\\) 이고 공분산이 \\(\\pmb \\Sigma\\) 인 p-차원 다변량 정규 분포를 따른다고 가정하자.\n\\[ \\pmb X \\sim N_p(\\pmb \\mu_1, \\pmb \\Sigma), \\quad \\pmb Y \\sim N_p(\\pmb \\mu_2, \\pmb \\Sigma) \\] 다변량 정규 분포의 확률밀도함수는 다음과 같이 주어진다.\n\\[\nf_p(\\pmb X \\mid {\\pmb \\mu},{\\pmb \\Sigma}  )= |2 \\pi \\pmb \\Sigma|^{-1/2}\n\\exp \\{ - \\tfrac{1}{2} ( \\pmb X -\\pmb \\mu)^t {\\pmb \\Sigma}^{-1}( \\pmb X-\\pmb \\mu) \\}\n\\]\n더 나아가 다음과 같은 가설 검정을 고려하자.\n\\[\nH_0 : \\pmb \\mu_1 = \\pmb \\mu_2 \\text{ vs } H_a: \\pmb \\mu_1 \\neq \\pmb \\mu_2\n\\]\n이제 가설 검정을 위하여 두 그룹에서 각각 \\(n_1, n_2\\)개의 다변량 표본이 관측되었다고 하자.\n\\[\n\\pmb X_1, \\pmb X_2, \\dots, \\pmb X_{n_1} \\sim_{IID} N(\\pmb \\mu_1, \\pmb \\Sigma), \\quad \\pmb Y_1, \\pmb Y_2, \\dots, \\pmb Y_{n_2} \\sim_{IID} N(\\pmb \\mu_2, \\pmb \\Sigma)\n\\] ### 로그 가능도 함수\n이제 로그 가능도 함수를 정의하자. 먼저 제약 조건이 없는 경우를 고려하자.\n\\[\n\\begin{aligned}\n\\ell({\\pmb \\mu_1},{\\pmb \\mu_2},{\\pmb \\Sigma}) &=\n\\log L({\\pmb \\mu_1},{\\pmb \\mu_2},{\\pmb \\Sigma}) \\\\\n& = \\log \\prod_{i=1}^{n_1} f_p(\\pmb X_i \\mid {\\pmb \\mu_1},{\\pmb \\Sigma} ) \\prod_{i=1}^{n_2} f_p(\\pmb Y_i \\mid {\\pmb \\mu_2},{\\pmb \\Sigma} ) \\\\\n& = \\sum_{i=1}^{n_1} \\log f_p(\\pmb X_i \\mid {\\pmb \\mu_1},{\\pmb \\Sigma} ) + \\sum_{i=1}^{n_2} \\log f_p(\\pmb Y_i \\mid {\\pmb \\mu_2},{\\pmb \\Sigma} ) \\\\\n& = -\\frac{n_1 + n_2 }{2}\\log|2\\pi \\pmb \\Sigma| \\\\\n& ~~ -\\frac{1}{2} \\Bigg [ \\sum_{i=1}^{n_1} (\\pmb X_i - \\pmb \\mu_1)^t \\pmb \\Sigma^{-1} (\\pmb X_i - \\pmb \\mu_1) + \\sum_{i=1}^{n_2} (\\pmb Y_i - \\pmb \\mu_2)^t \\pmb \\Sigma^{-1} (\\pmb Y_i - \\pmb \\mu_2) \\Bigg ]\n\\end{aligned}\n\\tag{D.3}\\]\n만약 귀무 가설이 참이라면 \\(\\pmb \\mu_1 = \\pmb \\mu_2 = \\pmb \\mu\\) 이므로 로그 가능도 함수는 다음과 같이 주어진다.\n\\[\n\\begin{aligned}\n\\ell({\\pmb \\mu},{\\pmb \\Sigma}) &=\n\\log L({\\pmb \\mu},{\\pmb \\Sigma}) \\\\\n& = \\log \\prod_{i=1}^{n_1} f_p(\\pmb X_i \\mid {\\pmb \\mu},{\\pmb \\Sigma} ) \\prod_{i=1}^{n_2} f_p(\\pmb Y_i \\mid {\\pmb \\mu},{\\pmb \\Sigma} ) \\\\\n& = \\sum_{i=1}^{n_1} \\log f_p(\\pmb X_i \\mid {\\pmb \\mu},{\\pmb \\Sigma} ) + \\sum_{i=1}^{n_2} \\log f_p(\\pmb Y_i \\mid {\\pmb \\mu},{\\pmb \\Sigma} ) \\\\\n& = -\\frac{n_1 + n_2 }{2}\\log|2\\pi \\pmb \\Sigma| \\\\\n& ~~ -\\frac{1}{2} \\Bigg [ \\sum_{i=1}^{n_1} (\\pmb X_i - \\pmb \\mu)^t \\pmb \\Sigma^{-1} (\\pmb X_i - \\pmb \\mu) + \\sum_{i=1}^{n_2} (\\pmb Y_i - \\pmb \\mu)^t \\pmb \\Sigma^{-1} (\\pmb Y_i - \\pmb \\mu) \\Bigg ]\n\\end{aligned}\n\\tag{D.4}\\]\n\n\nD.2.2 재곱합의 분해\n이제 이차형식의 다음과 같은 대각합(trace) 표현을 이용하면\n\\[\n{\\pmb x}^t \\pmb A {\\pmb x} =tr({\\pmb x}^t \\pmb A {\\pmb x}) = tr(\\pmb A {\\pmb x} {\\pmb x}^t )\n\\]\n로그 가능도 함수에 나타나는 제곱합 항들을 다음과 같이 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\sum_{i=1}^{n_1} (\\pmb X_i - \\pmb \\mu_1)^t \\pmb \\Sigma^{-1} (\\pmb X_i - \\pmb \\mu_1)\n&=\n\\operatorname{tr} \\Bigg [ \\sum_{i=1}^{n_1} (\\pmb X_i - \\pmb \\mu_1)^t \\pmb \\Sigma^{-1} (\\pmb X_i - \\pmb \\mu_1) \\Bigg ] \\\\\n&=\n\\sum_{i=1}^{n_1}  \\operatorname{tr} \\Bigg [ (\\pmb X_i - \\pmb \\mu_1)^t \\pmb \\Sigma^{-1} (\\pmb X_i - \\pmb \\mu_1) \\Bigg ] \\\\\n&=\n\\sum_{i=1}^{n_1}  \\operatorname{tr} \\Bigg [ \\pmb \\Sigma^{-1} (\\pmb X_i - \\pmb \\mu_1) (\\pmb X_i - \\pmb \\mu_1)^t  \\Bigg  ] \\\\\n  &=\n\\operatorname{tr} \\Bigg [ \\pmb \\Sigma^{-1}  \\sum_{i=1}^{n_1}  (\\pmb X_i - \\pmb \\mu_1) (\\pmb X_i - \\pmb \\mu_1)^t  \\Bigg  ]\n\\end{aligned}\n\\tag{D.5}\\]\n또한 위의 식에서 \\(\\pmb X_i - \\pmb \\mu_1\\) 를 \\((\\pmb X_i - \\bar{\\pmb X}_1) + (\\bar{\\pmb X}_1 - \\pmb \\mu_1)\\) 로 전개하면 평균 분해를 이용하여 다음과 같이 쓸 수 있다.\n\\[\n\\begin{aligned}\n\\sum_{i=1}^{n_1}  (\\pmb X_i - \\pmb \\mu_1) (\\pmb X_i - \\pmb \\mu_1)^t\n  &=\n  \\sum_{i=1}^{n_1}  \\Big [ (\\pmb X_i - \\bar{\\pmb X}_1) + (\\bar{\\pmb X}_1 - \\pmb \\mu_1) \\Big ] \\Big [ (\\pmb X_i - \\bar{\\pmb X}_1) + (\\bar{\\pmb X}_1 - \\pmb \\mu_1) \\Big ]^t \\\\\n  &=\n  \\sum_{i=1}^{n_1}  (\\pmb X_i - \\bar{\\pmb X}_1) (\\pmb X_i - \\bar{\\pmb X}_1)^t + n_1 (\\bar{\\pmb X}_1 - \\pmb \\mu_1) (\\bar{\\pmb X}_1 - \\pmb \\mu_1)^t \\\\\n  &=\n  n_1 S_x + n_1 (\\bar{\\pmb X} - \\pmb \\mu_1) (\\bar{\\pmb X} - \\pmb \\mu_1)^t\n\\end{aligned}\n  \\tag{D.6}\\]\n위의 식에서 \\(S_x\\) 는 확률 표본 \\(\\pmb X_1, \\pmb X_2, \\dots, \\pmb X_{n_1}\\) 으로 만들어진 표본 공분산 행렬이다 (아래 식에서 공분산행렬의 추정에서 분포를 최대가능도 추정량으로 하여 \\(n_1-1\\) 대신 \\(n_1\\) 을 적용하였다)\n\\[\nS_X = \\frac{1}{n_1} \\sum_{i=1}^{n_1}  (\\pmb X_i - \\bar{\\pmb X}_1) (\\pmb X_i - \\bar{\\pmb X}_1)^t\n\\]\n이제 식 D.6 을 식 D.5 에 적용하면 다음과 같이 쓸 수 있다.\n\\[\n\\begin{aligned}\n\\sum_{i=1}^{n_1} (\\pmb X_i - \\pmb \\mu_1)^t \\pmb \\Sigma^{-1} (\\pmb X_i - \\pmb \\mu_1)\n  &=\n\\operatorname{tr} \\Bigg [ \\pmb \\Sigma^{-1}  \\sum_{i=1}^{n_1}  (\\pmb X_i - \\pmb \\mu_1) (\\pmb X_i - \\pmb \\mu_1)^t  \\Bigg  ]  \\\\\n   &=\n  \\operatorname{tr} \\Bigg [ n_1\\pmb \\Sigma^{-1} S_x + n_1 \\pmb \\Sigma^{-1}  (\\bar{\\pmb X} - \\pmb \\mu_1) (\\bar{\\pmb X} - \\pmb \\mu_1)^t \\Bigg ] \\\\\n   &=\n  n_1  \\Bigg [ \\operatorname{tr} ( \\pmb \\Sigma^{-1} S_x ) + \\operatorname{tr} ( \\pmb \\Sigma^{-1}  (\\bar{\\pmb X} - \\pmb \\mu_1) (\\bar{\\pmb X} - \\pmb \\mu_1)^t ) \\Bigg ]  \\\\\n   &= n_1   \\operatorname{tr} ( \\pmb \\Sigma^{-1} S_x ) + n_1  (\\bar{\\pmb X} - \\pmb \\mu_1)^t \\pmb   \\Sigma^{-1}  (\\bar{\\pmb X} - \\pmb \\mu_1)\n\\end{aligned}\n\\tag{D.7}\\]\n\n\nD.2.3 로그 가능도 함수의 재표현\n이제 분해식 식 D.7 를 식 D.3 에 적용하여 제약조건이 없는 경우의 로그 가능도 함수를 다음과 같이 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\ell({\\pmb \\mu_1},{\\pmb \\mu_2},{\\pmb \\Sigma})\n& =\n-\\frac{n_1 + n_2 }{2}\\log|2\\pi \\pmb \\Sigma| \\\\\n& ~~ -\\frac{1}{2} \\Bigg [ \\sum_{i=1}^{n_1} (\\pmb X_i - \\pmb \\mu_1)^t \\pmb \\Sigma^{-1} (\\pmb X_i - \\pmb \\mu_1) + \\sum_{i=1}^{n_2} (\\pmb Y_i - \\pmb \\mu_2)^t \\pmb \\Sigma^{-1} (\\pmb Y_i - \\pmb \\mu_2) \\Bigg ] \\\\\n& =\n-\\frac{n_1 + n_2 }{2}\\log|2\\pi \\pmb \\Sigma| \\\\\n& ~~ -\\frac{1}{2} \\Bigg [ n_1   \\operatorname{tr} ( \\pmb \\Sigma^{-1} S_x ) + n_1  (\\bar{\\pmb X} - \\pmb \\mu_1)^t \\pmb   \\Sigma^{-1}  (\\bar{\\pmb X} - \\pmb \\mu_1)  \\\\\n& \\quad  \\quad  \\quad + n_2   \\operatorname{tr} ( \\pmb \\Sigma^{-1} S_y ) + n_2  (\\bar{\\pmb Y} - \\pmb \\mu_2)^t \\pmb   \\Sigma^{-1}  (\\bar{\\pmb Y} - \\pmb \\mu_2)  \\Bigg ] \\\\\n& =\n-\\frac{n_1 + n_2 }{2}\\log|2\\pi {\\pmb \\Sigma} | -\\frac{1}{2} \\operatorname{tr} (  {\\pmb \\Sigma}^{-1} [n_1 S_x  + n_2 S_y]) \\\\\n& \\quad -\\frac{1}{2}  \\left [ n_1  (\\bar{\\pmb X} - {\\pmb \\mu_1})^t    {\\pmb \\Sigma}^{-1}  (\\bar{\\pmb X} - {\\pmb \\mu_1})  + n_2  (\\bar{\\pmb Y} - { \\pmb \\mu_2} )^t   {\\pmb \\Sigma}^{-1}  (\\bar{\\pmb Y} - { \\pmb \\mu_2} )  \\right ]  \\\\\n& =\n-\\frac{n_1 + n_2 }{2}\\log|2\\pi {\\pmb \\Sigma} | -\\frac{1}{2} \\operatorname{tr} (  {\\pmb \\Sigma}^{-1} \\pmb W ) \\\\\n& \\quad -\\frac{1}{2}  \\left [ n_1  (\\bar{\\pmb X} - {\\pmb \\mu_1})^t    {\\pmb \\Sigma}^{-1}  (\\bar{\\pmb X} - {\\pmb \\mu_1})  + n_2  (\\bar{\\pmb Y} - { \\pmb \\mu_2} )^t   {\\pmb \\Sigma}^{-1}  (\\bar{\\pmb Y} - { \\pmb \\mu_2} )  \\right ]\n\\end{aligned}\n\\tag{D.8}\\]\n위의 식에서 \\(\\pmb W\\) 는 그룹내의 변동을 표시하는 제곱합 행렬이다. 참고로 식 3.4 에서 정의된 풀링된 공분산 행렬 \\(\\pmb S_p\\) 와 다음과 같은 관계가 있다.\n\\[\n\\begin{aligned}\n\\pmb W & =  n_1 S_X + n_2 S_Y \\\\\n& = \\sum_{i=1}^{n_1}  (\\pmb X_i - \\bar{\\pmb X}_1) (\\pmb X_i - \\bar{\\pmb X}_1)^t + \\sum_{i=1}^{n_2}  (\\pmb Y_i - \\bar{\\pmb Y}_2) (\\pmb Y_i - \\bar{\\pmb Y}_2)^t \\\\\n& = (n_1+n_2 -2) \\pmb S_{p}\n\\end{aligned}\n\\tag{D.9}\\]\n제약조건이 없는 가능도 함수 식 D.8 에 대해서 최대 가능도 추정을 적용하면 다음과 같은 평균에 대한 최대가능도 추정량은 각각 그룹의 표본 평균이 된다.\n\\[\n\\hat{\\pmb \\mu}_1 = \\bar{\\pmb X}_1, \\quad \\hat{\\pmb \\mu}_2 = \\bar{\\pmb Y}_1\n\\]\n이제 위의 평균에 대한 최대가능도 추정량을 제약 조건이 없는 가능도 함수 식 D.8 에 대입하면 다음과 같은 식을 얻게 된다.\n\\[\n\\begin{aligned}\n\\ell(\\hat {\\pmb \\mu_1},\\hat {\\pmb \\mu_2},{\\pmb \\Sigma})\n& =\n-\\frac{n_1 + n_2 }{2}\\log|2\\pi {\\pmb \\Sigma} | -\\frac{1}{2} \\operatorname{tr} (  {\\pmb \\Sigma}^{-1} \\pmb W ) \\\\\n& \\quad -\\frac{1}{2}  \\left [ n_1  (\\bar{\\pmb X} - \\hat {\\pmb \\mu_1})^t   \\hat {\\pmb \\Sigma}^{-1}  (\\bar{\\pmb X} - \\hat  {\\pmb \\mu_1})  + n_2  (\\bar{\\pmb Y} - \\hat  { \\pmb \\mu_2} )^t  \\hat {\\pmb \\Sigma}^{-1}  (\\bar{\\pmb Y} - \\hat  { \\pmb \\mu_2} )  \\right ] \\\\\n& =\n-\\frac{n_1 + n_2 }{2}\\log|2\\pi {\\pmb \\Sigma} | -\\frac{1}{2} \\operatorname{tr} (  {\\pmb \\Sigma}^{-1} \\pmb W )  + 0\n\\end{aligned}\n\\]\n위의 식에서 공분산 행렬에 대한 최대가능도 추정량을 구하면 다음과 같은 추정량을 얻게되며\n\\[\n\\hat {\\pmb \\Sigma} = \\frac{1}{n_1 + n_2} \\pmb W\n\\]\n따라서 공분산 행렬에 대한 추정량을 제약조건이 없는 로그 가능도 함수에 대입하면 다음의 값을 얻게된다.\n\\[\n\\begin{aligned}\n\\ell(\\hat {\\pmb \\mu_1},\\hat {\\pmb \\mu_2},\\hat {\\pmb \\Sigma})\n& =\n-\\frac{n_1 + n_2 }{2}\\log|2\\pi \\hat {\\pmb \\Sigma} | -\\frac{1}{2} \\operatorname{tr} (  \\hat {\\pmb \\Sigma}^{-1} \\pmb W ) \\\\\n& = -\\frac{n_1 + n_2 }{2}\\log|2\\pi \\hat {\\pmb \\Sigma} | -\\frac{1}{2} \\operatorname{tr} (  (n_1 + n_2) \\pmb W^{-1} \\pmb W ) \\\\\n& = -\\frac{n_1 + n_2 }{2}\\log|2\\pi \\hat {\\pmb \\Sigma} | -\\frac{p(n_1 + n_2)}{2}\n\\end{aligned}\n\\tag{D.10}\\]\n이제 제약조건 \\(\\pmb \\mu_1 = \\pmb \\mu_2 = \\pmb \\mu\\) 가 있는 경우의 로그 가능도 함수를 고려하자. 식 D.8 의 마지막 항을 이용하면 다음과 같이 제약 조건이 있는 가능도 함수 식 D.4 를 다음과 같이 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\ell({\\pmb \\mu},{\\pmb \\Sigma})\n& =\n-\\frac{n_1 + n_2 }{2}\\log|2\\pi {\\pmb \\Sigma} | -\\frac{1}{2} \\operatorname{tr} (  {\\pmb \\Sigma}^{-1} \\pmb W ) \\\\\n& \\quad -\\frac{1}{2}  \\left [ n_1  (\\bar{\\pmb X} - {\\pmb \\mu})^t    {\\pmb \\Sigma}^{-1}  (\\bar{\\pmb X} - {\\pmb \\mu})  + n_2  (\\bar{\\pmb Y} - { \\pmb \\mu} )^t   {\\pmb \\Sigma}^{-1}  (\\bar{\\pmb Y} - { \\pmb \\mu} )  \\right ]\n\\end{aligned}\n\\tag{D.11}\\]\n위의 제약조건이 있는 로그 가능도 함수에 대하여 평균 벡터 \\(\\pmb \\mu\\) 에 최대 가능도 추정량을 구하면 다음과 같다.\n\\[ \\hat{\\pmb \\mu} = \\frac{n_1 \\bar{\\pmb X} + n_2 \\bar{\\pmb Y}}{n_1 + n_2}  \\]\n여기서 그룹 간의 변동을 나타내는 제곱합 행렬 \\(\\pmb B\\) 는 다음과 같이 정의한다.\n\\[\n\\pmb B \\equiv n_1 (\\bar{\\pmb X} - \\hat{\\pmb \\mu}) (\\bar{\\pmb X} - \\hat{\\pmb \\mu})^t + n_2 (\\bar{\\pmb Y} - \\hat{\\pmb \\mu}) (\\bar{\\pmb Y} - \\hat{\\pmb \\mu})^t\n\\tag{D.12}\\]\n이제 평균 벡터의 추정량 \\(\\hat{\\pmb \\mu}\\) 를 식 D.11 을 에 대입하면 다음과 같이 로그 가능도 함수가 나타나며\n\\[\n\\begin{aligned}\n\\ell( \\hat {\\pmb \\mu},{\\pmb \\Sigma})\n& =\n-\\frac{n_1 + n_2 }{2}\\log|2\\pi {\\pmb \\Sigma} | -\\frac{1}{2} \\operatorname{tr} (  {\\pmb \\Sigma}^{-1} \\pmb W ) \\\\\n& \\quad -\\frac{1}{2}  \\left [ n_1  (\\bar{\\pmb X} - \\hat{\\pmb \\mu})^t    {\\pmb \\Sigma}^{-1}  (\\bar{\\pmb X} - \\hat {\\pmb \\mu})  + n_2  (\\bar{\\pmb Y} - \\hat { \\pmb \\mu} )^t   {\\pmb \\Sigma}^{-1}  (\\bar{\\pmb Y} - \\hat { \\pmb \\mu} )  \\right ] \\\\\n& = -\\frac{n_1 + n_2 }{2}\\log|2\\pi {\\pmb \\Sigma} | -\\frac{1}{2} \\operatorname{tr} (  {\\pmb \\Sigma}^{-1} \\pmb W ) \\\\\n& \\quad -\\frac{1}{2} \\operatorname{tr}   \\left [ {\\pmb \\Sigma}^{-1} \\{ n_1 (\\bar{\\pmb X} - \\hat{\\pmb \\mu}) (\\bar{\\pmb X} - \\hat{\\pmb \\mu})^t + n_2 (\\bar{\\pmb Y} - \\hat{\\pmb \\mu}) (\\bar{\\pmb Y} - \\hat{\\pmb \\mu})^t \\}  \\right ] \\\\\n& = -\\frac{n_1 + n_2 }{2}\\log|2\\pi {\\pmb \\Sigma} | -\\frac{1}{2} \\operatorname{tr} (  {\\pmb \\Sigma}^{-1} (\\pmb W + \\pmb B) )\n\\end{aligned}\n\\tag{D.13}\\]\n이제 오그 가능도 함수 식 D.13 에서 공분산 행렬 \\(\\pmb \\Sigma\\) 에 대한 최대 가능도 추정량을 구하면 다음과 같다.\n\\[ \\hat {\\pmb \\Sigma}_0 = \\frac{1}{n_1 + n_2} (\\pmb W + \\pmb B) \\]\n위의 공분산 행렬에 대한 추정량을 식 D.13 에 대입하면 제약조건이 있는 경우 로그 가능도 함수의 최대값은 다음과 같이 얻어진다.\n\\[\n\\ell( \\hat {\\pmb \\mu},\\hat {\\pmb \\Sigma}_0)\n= -\\frac{n_1 + n_2 }{2}\\log|2\\pi \\hat {\\pmb \\Sigma}_0 | -\\frac{p(n_1 + n_2)}{2}\n\\tag{D.14}\\]\n\n\nD.2.4 가능도비 검정 통계량\n이제 로그가능도비 통계량 \\(\\lambda\\) 를 식 D.2 에 정의된 식을 이용하여 다음과 같이 쓸 수 있다.\n\\[\n\\begin{aligned}\n\\lambda\n&= -2 \\log \\Lambda \\\\\n&= -2 \\left\\{ \\ell\\!\\left(\\hat{\\boldsymbol{\\mu}},\\,\\hat{\\boldsymbol{\\Sigma}}_{0}\\right)\n            - \\ell\\!\\left(\\hat{\\boldsymbol{\\mu}}_{1},\\,\\hat{\\boldsymbol{\\mu}}_{2},\\,\\hat{\\boldsymbol{\\Sigma}}\\right) \\right\\} \\\\\n&= N \\log \\left|\\hat{\\boldsymbol{\\Sigma}}_{0}\\right|\n  - N \\log \\left|\\hat{\\boldsymbol{\\Sigma}}\\right| \\\\\n&= N \\log \\left(\n      \\frac{ \\left|\\hat{\\boldsymbol{\\Sigma}}_{0}\\right| }{ \\left|\\hat{\\boldsymbol{\\Sigma}}\\right| }\n    \\right) \\\\\n&= N \\log \\left(\n      \\frac{ \\left|\\boldsymbol{W}+\\boldsymbol{B}\\right| }{ \\left|\\boldsymbol{W}\\right| }\n    \\right),\n\\end{aligned}\n\\tag{D.15}\\]\n위의 식에서 \\(N = n_1 + n_2\\) 이다.\n마지막으로 두 집단에서는 다음과 같은 행렬식 공식를 이용하자 (부록의 섹션 A.9 참조)\n\\[\n|{\\pmb A}+{\\pmb u} {\\pmb v}^t |=| {\\pmb A}| \\big ( {\\pmb 1} +{\\pmb v}^t {\\pmb A}^{-1} {\\pmb u} \\big )\n\\tag{D.16}\\]\n위의 정리를 이용하기 위하여 식 D.12 정의된 그룹간 제곱합 행렬 \\(\\pmb B\\) 를 다음과 같이 표현해 보자\n\\[\n\\begin{aligned}\n\\pmb B & =  n_1 (\\bar{\\pmb X} - \\hat{\\pmb \\mu}) (\\bar{\\pmb X} - \\hat{\\pmb \\mu})^t + n_2 (\\bar{\\pmb Y} - \\hat{\\pmb \\mu}) (\\bar{\\pmb Y} - \\hat{\\pmb \\mu})^t \\\\\n& = n_1 \\left [ \\bar{\\pmb X} - \\frac{n_1 \\bar{\\pmb X} + n_2 \\bar{\\pmb Y}}{N} \\right ] + n_2 \\left [ \\bar{\\pmb Y} - \\frac{n_1 \\bar{\\pmb X} + n_2 \\bar{\\pmb Y}}{N} \\right ]^t \\\\\n& = \\frac{n_1 n_2}{N} (\\bar{\\pmb X} - \\bar{\\pmb Y}) (\\bar{\\pmb X} - \\bar{\\pmb Y})^t \\\\\n& = \\alpha {\\pmb d} {\\pmb d}^t,\n\\end{aligned}\n\\tag{D.17}\\]\n위의 식에서\n\\[\n\\alpha = \\frac{n_1 n_2}{N}, \\quad d = \\bar{\\pmb X} - \\bar{\\pmb Y}\n\\]\n따라서 식 D.16 에서 \\(\\pmb u = \\pmb v = \\sqrt{\\alpha} \\pmb d\\) 로 놓으면\n\\[\n\\begin{aligned}\n| {\\pmb W} + {\\pmb B} | & = | {\\pmb W} + \\alpha {\\pmb d} {\\pmb d}^t | \\\\\n&= |{\\pmb W}|  \\left( {\\pmb 1} + \\alpha {\\pmb d}^t {\\pmb W}^{-1} {\\pmb d} \\right)\n\\end{aligned}\n\\] 이제 위의 식을 식 D.15 에 넣고 정리하면 다음과 같은 결과를 얻는다.\n\\[\n\\begin{aligned}\n\\lambda\n&= -2 \\log \\Lambda \\\\\n&= N \\log \\left(\n      \\frac{ \\left|\\boldsymbol{W}+\\boldsymbol{B}\\right| }{ \\left|\\boldsymbol{W}\\right| }\n    \\right) \\\\\n&= N \\log \\left(\n      \\frac{ |{\\pmb W}|  \\left( 1 + \\alpha {\\pmb d}^t {\\pmb W}^{-1} {\\pmb d} \\right) }{ |{\\pmb W}| }\n    \\right) \\\\\n&= N \\log\\Big(1+\\alpha\\ d^\\top W^{-1} d\\Big) \\\\\n& = N \\log \\Big ( 1+ \\frac{T^2}{N-2} \\Big )\n\\end{aligned}\n\\tag{D.18}\\]\n위의 식에서 주어진 \\(T^2\\) 는 식 3.5 에서 장의한 Hotelling의 \\(T^2\\) 통계량이다. 따라서 로그 가능도비 검정 통계량 \\(\\lambda\\) 와 Hotelling의 \\(T^2\\) 통계량은 단조 증가 함수 관계에 있음을 알 수 있다. 이러한 결과로서 Hotelling의 \\(T^2\\) 을 이용한 검정은 가능도비 검정이다.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>가능도비 검정</span>"
    ]
  }
]